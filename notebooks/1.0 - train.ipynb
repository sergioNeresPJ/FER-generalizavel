{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1218456,"sourceType":"datasetVersion","datasetId":696331},{"sourceId":2905596,"sourceType":"datasetVersion","datasetId":1780823},{"sourceId":4876418,"sourceType":"datasetVersion","datasetId":2784784},{"sourceId":8926506,"sourceType":"datasetVersion","datasetId":5369417},{"sourceId":9730490,"sourceType":"datasetVersion","datasetId":5954662},{"sourceId":10462385,"sourceType":"datasetVersion","datasetId":6477402},{"sourceId":10463310,"sourceType":"datasetVersion","datasetId":6478017}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n\ntry:\n    arquivos = os.listdir('/kaggle/working/')\n    arquivos.sort()\n    print(arquivos)\nexcept FileNotFoundError:\n    print(\"The directory /kaggle/working/ was not found.\")\nexcept Exception as e:\n    print(f\"An error occurred: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T12:50:26.614305Z","iopub.execute_input":"2025-02-16T12:50:26.614510Z","iopub.status.idle":"2025-02-16T12:50:26.619506Z","shell.execute_reply.started":"2025-02-16T12:50:26.614490Z","shell.execute_reply":"2025-02-16T12:50:26.618526Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## simple_tokenizer.py","metadata":{}},{"cell_type":"code","source":"! pip install ftfy","metadata":{"id":"y_wipLK2yRkq","executionInfo":{"status":"ok","timestamp":1736782222336,"user_tz":180,"elapsed":3682,"user":{"displayName":"Sergio Neres Pereira Junior","userId":"05770949936273593301"}},"outputId":"a31cfc60-8704-4636-bd18-4e4b313c2d22","trusted":true,"execution":{"iopub.status.busy":"2025-02-16T12:50:53.786542Z","iopub.execute_input":"2025-02-16T12:50:53.786907Z","iopub.status.idle":"2025-02-16T12:50:58.426241Z","shell.execute_reply.started":"2025-02-16T12:50:53.786876Z","shell.execute_reply":"2025-02-16T12:50:58.425077Z"},"scrolled":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gzip\nimport html\nimport os\nfrom functools import lru_cache\n\nimport ftfy\nimport regex as re\n\n\n\n@lru_cache()\ndef default_bpe():\n    return os.path.join('/kaggle/input/cafe-repo/Generalizable-FER-main/code/clip/bpe_simple_vocab_16e6.txt')\n\n\n@lru_cache()\ndef bytes_to_unicode():\n    \"\"\"\n    Returns list of utf-8 byte and a corresponding list of unicode strings.\n    The reversible bpe codes work on unicode strings.\n    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n    This is a signficant percentage of your normal, say, 32K bpe vocab.\n    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n    And avoids mapping to whitespace/control characters the bpe code barfs on.\n    \"\"\"\n    bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"¡\"), ord(\"¬\")+1))+list(range(ord(\"®\"), ord(\"ÿ\")+1))\n    cs = bs[:]\n    n = 0\n    for b in range(2**8):\n        if b not in bs:\n            bs.append(b)\n            cs.append(2**8+n)\n            n += 1\n    cs = [chr(n) for n in cs]\n    return dict(zip(bs, cs))\n\n\ndef get_pairs(word):\n    \"\"\"Return set of symbol pairs in a word.\n    Word is represented as tuple of symbols (symbols being variable-length strings).\n    \"\"\"\n    pairs = set()\n    prev_char = word[0]\n    for char in word[1:]:\n        pairs.add((prev_char, char))\n        prev_char = char\n    return pairs\n\n\ndef basic_clean(text):\n    text = ftfy.fix_text(text)\n    text = html.unescape(html.unescape(text))\n    return text.strip()\n\n\ndef whitespace_clean(text):\n    text = re.sub(r'\\s+', ' ', text)\n    text = text.strip()\n    return text\n\n\nclass SimpleTokenizer(object):\n    def __init__(self, bpe_path: str = default_bpe()):\n        self.byte_encoder = bytes_to_unicode()\n        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n        merges = open(bpe_path, 'rb').read().decode(\"utf-8\").split('\\n')\n        merges = merges[1:49152-256-2+1]\n        merges = [tuple(merge.split()) for merge in merges]\n        vocab = list(bytes_to_unicode().values())\n        vocab = vocab + [v+'</w>' for v in vocab]\n        for merge in merges:\n            vocab.append(''.join(merge))\n        vocab.extend(['<|startoftext|>', '<|endoftext|>'])\n        self.encoder = dict(zip(vocab, range(len(vocab))))\n        self.decoder = {v: k for k, v in self.encoder.items()}\n        self.bpe_ranks = dict(zip(merges, range(len(merges))))\n        self.cache = {'<|startoftext|>': '<|startoftext|>', '<|endoftext|>': '<|endoftext|>'}\n        self.pat = re.compile(r\"\"\"<\\|startoftext\\|>|<\\|endoftext\\|>|'s|'t|'re|'ve|'m|'ll|'d|[\\p{L}]+|[\\p{N}]|[^\\s\\p{L}\\p{N}]+\"\"\", re.IGNORECASE)\n\n    def bpe(self, token):\n        if token in self.cache:\n            return self.cache[token]\n        word = tuple(token[:-1]) + ( token[-1] + '</w>',)\n        pairs = get_pairs(word)\n\n        if not pairs:\n            return token+'</w>'\n\n        while True:\n            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n            if bigram not in self.bpe_ranks:\n                break\n            first, second = bigram\n            new_word = []\n            i = 0\n            while i < len(word):\n                try:\n                    j = word.index(first, i)\n                    new_word.extend(word[i:j])\n                    i = j\n                except:\n                    new_word.extend(word[i:])\n                    break\n\n                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n                    new_word.append(first+second)\n                    i += 2\n                else:\n                    new_word.append(word[i])\n                    i += 1\n            new_word = tuple(new_word)\n            word = new_word\n            if len(word) == 1:\n                break\n            else:\n                pairs = get_pairs(word)\n        word = ' '.join(word)\n        self.cache[token] = word\n        return word\n\n    def encode(self, text):\n        bpe_tokens = []\n        text = whitespace_clean(basic_clean(text)).lower()\n        for token in re.findall(self.pat, text):\n            token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))\n            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))\n        return bpe_tokens\n\n    def decode(self, tokens):\n        text = ''.join([self.decoder[token] for token in tokens])\n        text = bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors=\"replace\").replace('</w>', ' ')\n        return text\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T12:50:58.428060Z","iopub.execute_input":"2025-02-16T12:50:58.428403Z","iopub.status.idle":"2025-02-16T12:50:58.506857Z","shell.execute_reply.started":"2025-02-16T12:50:58.428370Z","shell.execute_reply":"2025-02-16T12:50:58.506245Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## model.py","metadata":{}},{"cell_type":"code","source":"from collections import OrderedDict\nfrom typing import Tuple, Union\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1):\n        super().__init__()\n\n        # all conv layers have stride 1. an avgpool is performed after the second convolution when stride > 1\n        self.conv1 = nn.Conv2d(inplanes, planes, 1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n\n        self.conv2 = nn.Conv2d(planes, planes, 3, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n\n        self.avgpool = nn.AvgPool2d(stride) if stride > 1 else nn.Identity()\n\n        self.conv3 = nn.Conv2d(planes, planes * self.expansion, 1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = None\n        self.stride = stride\n\n        if stride > 1 or inplanes != planes * Bottleneck.expansion:\n            # downsampling layer is prepended with an avgpool, and the subsequent convolution has stride 1\n            self.downsample = nn.Sequential(OrderedDict([\n                (\"-1\", nn.AvgPool2d(stride)),\n                (\"0\", nn.Conv2d(inplanes, planes * self.expansion, 1, stride=1, bias=False)),\n                (\"1\", nn.BatchNorm2d(planes * self.expansion))\n            ]))\n\n    def forward(self, x: torch.Tensor):\n        identity = x\n\n        out = self.relu(self.bn1(self.conv1(x)))\n        out = self.relu(self.bn2(self.conv2(out)))\n        out = self.avgpool(out)\n        out = self.bn3(self.conv3(out))\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n        return out\n\n\nclass AttentionPool2d(nn.Module):\n    def __init__(self, spacial_dim: int, embed_dim: int, num_heads: int, output_dim: int = None):\n        super().__init__()\n        self.positional_embedding = nn.Parameter(torch.randn(spacial_dim ** 2 + 1, embed_dim) / embed_dim ** 0.5)\n        self.k_proj = nn.Linear(embed_dim, embed_dim)\n        self.q_proj = nn.Linear(embed_dim, embed_dim)\n        self.v_proj = nn.Linear(embed_dim, embed_dim)\n        self.c_proj = nn.Linear(embed_dim, output_dim or embed_dim)\n        self.num_heads = num_heads\n\n    def forward(self, x):\n        x = x.reshape(x.shape[0], x.shape[1], x.shape[2] * x.shape[3]).permute(2, 0, 1)  # NCHW -> (HW)NC\n        x = torch.cat([x.mean(dim=0, keepdim=True), x], dim=0)  # (HW+1)NC\n        x = x + self.positional_embedding[:, None, :].to(x.dtype)  # (HW+1)NC\n        x, _ = F.multi_head_attention_forward(\n            query=x, key=x, value=x,\n            embed_dim_to_check=x.shape[-1],\n            num_heads=self.num_heads,\n            q_proj_weight=self.q_proj.weight,\n            k_proj_weight=self.k_proj.weight,\n            v_proj_weight=self.v_proj.weight,\n            in_proj_weight=None,\n            in_proj_bias=torch.cat([self.q_proj.bias, self.k_proj.bias, self.v_proj.bias]),\n            bias_k=None,\n            bias_v=None,\n            add_zero_attn=False,\n            dropout_p=0,\n            out_proj_weight=self.c_proj.weight,\n            out_proj_bias=self.c_proj.bias,\n            use_separate_proj_weight=True,\n            training=self.training,\n            need_weights=False\n        )\n\n        return x[0]\n\n\nclass ModifiedResNet(nn.Module):\n    \"\"\"\n    A ResNet class that is similar to torchvision's but contains the following changes:\n    - There are now 3 \"stem\" convolutions as opposed to 1, with an average pool instead of a max pool.\n    - Performs anti-aliasing strided convolutions, where an avgpool is prepended to convolutions with stride > 1\n    - The final pooling layer is a QKV attention instead of an average pool\n    \"\"\"\n\n    def __init__(self, layers, output_dim, heads, input_resolution=224, width=64):\n        super().__init__()\n        self.output_dim = output_dim\n        self.input_resolution = input_resolution\n\n        # the 3-layer stem\n        self.conv1 = nn.Conv2d(3, width // 2, kernel_size=3, stride=2, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(width // 2)\n        self.conv2 = nn.Conv2d(width // 2, width // 2, kernel_size=3, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(width // 2)\n        self.conv3 = nn.Conv2d(width // 2, width, kernel_size=3, padding=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(width)\n        self.avgpool = nn.AvgPool2d(2)\n        self.relu = nn.ReLU(inplace=True)\n\n        # residual layers\n        self._inplanes = width  # this is a *mutable* variable used during construction\n        self.layer1 = self._make_layer(width, layers[0])\n        self.layer2 = self._make_layer(width * 2, layers[1], stride=2)\n        self.layer3 = self._make_layer(width * 4, layers[2], stride=2)\n        self.layer4 = self._make_layer(width * 8, layers[3], stride=2)\n\n        embed_dim = width * 32  # the ResNet feature dimension\n        self.attnpool = AttentionPool2d(input_resolution // 32, embed_dim, heads, output_dim)\n\n    def _make_layer(self, planes, blocks, stride=1):\n        layers = [Bottleneck(self._inplanes, planes, stride)]\n\n        self._inplanes = planes * Bottleneck.expansion\n        for _ in range(1, blocks):\n            layers.append(Bottleneck(self._inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        def stem(x):\n            for conv, bn in [(self.conv1, self.bn1), (self.conv2, self.bn2), (self.conv3, self.bn3)]:\n                x = self.relu(bn(conv(x)))\n            x = self.avgpool(x)\n            return x\n\n        x = x.type(self.conv1.weight.dtype)\n        x = stem(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.attnpool(x)\n\n        return x\n\n\nclass LayerNorm(nn.LayerNorm):\n    \"\"\"Subclass torch's LayerNorm to handle fp16.\"\"\"\n\n    def forward(self, x: torch.Tensor):\n        orig_type = x.dtype\n        ret = super().forward(x.type(torch.float32))\n        return ret.type(orig_type)\n\n\nclass QuickGELU(nn.Module):\n    def forward(self, x: torch.Tensor):\n        return x * torch.sigmoid(1.702 * x)\n\n\nclass ResidualAttentionBlock(nn.Module):\n    def __init__(self, d_model: int, n_head: int, attn_mask: torch.Tensor = None):\n        super().__init__()\n\n        self.attn = nn.MultiheadAttention(d_model, n_head)\n        self.ln_1 = LayerNorm(d_model)\n        self.mlp = nn.Sequential(OrderedDict([\n            (\"c_fc\", nn.Linear(d_model, d_model * 4)),\n            (\"gelu\", QuickGELU()),\n            (\"c_proj\", nn.Linear(d_model * 4, d_model))\n        ]))\n        self.ln_2 = LayerNorm(d_model)\n        self.attn_mask = attn_mask\n\n    def attention(self, x: torch.Tensor):\n        self.attn_mask = self.attn_mask.to(dtype=x.dtype, device=x.device) if self.attn_mask is not None else None\n        return self.attn(x, x, x, need_weights=False, attn_mask=self.attn_mask)[0]\n\n    def forward(self, x: torch.Tensor):\n        x = x + self.attention(self.ln_1(x))\n        x = x + self.mlp(self.ln_2(x))\n        return x\n\n\nclass Transformer(nn.Module):\n    def __init__(self, width: int, layers: int, heads: int, attn_mask: torch.Tensor = None):\n        super().__init__()\n        self.width = width\n        self.layers = layers\n        self.resblocks = nn.Sequential(*[ResidualAttentionBlock(width, heads, attn_mask) for _ in range(layers)])\n\n    def forward(self, x: torch.Tensor):\n        return self.resblocks(x)\n\n\nclass VisionTransformer(nn.Module):\n    def __init__(self, input_resolution: int, patch_size: int, width: int, layers: int, heads: int, output_dim: int):\n        super().__init__()\n        self.input_resolution = input_resolution\n        self.output_dim = output_dim\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=width, kernel_size=patch_size, stride=patch_size, bias=False)\n\n        scale = width ** -0.5\n        self.class_embedding = nn.Parameter(scale * torch.randn(width))\n        self.positional_embedding = nn.Parameter(scale * torch.randn((input_resolution // patch_size) ** 2 + 1, width))\n        self.ln_pre = LayerNorm(width)\n\n        self.transformer = Transformer(width, layers, heads)\n\n        self.ln_post = LayerNorm(width)\n        self.proj = nn.Parameter(scale * torch.randn(width, output_dim))\n\n    def forward(self, x: torch.Tensor):\n        x = self.conv1(x)  # shape = [*, width, grid, grid]\n        x = x.reshape(x.shape[0], x.shape[1], -1)  # shape = [*, width, grid ** 2]\n        x = x.permute(0, 2, 1)  # shape = [*, grid ** 2, width]\n        x = torch.cat([self.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1)  # shape = [*, grid ** 2 + 1, width]\n        x = x + self.positional_embedding.to(x.dtype)\n        x = self.ln_pre(x)\n\n        x = x.permute(1, 0, 2)  # NLD -> LND\n        x = self.transformer(x)\n        x = x.permute(1, 0, 2)  # LND -> NLD\n\n        x = self.ln_post(x[:, 0, :])\n\n        if self.proj is not None:\n            x = x @ self.proj\n\n        return x\n\n\nclass CLIP(nn.Module):\n    def __init__(self,\n                 embed_dim: int,\n                 # vision\n                 image_resolution: int,\n                 vision_layers: Union[Tuple[int, int, int, int], int],\n                 vision_width: int,\n                 vision_patch_size: int,\n                 # text\n                 context_length: int,\n                 vocab_size: int,\n                 transformer_width: int,\n                 transformer_heads: int,\n                 transformer_layers: int\n                 ):\n        super().__init__()\n\n        self.context_length = context_length\n\n        if isinstance(vision_layers, (tuple, list)):\n            vision_heads = vision_width * 32 // 64\n            self.visual = ModifiedResNet(\n                layers=vision_layers,\n                output_dim=embed_dim,\n                heads=vision_heads,\n                input_resolution=image_resolution,\n                width=vision_width\n            )\n        else:\n            vision_heads = vision_width // 64\n            self.visual = VisionTransformer(\n                input_resolution=image_resolution,\n                patch_size=vision_patch_size,\n                width=vision_width,\n                layers=vision_layers,\n                heads=vision_heads,\n                output_dim=embed_dim\n            )\n\n        self.transformer = Transformer(\n            width=transformer_width,\n            layers=transformer_layers,\n            heads=transformer_heads,\n            attn_mask=self.build_attention_mask()\n        )\n\n        self.vocab_size = vocab_size\n        self.token_embedding = nn.Embedding(vocab_size, transformer_width)\n        self.positional_embedding = nn.Parameter(torch.empty(self.context_length, transformer_width))\n        self.ln_final = LayerNorm(transformer_width)\n\n        self.text_projection = nn.Parameter(torch.empty(transformer_width, embed_dim))\n        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n\n        self.initialize_parameters()\n\n    def initialize_parameters(self):\n        nn.init.normal_(self.token_embedding.weight, std=0.02)\n        nn.init.normal_(self.positional_embedding, std=0.01)\n\n        if isinstance(self.visual, ModifiedResNet):\n            if self.visual.attnpool is not None:\n                std = self.visual.attnpool.c_proj.in_features ** -0.5\n                nn.init.normal_(self.visual.attnpool.q_proj.weight, std=std)\n                nn.init.normal_(self.visual.attnpool.k_proj.weight, std=std)\n                nn.init.normal_(self.visual.attnpool.v_proj.weight, std=std)\n                nn.init.normal_(self.visual.attnpool.c_proj.weight, std=std)\n\n            for resnet_block in [self.visual.layer1, self.visual.layer2, self.visual.layer3, self.visual.layer4]:\n                for name, param in resnet_block.named_parameters():\n                    if name.endswith(\"bn3.weight\"):\n                        nn.init.zeros_(param)\n\n        proj_std = (self.transformer.width ** -0.5) * ((2 * self.transformer.layers) ** -0.5)\n        attn_std = self.transformer.width ** -0.5\n        fc_std = (2 * self.transformer.width) ** -0.5\n        for block in self.transformer.resblocks:\n            nn.init.normal_(block.attn.in_proj_weight, std=attn_std)\n            nn.init.normal_(block.attn.out_proj.weight, std=proj_std)\n            nn.init.normal_(block.mlp.c_fc.weight, std=fc_std)\n            nn.init.normal_(block.mlp.c_proj.weight, std=proj_std)\n\n        if self.text_projection is not None:\n            nn.init.normal_(self.text_projection, std=self.transformer.width ** -0.5)\n\n    def build_attention_mask(self):\n        # lazily create causal attention mask, with full attention between the vision tokens\n        # pytorch uses additive attention mask; fill with -inf\n        mask = torch.empty(self.context_length, self.context_length)\n        mask.fill_(float(\"-inf\"))\n        mask.triu_(1)  # zero out the lower diagonal\n        return mask\n\n    @property\n    def dtype(self):\n        return self.visual.conv1.weight.dtype\n\n    def encode_image(self, image):\n        return self.visual(image.type(self.dtype))\n\n    def encode_text(self, text):\n        x = self.token_embedding(text).type(self.dtype)  # [batch_size, n_ctx, d_model]\n\n        x = x + self.positional_embedding.type(self.dtype)\n        x = x.permute(1, 0, 2)  # NLD -> LND\n        x = self.transformer(x)\n        x = x.permute(1, 0, 2)  # LND -> NLD\n        x = self.ln_final(x).type(self.dtype)\n\n        # x.shape = [batch_size, n_ctx, transformer.width]\n        # take features from the eot embedding (eot_token is the highest number in each sequence)\n        x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ self.text_projection\n\n        return x\n\n    def forward(self, image, text):\n        image_features = self.encode_image(image)\n        text_features = self.encode_text(text)\n\n        # normalized features\n        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n\n        # cosine similarity as logits\n        logit_scale = self.logit_scale.exp()\n        logits_per_image = logit_scale * image_features @ text_features.t()\n        logits_per_text = logits_per_image.t()\n\n        # shape = [global_batch_size, global_batch_size]\n        return logits_per_image, logits_per_text\n\n\ndef convert_weights(model: nn.Module):\n    \"\"\"Convert applicable model parameters to fp16\"\"\"\n\n    def _convert_weights_to_fp16(l):\n        if isinstance(l, (nn.Conv1d, nn.Conv2d, nn.Linear)):\n            l.weight.data = l.weight.data.half()\n            if l.bias is not None:\n                l.bias.data = l.bias.data.half()\n\n        if isinstance(l, nn.MultiheadAttention):\n            for attr in [*[f\"{s}_proj_weight\" for s in [\"in\", \"q\", \"k\", \"v\"]], \"in_proj_bias\", \"bias_k\", \"bias_v\"]:\n                tensor = getattr(l, attr)\n                if tensor is not None:\n                    tensor.data = tensor.data.half()\n\n        for name in [\"text_projection\", \"proj\"]:\n            if hasattr(l, name):\n                attr = getattr(l, name)\n                if attr is not None:\n                    attr.data = attr.data.half()\n\n    model.apply(_convert_weights_to_fp16)\n\n\ndef build_model(state_dict: dict):\n    vit = \"visual.proj\" in state_dict\n\n    if vit:\n        vision_width = state_dict[\"visual.conv1.weight\"].shape[0]\n        vision_layers = len([k for k in state_dict.keys() if k.startswith(\"visual.\") and k.endswith(\".attn.in_proj_weight\")])\n        vision_patch_size = state_dict[\"visual.conv1.weight\"].shape[-1]\n        grid_size = round((state_dict[\"visual.positional_embedding\"].shape[0] - 1) ** 0.5)\n        image_resolution = vision_patch_size * grid_size\n    else:\n        counts: list = [len(set(k.split(\".\")[2] for k in state_dict if k.startswith(f\"visual.layer{b}\"))) for b in [1, 2, 3, 4]]\n        vision_layers = tuple(counts)\n        vision_width = state_dict[\"visual.layer1.0.conv1.weight\"].shape[0]\n        output_width = round((state_dict[\"visual.attnpool.positional_embedding\"].shape[0] - 1) ** 0.5)\n        vision_patch_size = None\n        assert output_width ** 2 + 1 == state_dict[\"visual.attnpool.positional_embedding\"].shape[0]\n        image_resolution = output_width * 32\n\n    embed_dim = state_dict[\"text_projection\"].shape[1]\n    context_length = state_dict[\"positional_embedding\"].shape[0]\n    vocab_size = state_dict[\"token_embedding.weight\"].shape[0]\n    transformer_width = state_dict[\"ln_final.weight\"].shape[0]\n    transformer_heads = transformer_width // 64\n    transformer_layers = len(set(k.split(\".\")[2] for k in state_dict if k.startswith(f\"transformer.resblocks\")))\n\n    model = CLIP(\n        embed_dim,\n        image_resolution, vision_layers, vision_width, vision_patch_size,\n        context_length, vocab_size, transformer_width, transformer_heads, transformer_layers\n    )\n\n    for key in [\"input_resolution\", \"context_length\", \"vocab_size\"]:\n        if key in state_dict:\n            del state_dict[key]\n\n    convert_weights(model)\n    model.load_state_dict(state_dict)\n    return model.eval()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T12:50:58.508515Z","iopub.execute_input":"2025-02-16T12:50:58.508739Z","iopub.status.idle":"2025-02-16T12:50:58.552525Z","shell.execute_reply.started":"2025-02-16T12:50:58.508721Z","shell.execute_reply":"2025-02-16T12:50:58.551817Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## clip.py","metadata":{}},{"cell_type":"code","source":"import hashlib\nimport os\nimport urllib\nimport warnings\nfrom typing import Any, Union, List\nimport torch\nfrom PIL import Image\nfrom torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\nfrom tqdm import tqdm\n\n\n\ntry:\n    from torchvision.transforms import InterpolationMode\n    BICUBIC = InterpolationMode.BICUBIC\nexcept ImportError:\n    BICUBIC = Image.BICUBIC","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T12:50:58.553625Z","iopub.execute_input":"2025-02-16T12:50:58.553849Z","iopub.status.idle":"2025-02-16T12:50:59.677639Z","shell.execute_reply.started":"2025-02-16T12:50:58.553829Z","shell.execute_reply":"2025-02-16T12:50:59.677026Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if torch.__version__.split(\".\") < [\"1\", \"7\", \"1\"]:\n    warnings.warn(\"PyTorch version 1.7.1 or higher is recommended\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T12:50:59.678431Z","iopub.execute_input":"2025-02-16T12:50:59.678729Z","iopub.status.idle":"2025-02-16T12:50:59.682513Z","shell.execute_reply.started":"2025-02-16T12:50:59.678709Z","shell.execute_reply":"2025-02-16T12:50:59.681727Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def _download(url: str, root: str):\n    os.makedirs(root, exist_ok=True)\n    filename = os.path.basename(url)\n\n    expected_sha256 = url.split(\"/\")[-2]\n    download_target = os.path.join(root, filename)\n\n    if os.path.exists(download_target) and not os.path.isfile(download_target):\n        raise RuntimeError(f\"{download_target} exists and is not a regular file\")\n\n    if os.path.isfile(download_target):\n        if hashlib.sha256(open(download_target, \"rb\").read()).hexdigest() == expected_sha256:\n            return download_target\n        else:\n            warnings.warn(f\"{download_target} exists, but the SHA256 checksum does not match; re-downloading the file\")\n\n    with urllib.request.urlopen(url) as source, open(download_target, \"wb\") as output:\n        with tqdm(total=int(source.info().get(\"Content-Length\")), ncols=80, unit='iB', unit_scale=True, unit_divisor=1024) as loop:\n            while True:\n                buffer = source.read(8192)\n                if not buffer:\n                    break\n\n                output.write(buffer)\n                loop.update(len(buffer))\n\n    if hashlib.sha256(open(download_target, \"rb\").read()).hexdigest() != expected_sha256:\n        raise RuntimeError(f\"Model has been downloaded but the SHA256 checksum does not not match\")\n\n    return download_target","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T12:50:59.683256Z","iopub.execute_input":"2025-02-16T12:50:59.683515Z","iopub.status.idle":"2025-02-16T12:50:59.697170Z","shell.execute_reply.started":"2025-02-16T12:50:59.683488Z","shell.execute_reply":"2025-02-16T12:50:59.696507Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def _convert_image_to_rgb(image):\n    return image.convert(\"RGB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T12:50:59.698080Z","iopub.execute_input":"2025-02-16T12:50:59.698291Z","iopub.status.idle":"2025-02-16T12:50:59.711716Z","shell.execute_reply.started":"2025-02-16T12:50:59.698260Z","shell.execute_reply":"2025-02-16T12:50:59.711113Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def _transform(n_px):\n    return Compose([\n        Resize(n_px, interpolation=BICUBIC),\n        CenterCrop(n_px),\n        _convert_image_to_rgb,\n        ToTensor(),\n        Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n    ])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T12:50:59.714260Z","iopub.execute_input":"2025-02-16T12:50:59.714449Z","iopub.status.idle":"2025-02-16T12:50:59.725257Z","shell.execute_reply.started":"2025-02-16T12:50:59.714432Z","shell.execute_reply":"2025-02-16T12:50:59.724487Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def available_models() -> List[str]:\n    \"\"\"Returns the names of available CLIP models\"\"\"\n    return list(_MODELS.keys())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T12:50:59.726762Z","iopub.execute_input":"2025-02-16T12:50:59.727070Z","iopub.status.idle":"2025-02-16T12:50:59.743009Z","shell.execute_reply.started":"2025-02-16T12:50:59.727040Z","shell.execute_reply":"2025-02-16T12:50:59.742257Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def patch_device(module):\n        try:\n            graphs = [module.graph] if hasattr(module, \"graph\") else []\n        except RuntimeError:\n            graphs = []\n\n        if hasattr(module, \"forward1\"):\n            graphs.append(module.forward1.graph)\n\n        for graph in graphs:\n            for node in graph.findAllNodes(\"prim::Constant\"):\n                if \"value\" in node.attributeNames() and str(node[\"value\"]).startswith(\"cuda\"):\n                    node.copyAttributes(device_node)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T12:50:59.743735Z","iopub.execute_input":"2025-02-16T12:50:59.743980Z","iopub.status.idle":"2025-02-16T12:50:59.756474Z","shell.execute_reply.started":"2025-02-16T12:50:59.743961Z","shell.execute_reply":"2025-02-16T12:50:59.755861Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load(name: str, device: Union[str, torch.device] = \"cuda\" if torch.cuda.is_available() else \"cpu\", jit: bool = False, download_root: str = None):\n    \"\"\"Load a CLIP model\n\n    Parameters\n    ----------\n    name : str\n        A model name listed by `clip.available_models()`, or the path to a model checkpoint containing the state_dict\n\n    device : Union[str, torch.device]\n        The device to put the loaded model\n\n    jit : bool\n        Whether to load the optimized JIT model or more hackable non-JIT model (default).\n\n    download_root: str\n        path to download the model files; by default, it uses \"~/.cache/clip\"\n\n    Returns\n    -------\n    model : torch.nn.Module\n        The CLIP model\n\n    preprocess : Callable[[PIL.Image], torch.Tensor]\n        A torchvision transform that converts a PIL image into a tensor that the returned model can take as its input\n    \"\"\"\n    if name in _MODELS:\n        model_path = _download(_MODELS[name], download_root or os.path.expanduser(\"~/.cache/clip\"))\n    elif os.path.isfile(name):\n        model_path = name\n    else:\n        raise RuntimeError(f\"Model {name} not found; available models = {available_models()}\")\n\n    try:\n        # loading JIT archive\n        model = torch.jit.load(model_path, map_location=device if jit else \"cpu\").eval()\n        state_dict = None\n    except RuntimeError:\n        # loading saved state dict\n        if jit:\n            warnings.warn(f\"File {model_path} is not a JIT archive. Loading as a state dict instead\")\n            jit = False\n        state_dict = torch.load(model_path, map_location=\"cpu\")\n\n    if not jit:\n        model = build_model(state_dict or model.state_dict()).to(device)\n        if str(device) == \"cpu\":\n            model.float()\n        return model, _transform(model.visual.input_resolution)\n\n    # patch the device names\n    device_holder = torch.jit.trace(lambda: torch.ones([]).to(torch.device(device)), example_inputs=[])\n    device_node = [n for n in device_holder.graph.findAllNodes(\"prim::Constant\") if \"Device\" in repr(n)][-1]\n\n    model.apply(patch_device)\n    patch_device(model.encode_image)\n    patch_device(model.encode_text)\n\n    # patch dtype to float32 on CPU\n    if str(device) == \"cpu\":\n        float_holder = torch.jit.trace(lambda: torch.ones([]).float(), example_inputs=[])\n        float_input = list(float_holder.graph.findNode(\"aten::to\").inputs())[1]\n        float_node = float_input.node()\n\n        def patch_float(module):\n            try:\n                graphs = [module.graph] if hasattr(module, \"graph\") else []\n            except RuntimeError:\n                graphs = []\n\n            if hasattr(module, \"forward1\"):\n                graphs.append(module.forward1.graph)\n\n            for graph in graphs:\n                for node in graph.findAllNodes(\"aten::to\"):\n                    inputs = list(node.inputs())\n                    for i in [1, 2]:  # dtype can be the second or third argument to aten::to()\n                        if inputs[i].node()[\"value\"] == 5:\n                            inputs[i].node().copyAttributes(float_node)\n\n        model.apply(patch_float)\n        patch_float(model.encode_image)\n        patch_float(model.encode_text)\n\n        model.float()\n\n    return model, _transform(model.input_resolution.item())\n\n\ndef tokenize(texts: Union[str, List[str]], context_length: int = 77, truncate: bool = False) -> torch.LongTensor:\n    \"\"\"\n    Returns the tokenized representation of given input string(s)\n\n    Parameters\n    ----------\n    texts : Union[str, List[str]]\n        An input string or a list of input strings to tokenize\n\n    context_length : int\n        The context length to use; all CLIP models use 77 as the context length\n\n    truncate: bool\n        Whether to truncate the text in case its encoding is longer than the context length\n\n    Returns\n    -------\n    A two-dimensional tensor containing the resulting tokens, shape = [number of input strings, context_length]\n    \"\"\"\n    if isinstance(texts, str):\n        texts = [texts]\n\n    sot_token = _tokenizer.encoder[\"<|startoftext|>\"]\n    eot_token = _tokenizer.encoder[\"<|endoftext|>\"]\n    all_tokens = [[sot_token] + _tokenizer.encode(text) + [eot_token] for text in texts]\n    result = torch.zeros(len(all_tokens), context_length, dtype=torch.long)\n\n    for i, tokens in enumerate(all_tokens):\n        if len(tokens) > context_length:\n            if truncate:\n                tokens = tokens[:context_length]\n                tokens[-1] = eot_token\n            else:\n                raise RuntimeError(f\"Input {texts[i]} is too long for context length {context_length}\")\n        result[i, :len(tokens)] = torch.tensor(tokens)\n\n    return result\n","metadata":{"id":"9uM4i71QtdsU","executionInfo":{"status":"ok","timestamp":1736782190915,"user_tz":180,"elapsed":989,"user":{"displayName":"Sergio Neres Pereira Junior","userId":"05770949936273593301"}},"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T12:50:59.757288Z","iopub.execute_input":"2025-02-16T12:50:59.757526Z","iopub.status.idle":"2025-02-16T12:50:59.775158Z","shell.execute_reply.started":"2025-02-16T12:50:59.757507Z","shell.execute_reply":"2025-02-16T12:50:59.774351Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"_tokenizer = SimpleTokenizer()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T12:50:59.775857Z","iopub.execute_input":"2025-02-16T12:50:59.776101Z","iopub.status.idle":"2025-02-16T12:50:59.912526Z","shell.execute_reply.started":"2025-02-16T12:50:59.776081Z","shell.execute_reply":"2025-02-16T12:50:59.911878Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"__all__ = [\"available_models\", \"load\", \"tokenize\"]\n\n_MODELS = {\n    \"RN50\": \"https://openaipublic.azureedge.net/clip/models/afeb0e10f9e5a86da6080e35cf09123aca3b358a0c3e3b6c78a7b63bc04b6762/RN50.pt\",\n    \"RN101\": \"https://openaipublic.azureedge.net/clip/models/8fa8567bab74a42d41c5915025a8e4538c3bdbe8804a470a72f30b0d94fab599/RN101.pt\",\n    \"RN50x4\": \"https://openaipublic.azureedge.net/clip/models/7e526bd135e493cef0776de27d5f42653e6b4c8bf9e0f653bb11773263205fdd/RN50x4.pt\",\n    \"RN50x16\": \"https://openaipublic.azureedge.net/clip/models/52378b407f34354e150460fe41077663dd5b39c54cd0bfd2b27167a4a06ec9aa/RN50x16.pt\",\n    \"ViT-B/32\": \"https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt\",\n    \"ViT-B/16\": \"https://openaipublic.azureedge.net/clip/models/5806e77cd80f8b59890b7e101eabd078d9fb84e6937f9e85e4ecb61988df416f/ViT-B-16.pt\",\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T12:50:59.913321Z","iopub.execute_input":"2025-02-16T12:50:59.913624Z","iopub.status.idle":"2025-02-16T12:50:59.919168Z","shell.execute_reply.started":"2025-02-16T12:50:59.913594Z","shell.execute_reply":"2025-02-16T12:50:59.918187Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Main code","metadata":{}},{"cell_type":"code","source":"import sys\nsys.path.append('/kaggle/input/cafe-repo/Generalizable-FER-main/code/clip')","metadata":{"id":"KbEsq3Rw0zdc","executionInfo":{"status":"ok","timestamp":1736782222337,"user_tz":180,"elapsed":6,"user":{"displayName":"Sergio Neres Pereira Junior","userId":"05770949936273593301"}},"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T12:50:59.920074Z","iopub.execute_input":"2025-02-16T12:50:59.920338Z","iopub.status.idle":"2025-02-16T12:50:59.937277Z","shell.execute_reply.started":"2025-02-16T12:50:59.920317Z","shell.execute_reply":"2025-02-16T12:50:59.936611Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport csv\nimport math\nimport random\nimport numpy as np\nimport pandas as pd\nimport argparse\nimport pickle\nimport torch\nimport torch.nn as nn\nfrom torchvision import transforms\nimport torchvision.models as models\nimport torch.utils.data as data\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport pickle\nfrom torch.autograd import Variable\nimport torch.utils.data as data\nimport pandas as pd\nimport random\nfrom torchvision import transforms","metadata":{"id":"xsWRxVafuQ3C","executionInfo":{"status":"ok","timestamp":1736782244220,"user_tz":180,"elapsed":21888,"user":{"displayName":"Sergio Neres Pereira Junior","userId":"05770949936273593301"}},"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T12:50:59.938105Z","iopub.execute_input":"2025-02-16T12:50:59.938362Z","iopub.status.idle":"2025-02-16T12:51:00.565140Z","shell.execute_reply.started":"2025-02-16T12:50:59.938341Z","shell.execute_reply":"2025-02-16T12:51:00.564229Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nclip_model, preprocess = load(\"ViT-B/32\", device=device)","metadata":{"id":"0ybRCZQUukWl","executionInfo":{"status":"ok","timestamp":1736782259010,"user_tz":180,"elapsed":14797,"user":{"displayName":"Sergio Neres Pereira Junior","userId":"05770949936273593301"}},"outputId":"3cc11ee8-a377-462a-9214-6b1f16824f2e","trusted":true,"execution":{"iopub.status.busy":"2025-02-16T12:51:00.565991Z","iopub.execute_input":"2025-02-16T12:51:00.566430Z","iopub.status.idle":"2025-02-16T12:51:07.942931Z","shell.execute_reply.started":"2025-02-16T12:51:00.566397Z","shell.execute_reply":"2025-02-16T12:51:07.942198Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"parser = argparse.ArgumentParser()\nparser.add_argument('--raf_path', type=str, default='../../data/raf-basic', help='raf_dataset_path')\nparser.add_argument('--resnet50_path', type=str, default='../../data/resnet50_ft_weight.pkl', help='pretrained_backbone_path')\nparser.add_argument('--label_path', type=str, default='list_patition_label.txt', help='label_path')\nparser.add_argument('--workers', type=int, default=2, help='number of workers')\nparser.add_argument('--batch_size', type=int, default=32, help='batch_size')\nparser.add_argument('--w', type=int, default=7, help='width of the attention map')\nparser.add_argument('--h', type=int, default=7, help='height of the attention map')\nparser.add_argument('--gpu', type=int, default=0, help='the number of the device')\nparser.add_argument('--lam', type=float, default=5, help='kl_lambda')\nparser.add_argument('--epochs', type=int, default=60, help='number of epochs')\nargs = parser.parse_args(args=[])","metadata":{"id":"8zH1mnyP_qbW","executionInfo":{"status":"ok","timestamp":1736782259432,"user_tz":180,"elapsed":425,"user":{"displayName":"Sergio Neres Pereira Junior","userId":"05770949936273593301"}},"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T12:51:07.943686Z","iopub.execute_input":"2025-02-16T12:51:07.943924Z","iopub.status.idle":"2025-02-16T12:51:07.950970Z","shell.execute_reply.started":"2025-02-16T12:51:07.943902Z","shell.execute_reply":"2025-02-16T12:51:07.950199Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Arquitetura do Modelo","metadata":{"id":"j7gTrxyDwX05"}},{"cell_type":"code","source":"def conv3x3(in_planes, out_planes, stride=1):\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)","metadata":{"id":"Uc94_GwGwSnP","executionInfo":{"status":"ok","timestamp":1736782259432,"user_tz":180,"elapsed":5,"user":{"displayName":"Sergio Neres Pereira Junior","userId":"05770949936273593301"}},"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T12:51:07.951677Z","iopub.execute_input":"2025-02-16T12:51:07.951899Z","iopub.status.idle":"2025-02-16T12:51:07.970048Z","shell.execute_reply.started":"2025-02-16T12:51:07.951880Z","shell.execute_reply":"2025-02-16T12:51:07.969428Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class BasicBlock(nn.Module):\n\n    expansion = 1\n\n    def __init__(self, in_channels, out_channels, stride = 1, downsample = False):\n        super().__init__()\n\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size = 3,\n                               stride = stride, padding = 1, bias = False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size = 3,\n                               stride = 1, padding = 1, bias = False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n\n        self.relu = nn.ReLU(inplace = True)\n\n        if downsample:\n            conv = nn.Conv2d(in_channels, out_channels, kernel_size = 1,\n                             stride = stride, bias = False)\n            bn = nn.BatchNorm2d(out_channels)\n            downsample = nn.Sequential(conv, bn)\n        else:\n            downsample = None\n\n        self.downsample = downsample\n\n    def forward(self, x):\n\n        i = x\n\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        x = self.conv2(x)\n        x = self.bn2(x)\n\n        if self.downsample is not None:\n            i = self.downsample(i)\n\n        x += i\n        x = self.relu(x)\n\n        return x","metadata":{"id":"TYyPd4MowTW4","executionInfo":{"status":"ok","timestamp":1736782259433,"user_tz":180,"elapsed":5,"user":{"displayName":"Sergio Neres Pereira Junior","userId":"05770949936273593301"}},"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T12:51:07.970776Z","iopub.execute_input":"2025-02-16T12:51:07.971017Z","iopub.status.idle":"2025-02-16T12:51:07.985677Z","shell.execute_reply.started":"2025-02-16T12:51:07.970998Z","shell.execute_reply":"2025-02-16T12:51:07.985054Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ResNet(nn.Module):\n    def __init__(self, block, n_blocks, channels, output_dim):\n        super().__init__()\n\n\n        self.in_channels = channels[0]\n\n        assert len(n_blocks) == len(channels) == 4\n\n        self.conv1 = nn.Conv2d(3, self.in_channels, kernel_size = 7, stride = 2, padding = 3, bias = False)\n        self.bn1 = nn.BatchNorm2d(self.in_channels)\n        self.relu = nn.ReLU(inplace = True)\n        self.maxpool = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)\n\n        self.layer1 = self.get_resnet_layer(block, n_blocks[0], channels[0])\n        self.layer2 = self.get_resnet_layer(block, n_blocks[1], channels[1], stride = 2)\n        self.layer3 = self.get_resnet_layer(block, n_blocks[2], channels[2], stride = 2)\n        self.layer4 = self.get_resnet_layer(block, n_blocks[3], channels[3], stride = 2)\n\n        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n        self.fc = nn.Linear(self.in_channels, output_dim)\n\n    def get_resnet_layer(self, block=BasicBlock, n_blocks=[2,2,2,2], channels=[64, 128, 256, 512], stride = 1):\n\n        layers = []\n\n        if self.in_channels != block.expansion * channels:\n            downsample = True\n        else:\n            downsample = False\n\n        layers.append(block(self.in_channels, channels, stride, downsample))\n\n        for i in range(1, n_blocks):\n            layers.append(block(block.expansion * channels, channels))\n\n        self.in_channels = block.expansion * channels\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        h = x.view(x.shape[0], -1)\n        x = self.fc(h)\n\n        return x, h","metadata":{"id":"-kvynhInwkJI","executionInfo":{"status":"ok","timestamp":1736782259918,"user_tz":180,"elapsed":490,"user":{"displayName":"Sergio Neres Pereira Junior","userId":"05770949936273593301"}},"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T12:51:07.986518Z","iopub.execute_input":"2025-02-16T12:51:07.986835Z","iopub.status.idle":"2025-02-16T12:51:08.002329Z","shell.execute_reply.started":"2025-02-16T12:51:07.986805Z","shell.execute_reply":"2025-02-16T12:51:08.001504Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Flatten(nn.Module):\n    def forward(self, input):\n        return input.view(input.size(0), -1)","metadata":{"id":"vcdd3Kpuwr4s","executionInfo":{"status":"ok","timestamp":1736782259919,"user_tz":180,"elapsed":3,"user":{"displayName":"Sergio Neres Pereira Junior","userId":"05770949936273593301"}},"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T12:51:08.003025Z","iopub.execute_input":"2025-02-16T12:51:08.003243Z","iopub.status.idle":"2025-02-16T12:51:08.019952Z","shell.execute_reply.started":"2025-02-16T12:51:08.003224Z","shell.execute_reply":"2025-02-16T12:51:08.019339Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def Mask(nb_batch):\n    bar = []\n    for i in range(7):\n        foo = [1] * 63 + [0] *  10\n        if i == 6:\n            foo = [1] * 64 + [0] *  10\n        random.shuffle(foo)  #### generate mask\n        bar += foo\n    bar = [bar for i in range(nb_batch)]\n    bar = np.array(bar).astype(\"float32\")\n    bar = bar.reshape(nb_batch,512,1,1)\n    bar = torch.from_numpy(bar)\n    bar = bar.cuda()\n    bar = Variable(bar)\n    return bar","metadata":{"id":"w7Ktp8OsTcjG","executionInfo":{"status":"ok","timestamp":1736782517023,"user_tz":180,"elapsed":271,"user":{"displayName":"Sergio Neres Pereira Junior","userId":"05770949936273593301"}},"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T12:51:08.020761Z","iopub.execute_input":"2025-02-16T12:51:08.020981Z","iopub.status.idle":"2025-02-16T12:51:08.033577Z","shell.execute_reply.started":"2025-02-16T12:51:08.020963Z","shell.execute_reply":"2025-02-16T12:51:08.032833Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"###### channel separation and channel diverse loss\ndef supervisor(x, targets, cnum):\n    branch = x\n    branch = branch.reshape(branch.size(0),branch.size(1), 1, 1)\n    branch = my_MaxPool2d(kernel_size=(1,cnum), stride=(1,cnum))(branch)\n    branch = branch.reshape(branch.size(0),branch.size(1), branch.size(2) * branch.size(3))\n    loss_2 = 1.0 - 1.0*torch.mean(torch.sum(branch,2))/cnum # set margin = 3.0\n\n    mask = Mask(x.size(0))\n    branch_1 = x.reshape(x.size(0),x.size(1), 1, 1) * mask\n    branch_1 = my_MaxPool2d(kernel_size=(1,cnum), stride=(1,cnum))(branch_1)\n    branch_1 = branch_1.view(branch_1.size(0), -1)\n    loss_1 = nn.CrossEntropyLoss()(branch_1, targets)\n    return [loss_1, loss_2]","metadata":{"id":"WiNmF39ESMlG","executionInfo":{"status":"ok","timestamp":1736782259919,"user_tz":180,"elapsed":3,"user":{"displayName":"Sergio Neres Pereira Junior","userId":"05770949936273593301"}},"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T12:51:08.036401Z","iopub.execute_input":"2025-02-16T12:51:08.036602Z","iopub.status.idle":"2025-02-16T12:51:08.049883Z","shell.execute_reply.started":"2025-02-16T12:51:08.036584Z","shell.execute_reply":"2025-02-16T12:51:08.049167Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.nn.modules.utils import _single, _pair, _triple\nfrom torch.nn.parameter import Parameter\n\nclass my_MaxPool2d(nn.Module):\n    def __init__(self, kernel_size, stride=None, padding=0, dilation=1,\n                 return_indices=False, ceil_mode=False):\n        super(my_MaxPool2d, self).__init__()\n        self.kernel_size = kernel_size\n        self.stride = stride or kernel_size\n        self.padding = padding\n        self.dilation = dilation\n        self.return_indices = return_indices\n        self.ceil_mode = ceil_mode\n\n    def forward(self, input):\n        input = input.transpose(3,1)\n\n\n        input = F.max_pool2d(input, self.kernel_size, self.stride,\n                            self.padding, self.dilation, self.ceil_mode,\n                            self.return_indices)\n        input = input.transpose(3,1).contiguous()\n\n        return input\n\n    def __repr__(self):\n        kh, kw = _pair(self.kernel_size)\n        dh, dw = _pair(self.stride)\n        padh, padw = _pair(self.padding)\n        dilh, dilw = _pair(self.dilation)\n        padding_str = ', padding=(' + str(padh) + ', ' + str(padw) + ')' \\\n            if padh != 0 or padw != 0 else ''\n        dilation_str = (', dilation=(' + str(dilh) + ', ' + str(dilw) + ')'\n                        if dilh != 0 and dilw != 0 else '')\n        ceil_str = ', ceil_mode=' + str(self.ceil_mode)\n        return self.__class__.__name__ + '(' \\\n            + 'kernel_size=(' + str(kh) + ', ' + str(kw) + ')' \\\n            + ', stride=(' + str(dh) + ', ' + str(dw) + ')' \\\n            + padding_str + dilation_str + ceil_str + ')'\n\n\nclass my_AvgPool2d(nn.Module):\n    def __init__(self, kernel_size, stride=None, padding=0, ceil_mode=False,\n                 count_include_pad=True):\n        super(my_AvgPool2d, self).__init__()\n        self.kernel_size = kernel_size\n        self.stride = stride or kernel_size\n        self.padding = padding\n        self.ceil_mode = ceil_mode\n        self.count_include_pad = count_include_pad\n\n    def forward(self, input):\n        input = input.transpose(3,1)\n        input = F.avg_pool2d(input, self.kernel_size, self.stride,\n                            self.padding, self.ceil_mode, self.count_include_pad)\n        input = input.transpose(3,1).contiguous()\n        return input\n\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(' \\\n            + 'kernel_size=' + str(self.kernel_size) \\\n            + ', stride=' + str(self.stride) \\\n            + ', padding=' + str(self.padding) \\\n            + ', ceil_mode=' + str(self.ceil_mode) \\\n            + ', count_include_pad=' + str(self.count_include_pad) + ')'","metadata":{"id":"fj5xkMsyTK68","executionInfo":{"status":"ok","timestamp":1736782522392,"user_tz":180,"elapsed":293,"user":{"displayName":"Sergio Neres Pereira Junior","userId":"05770949936273593301"}},"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T12:51:08.050768Z","iopub.execute_input":"2025-02-16T12:51:08.050989Z","iopub.status.idle":"2025-02-16T12:51:08.061463Z","shell.execute_reply.started":"2025-02-16T12:51:08.050971Z","shell.execute_reply":"2025-02-16T12:51:08.060689Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, pretrained=True, num_classes=7, drop_rate=0, model_path=None):\n        super(Model, self).__init__()\n\n        res18 = ResNet(block = BasicBlock, n_blocks = [2,2,2,2], channels = [64, 128, 256, 512], output_dim=1000)\n        msceleb_model = torch.load(model_path, map_location=torch.device('cpu'))\n        state_dict = msceleb_model['state_dict']\n        res18.load_state_dict(state_dict, strict=False)\n\n        self.drop_rate = drop_rate\n        self.features = nn.Sequential(*list(res18.children())[:-2])\n        self.features2 = nn.Sequential(*list(res18.children())[-2:-1])\n\n        fc_in_dim = list(res18.children())[-1].in_features  # original fc layer's in dimention 512\n        self.fc = nn.Linear(fc_in_dim, num_classes)  # new fc layer 512x7\n\n        self.parm={}\n        for name,parameters in self.fc.named_parameters():\n            print(name,':',parameters.size())\n            self.parm[name]=parameters\n\n    def forward(self, x, clip_model, targets, phase='train'):\n        with torch.no_grad():\n            image_features = clip_model.encode_image(x)\n\n        x = self.features(x)\n        feat = x\n\n        x = self.features2(x)\n        x = x.view(x.size(0), -1)\n        ################### sigmoid mask (important)\n        if phase=='train':\n            MC_loss = supervisor(image_features * torch.sigmoid(x), targets, cnum=73)\n\n        x = image_features * torch.sigmoid(x)\n        out = self.fc(x)\n\n        if phase=='train':\n            return out, MC_loss\n        else:\n            return out, out","metadata":{"id":"ahqTRFFSwsXl","executionInfo":{"status":"ok","timestamp":1736782371449,"user_tz":180,"elapsed":5,"user":{"displayName":"Sergio Neres Pereira Junior","userId":"05770949936273593301"}},"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T12:51:08.062268Z","iopub.execute_input":"2025-02-16T12:51:08.062561Z","iopub.status.idle":"2025-02-16T12:51:08.079182Z","shell.execute_reply.started":"2025-02-16T12:51:08.062511Z","shell.execute_reply":"2025-02-16T12:51:08.078498Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Treinamento E Teste Codigos","metadata":{"id":"lkEjrsCVwmUg"}},{"cell_type":"code","source":"def setup_seed(seed):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.backends.cudnn.deterministic = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T12:51:08.080134Z","iopub.execute_input":"2025-02-16T12:51:08.080425Z","iopub.status.idle":"2025-02-16T12:51:08.097178Z","shell.execute_reply.started":"2025-02-16T12:51:08.080394Z","shell.execute_reply":"2025-02-16T12:51:08.096441Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm import tqdm\n\ndef train(model, train_loader, optimizer, scheduler, device):\n  running_loss = 0.0\n  iter_cnt = 0\n  correct_sum = 0\n\n  model.to(device)\n  model.train()\n\n  total_loss = []\n  with tqdm(total=len(train_loader)) as pbar:\n      for batch_i, (imgs1, labels) in enumerate(train_loader):\n        imgs1 = imgs1.to(device)\n        labels = labels.to(device)\n\n        criterion = nn.CrossEntropyLoss(reduction='none')\n\n        output, MC_loss = model(imgs1, clip_model, labels, phase='train')\n\n        loss1 = nn.CrossEntropyLoss()(output, labels)\n\n        loss = loss1 + 5 * MC_loss[1] + 1.5 * MC_loss[0]\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        iter_cnt += 1\n        _, predicts = torch.max(output, 1)\n        correct_num = torch.eq(predicts, labels).sum()\n        correct_sum += correct_num\n        running_loss += loss\n\n        pbar.update(1)  # Update progress bar for each batch\n\n  scheduler.step()\n  running_loss = running_loss / iter_cnt\n  acc = correct_sum.float() / float(train_loader.dataset.__len__())\n  return acc, running_loss\n\nsetup_seed(3407)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T12:51:08.097983Z","iopub.execute_input":"2025-02-16T12:51:08.098262Z","iopub.status.idle":"2025-02-16T12:51:08.122147Z","shell.execute_reply.started":"2025-02-16T12:51:08.098225Z","shell.execute_reply":"2025-02-16T12:51:08.121276Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def test(model, test_loader, device):\n    with torch.no_grad():\n        model.eval()\n\n        running_loss = 0.0\n        iter_cnt = 0\n        correct_sum = 0\n        data_num = 0\n\n\n        for batch_i, (imgs1, labels) in enumerate(test_loader):\n            imgs1 = imgs1.to(device)\n            labels = labels.to(device)\n\n\n            outputs, _ = model(imgs1, clip_model, labels, phase='test')\n\n\n            loss = nn.CrossEntropyLoss()(outputs, labels)\n\n            iter_cnt += 1\n            _, predicts = torch.max(outputs, 1)\n\n            correct_num = torch.eq(predicts, labels).sum()\n            correct_sum += correct_num\n\n            running_loss += loss\n            data_num += outputs.size(0)\n\n        running_loss = running_loss / iter_cnt\n        test_acc = correct_sum.float() / float(data_num)\n        \n    return test_acc, running_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T12:51:08.123225Z","iopub.execute_input":"2025-02-16T12:51:08.123487Z","iopub.status.idle":"2025-02-16T12:51:08.131912Z","shell.execute_reply.started":"2025-02-16T12:51:08.123448Z","shell.execute_reply":"2025-02-16T12:51:08.130885Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_path = '/kaggle/input/cafe-repo/Generalizable-FER-main/models/resnet18_msceleb.pth'\nmodel = Model(model_path = model_path)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:54:55.172824Z","iopub.execute_input":"2025-02-16T17:54:55.173166Z","iopub.status.idle":"2025-02-16T17:54:55.469136Z","shell.execute_reply.started":"2025-02-16T17:54:55.173133Z","shell.execute_reply":"2025-02-16T17:54:55.468275Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.optim as optim\nfrom torch.optim.lr_scheduler import ExponentialLR\n\noptimizer = optim.Adam(model.parameters(), lr=0.0002, weight_decay=0.0001)\nscheduler = ExponentialLR(optimizer, gamma=0.9)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T12:51:09.851029Z","iopub.execute_input":"2025-02-16T12:51:09.851225Z","iopub.status.idle":"2025-02-16T12:51:09.855523Z","shell.execute_reply.started":"2025-02-16T12:51:09.851207Z","shell.execute_reply":"2025-02-16T12:51:09.854688Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"eval_transforms = transforms.Compose([\n    #transforms.ToPILImage(),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225])])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T12:51:09.856264Z","iopub.execute_input":"2025-02-16T12:51:09.856466Z","iopub.status.idle":"2025-02-16T12:51:09.870826Z","shell.execute_reply.started":"2025-02-16T12:51:09.856447Z","shell.execute_reply":"2025-02-16T12:51:09.870052Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_transforms = transforms.Compose([\n    #transforms.ToPILImage(),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225]),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomErasing(scale=(0.02, 0.25))\n    ])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T12:51:09.871556Z","iopub.execute_input":"2025-02-16T12:51:09.871815Z","iopub.status.idle":"2025-02-16T12:51:09.885011Z","shell.execute_reply.started":"2025-02-16T12:51:09.871766Z","shell.execute_reply":"2025-02-16T12:51:09.884287Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Caregamento de Dataset RafDB","metadata":{}},{"cell_type":"code","source":"import torch.utils.data as data\nimport cv2\nimport pandas as pd\nimport os\n# import image_utils\nimport random\nimport cv2\nimport numpy as np\n\n\n\nclass RafDataSet(data.Dataset):\n    def __init__(self, raf_path, idxs_raf, idxs_test, dataidxs=None, train=True, transform=None, basic_aug=False, download=False):\n        self.train = train\n        self.dataidxs = dataidxs\n        self.transform = transform\n        self.raf_path = raf_path\n        self.idxs_raf = idxs_raf\n        self.idxs_test = idxs_test\n\n        NAME_COLUMN = 0\n        LABEL_COLUMN = 1\n        df = pd.read_csv(os.path.join(self.raf_path, 'EmoLabel/list_patition_label.txt'), sep=' ', header=None)\n        if self.train:\n            dataset = df[df[NAME_COLUMN].str.startswith('train')]\n        else:\n            dataset = df[df[NAME_COLUMN].str.startswith('test')]\n        file_names = dataset.iloc[:, NAME_COLUMN].values\n        self.target = dataset.iloc[:, LABEL_COLUMN].astype(int).values - 1  # 0:Surprise, 1:Fear, 2:Disgust, 3:Happiness, 4:Sadness, 5:Anger, 6:Neutral\n        self.target = np.array(self.target)\n\n        self.file_paths = []\n        for f in file_names:    # use raf-db aligned images for training/testing\n            f = f.split(\".\")[0]\n            f = f + \"_aligned.jpg\"\n            print(f)\n            path = os.path.join(self.raf_path, 'Image/aligned', f)\n            self.file_paths.append(path)\n\n        self.basic_aug = basic_aug\n        ################\n        self.file_paths = np.array(self.file_paths)\n        if self.dataidxs is not None:\n            self.file_paths = self.file_paths[self.dataidxs]\n            self.target = self.target[self.dataidxs]\n        else:\n            self.file_paths = self.file_paths\n        self.file_paths = self.file_paths.tolist()\n\n\n    def __len__(self):\n        return len(self.file_paths)\n\n    def get_labels(self):\n        return self.target\n\n    def __getitem__(self, idx):\n        path = self.file_paths[idx]\n        sample = cv2.imread(path)\n        sample = sample[:, :, ::-1]  # BGR to RGB (Optional)\n        target = self.target[idx]\n\n        target = self.idxs_test[self.idxs_raf[target]]\n        \n        if self.transform is not None:\n            \n            sample = Image.fromarray(sample.copy())  # Convert NumPy array to PIL image\n            sample = self.transform(sample)\n        \n        return sample, target  # , idx (Optional to return index)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T13:14:24.771422Z","iopub.execute_input":"2025-02-16T13:14:24.771719Z","iopub.status.idle":"2025-02-16T13:14:24.781164Z","shell.execute_reply.started":"2025-02-16T13:14:24.771696Z","shell.execute_reply":"2025-02-16T13:14:24.780155Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"emotion_to_index = {\n            \"surprise\": 0,\n            \"fear\": 1,\n            \"disgust\": 2,\n            \"happiness\": 3,\n            \"sadness\": 4,\n            \"angry\": 5,\n            \"neutral\": 6\n        }\n\nindex_to_emotion = {v: k for k, v in emotion_to_index.items()}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T11:44:06.852846Z","iopub.execute_input":"2025-02-15T11:44:06.853186Z","iopub.status.idle":"2025-02-15T11:44:06.857339Z","shell.execute_reply.started":"2025-02-15T11:44:06.853160Z","shell.execute_reply":"2025-02-15T11:44:06.856442Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_dataset = RafDataSet('/kaggle/input/eacdata/raf-basic', idxs_raf=index_to_emotion, idxs_test=emotion_to_index, train=False, transform=eval_transforms)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.batch_size,\n                                          shuffle=False,\n                                          num_workers=args.workers,\n                                          pin_memory=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T11:44:08.065590Z","iopub.execute_input":"2025-02-15T11:44:08.065894Z","iopub.status.idle":"2025-02-15T11:44:08.472383Z","shell.execute_reply.started":"2025-02-15T11:44:08.065867Z","shell.execute_reply":"2025-02-15T11:44:08.471458Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = RafDataSet('/kaggle/input/eacdata/raf-basic', train=True, transform=train_transforms)\ntrain_loader = torch.utils.data.DataLoader(train_dataset,\n                                           batch_size=args.batch_size,\n                                           #batch_size=1,\n                                           shuffle=True,\n                                           num_workers=args.workers,\n                                           pin_memory=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T00:59:22.484685Z","iopub.execute_input":"2025-02-06T00:59:22.484973Z","iopub.status.idle":"2025-02-06T00:59:23.940094Z","shell.execute_reply.started":"2025-02-06T00:59:22.484950Z","shell.execute_reply":"2025-02-06T00:59:23.939461Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Obtendo o batch de imagens e rótulos\nfor images, labels in test_loader:\n    # Se você quiser mostrar apenas um batch\n    break\n\n# Definindo o layout para 4 linhas e 8 colunas\nfig, axes = plt.subplots(4, 8, figsize=(20, 10))  # 4x8 layout\naxes = axes.flatten()  # Flatten para facilitar a iteração\n\n# Loop para exibir as imagens no grid\nfor i, (img, label) in enumerate(zip(images, labels)):\n    if i >= len(axes):  # Se houver mais imagens do que subgráficos\n        break\n\n    # Convertendo a imagem para numpy e normalizando\n    img_np = img.permute(1, 2, 0).numpy()\n    img_np = (img_np - img_np.min()) / (img_np.max() - img_np.min())\n\n    # Exibindo a imagem\n    ax = axes[i]\n    ax.imshow(img_np)\n    ax.axis('off')  # Desativar os eixos\n\n    # Usando o mapa de rótulos para mostrar o nome da emoção\n    label_name = index_to_emotion[label.item()]\n    ax.set_title(f\"{label_name}\", fontsize=10)  # Título com o nome do label\n\n# Ajustar o layout para não sobrepor as imagens\nplt.tight_layout()\nplt.show()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T11:44:12.041224Z","iopub.execute_input":"2025-02-15T11:44:12.041679Z","iopub.status.idle":"2025-02-15T11:44:15.993024Z","shell.execute_reply.started":"2025-02-15T11:44:12.041631Z","shell.execute_reply":"2025-02-15T11:44:15.991648Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_acc = 0\npatience = 5  # Number of epochs to wait for improvement\nno_improvement = 0\n\nfor i in range(1, args.epochs + 1):\n    train_acc, train_loss = train(model, train_loader, optimizer, scheduler, device)\n    test_acc, test_loss = test(model, test_loader, device)\n    print('epoch: ', i, 'acc_test: ', test_acc, 'acc_train: ', train_acc)\n\n    # Early stopping logic with patience\n    if test_acc > best_acc:\n        best_acc = test_acc\n        no_improvement = 0  # Reset patience counter on improvement\n        torch.save({'model_state_dict': model.state_dict(),}, \"/kaggle/working/ours_best_RAFDB.pth\")\n    else:\n        no_improvement += 1  # Increment patience counter on no improvement\n\n    if no_improvement == patience:\n        print(f\"Early stopping after {i} epochs with no improvement in test accuracy\")\n        break  # Exit the training loop if patience is exhausted\n\n    torch.save({'model_state_dict': model.state_dict(),}, \"/kaggle/working/ours_final_RAFDB.pth\")\n    with open('results.txt', 'a') as f:\n        f.write(str(i)+'_'+str(test_acc)+'\\n')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T00:59:27.061119Z","iopub.execute_input":"2025-02-06T00:59:27.061412Z","iopub.status.idle":"2025-02-06T01:25:50.478107Z","shell.execute_reply.started":"2025-02-06T00:59:27.061388Z","shell.execute_reply":"2025-02-06T01:25:50.477077Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Caregamento de Dataset FER+","metadata":{"id":"YblEzXiawbnA"}},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom torch.utils.data import Dataset\nfrom PIL import Image\n\n\nclass FERPlusDataset(Dataset):\n    def __init__(self, root_dir, idxs_fer, idxs_test, subset=\"FER2013Train\", transform=None):\n        \"\"\"\n        Classe para lidar com o dataset FERPlus.\n\n        Args:\n            root_dir (str): Diretório raiz do dataset (ex: 'FER2013Plus').\n            subset (str): Subconjunto a ser usado ('FER2013Train', 'FER2013Test', 'FER2013Valid').\n            transform (callable, optional): Transformações para aplicar nas imagens.\n        \"\"\"\n        self.root_dir = root_dir\n        self.subset = subset\n        self.transform = transform\n        self.idxs_fer = idxs_fer\n        self.idxs_test = idxs_test\n\n        # Caminhos para imagens e labels\n        self.images_dir = os.path.join(root_dir, \"Images\", subset)\n        self.labels_path = os.path.join(root_dir, \"Labels\", subset, \"label.csv\")\n\n        # Carregar o arquivo de labels\n        if not os.path.exists(self.labels_path):\n            raise FileNotFoundError(f\"Arquivo de labels não encontrado: {self.labels_path}\")\n\n        self.columns = [\n            \"image_name\", \"format\", \"neutral\", \"happiness\", \"surprise\", \"sadness\",\n            \"anger\", \"disgust\", \"fear\", \"contempt\", \"unknown\", \"NF\"\n        ]\n\n        self.labels = pd.read_csv(self.labels_path, header=None, names=self.columns)\n\n        # Validar se os arquivos de imagem existem\n        self.image_files = self.labels['image_name']\n\n        # Dicionário para mapear emoções para índices\n        self.emotion_to_index = {\n            \"neutral\": 0,\n            \"happiness\": 1,\n            \"surprise\": 2,\n            \"sadness\": 3,\n            \"anger\": 4,\n            \"disgust\": 5,\n            \"fear\": 6\n        }\n\n    def get_single_label_filtered(self, row):\n        \"\"\"\n        Obtém o índice do rótulo mais votado entre as emoções, excluindo \"unknown\" e \"NF\".\n\n        Args:\n            row (pd.Series): Linha do DataFrame de rótulos.\n\n        Returns:\n            int: Índice do rótulo mais votado.\n        \"\"\"\n        # Filtrar rótulos \"unknown\" e \"NF\"\n        emotion_columns = [\"neutral\", \"happiness\", \"surprise\", \"sadness\",\n                           \"anger\", \"disgust\", \"fear\"]\n        # Obter o nome do rótulo mais votado\n        emotion_name = row[emotion_columns].idxmax()\n        # Retornar o índice correspondente\n        return self.emotion_to_index[emotion_name]\n\n    def __len__(self):\n        #return 1\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        if isinstance(idx, slice):\n            raise NotImplementedError(\"Slices não são suportados nesta implementação.\")\n\n        # Obter caminho da imagem\n        img_name = self.image_files[idx]\n        img_path = os.path.join(self.images_dir, img_name)\n\n        # Carregar imagem\n        image = Image.open(img_path).convert(\"RGB\")\n\n        # Aplicar transformações se existirem\n        if self.transform:\n            image = self.transform(image)\n        \n        # Obter o rótulo correspondente\n        label_row = self.labels.iloc[idx]\n        single_label = self.get_single_label_filtered(label_row)\n\n        single_label = self.idxs_test[self.idxs_fer[single_label]]\n\n        return image, single_label\n","metadata":{"id":"n2feF-1szmkc","executionInfo":{"status":"ok","timestamp":1736782274014,"user_tz":180,"elapsed":4,"user":{"displayName":"Sergio Neres Pereira Junior","userId":"05770949936273593301"}},"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T13:20:27.451394Z","iopub.execute_input":"2025-02-16T13:20:27.451744Z","iopub.status.idle":"2025-02-16T13:20:27.460586Z","shell.execute_reply.started":"2025-02-16T13:20:27.451714Z","shell.execute_reply":"2025-02-16T13:20:27.459854Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"root_dir = \"/kaggle/input/ferplus/FER2013Plus\"","metadata":{"id":"CY0B9YMxzq57","executionInfo":{"status":"ok","timestamp":1736782274341,"user_tz":180,"elapsed":331,"user":{"displayName":"Sergio Neres Pereira Junior","userId":"05770949936273593301"}},"outputId":"32036971-5a85-42cb-fcae-e14a65b2512c","trusted":true,"execution":{"iopub.status.busy":"2025-02-16T13:20:27.888011Z","iopub.execute_input":"2025-02-16T13:20:27.888291Z","iopub.status.idle":"2025-02-16T13:20:27.892025Z","shell.execute_reply.started":"2025-02-16T13:20:27.888270Z","shell.execute_reply":"2025-02-16T13:20:27.891152Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"emotion_to_index = {\n            \"neutral\": 0,\n            \"happiness\": 1,\n            \"surprise\": 2,\n            \"sadness\": 3,\n            \"anger\": 4,\n            \"disgust\": 5,\n            \"fear\": 6\n        }\n\nindex_to_emotion = {v: k for k, v in emotion_to_index.items()}","metadata":{"id":"licDm86q4Xho","executionInfo":{"status":"ok","timestamp":1736782274341,"user_tz":180,"elapsed":3,"user":{"displayName":"Sergio Neres Pereira Junior","userId":"05770949936273593301"}},"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T13:31:49.701324Z","iopub.execute_input":"2025-02-16T13:31:49.701657Z","iopub.status.idle":"2025-02-16T13:31:49.706147Z","shell.execute_reply.started":"2025-02-16T13:31:49.701624Z","shell.execute_reply":"2025-02-16T13:31:49.705246Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = FERPlusDataset(root_dir=root_dir, idxs_fer=index_to_emotion, idxs_test=emotion_to_index, subset=\"FER2013Train\", transform=train_transforms)\ntrain_loader = torch.utils.data.DataLoader(train_dataset,\n                                           batch_size=args.batch_size,\n                                           #batch_size=1,\n                                           shuffle=True,\n                                           num_workers=args.workers,\n                                           pin_memory=True)","metadata":{"id":"yl2C2-Xv39u5","executionInfo":{"status":"ok","timestamp":1736782274341,"user_tz":180,"elapsed":4,"user":{"displayName":"Sergio Neres Pereira Junior","userId":"05770949936273593301"}},"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T13:31:49.997326Z","iopub.execute_input":"2025-02-16T13:31:49.997620Z","iopub.status.idle":"2025-02-16T13:31:50.045015Z","shell.execute_reply.started":"2025-02-16T13:31:49.997597Z","shell.execute_reply":"2025-02-16T13:31:50.044132Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_dataset = FERPlusDataset(root_dir=root_dir, idxs_fer=index_to_emotion, idxs_test=emotion_to_index, subset=\"FER2013Test\", transform=eval_transforms)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.batch_size,\n                                          shuffle=False,\n                                          num_workers=args.workers,\n                                          pin_memory=True)","metadata":{"id":"ut6ROsPfxglo","executionInfo":{"status":"ok","timestamp":1736705738313,"user_tz":180,"elapsed":254,"user":{"displayName":"Sergio Neres Pereira Junior","userId":"05770949936273593301"}},"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T13:31:50.267263Z","iopub.execute_input":"2025-02-16T13:31:50.267553Z","iopub.status.idle":"2025-02-16T13:31:50.279552Z","shell.execute_reply.started":"2025-02-16T13:31:50.267528Z","shell.execute_reply":"2025-02-16T13:31:50.278776Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Obtendo o batch de imagens e rótulos\nfor images, labels in train_loader:\n    # Se você quiser mostrar apenas um batch\n    break\n\n# Definindo o layout para 4 linhas e 8 colunas\nfig, axes = plt.subplots(4, 8, figsize=(20, 10))  # 4x8 layout\naxes = axes.flatten()  # Flatten para facilitar a iteração\n\n# Loop para exibir as imagens no grid\nfor i, (img, label) in enumerate(zip(images, labels)):\n    if i >= len(axes):  # Se houver mais imagens do que subgráficos\n        break\n\n    # Convertendo a imagem para numpy e normalizando\n    img_np = img.permute(1, 2, 0).numpy()\n    img_np = (img_np - img_np.min()) / (img_np.max() - img_np.min())\n\n    # Exibindo a imagem\n    ax = axes[i]\n    ax.imshow(img_np)\n    ax.axis('off')  # Desativar os eixos\n\n    # Usando o mapa de rótulos para mostrar o nome da emoção\n    label_name = index_to_emotion[label.item()]\n    ax.set_title(f\"{label_name}\", fontsize=10)  # Título com o nome do label\n\n# Ajustar o layout para não sobrepor as imagens\nplt.tight_layout()\nplt.show()","metadata":{"id":"LOT8SIFf3yF3","executionInfo":{"status":"ok","timestamp":1736782280028,"user_tz":180,"elapsed":5689,"user":{"displayName":"Sergio Neres Pereira Junior","userId":"05770949936273593301"}},"outputId":"cee5b32e-3b5f-4048-ad40-477b63a851ad","trusted":true,"execution":{"iopub.status.busy":"2025-02-16T13:31:52.501977Z","iopub.execute_input":"2025-02-16T13:31:52.502274Z","iopub.status.idle":"2025-02-16T13:31:55.469078Z","shell.execute_reply.started":"2025-02-16T13:31:52.502251Z","shell.execute_reply":"2025-02-16T13:31:55.468077Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_acc = 0\npatience = 5  # Number of epochs to wait for improvement\nno_improvement = 0\n\nfor i in range(1, args.epochs + 1):\n    train_acc, train_loss = train(model, train_loader, optimizer, scheduler, device)\n    test_acc, test_loss = test(model, test_loader, device)\n    print('epoch: ', i, 'acc_test: ', test_acc, 'acc_train: ', train_acc)\n\n    # Early stopping logic with patience\n    if test_acc > best_acc:\n        best_acc = test_acc\n        no_improvement = 0  # Reset patience counter on improvement\n        torch.save({'model_state_dict': model.state_dict(),}, \"/kaggle/working/ours_best_FERPlus.pth\")\n    else:\n        continue\n        #no_improvement += 1  # Increment patience counter on no improvement\n\n    if no_improvement == patience:\n        print(f\"Early stopping after {i} epochs with no improvement in test accuracy\")\n        break  # Exit the training loop if patience is exhausted\n\n    torch.save({'model_state_dict': model.state_dict(),}, \"/kaggle/working/ours_final_FERPlus.pth\")\n    with open('results.txt', 'a') as f:\n        f.write(str(i)+'_'+str(test_acc)+'\\n')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T13:31:56.805035Z","iopub.execute_input":"2025-02-16T13:31:56.805348Z","iopub.status.idle":"2025-02-16T14:14:55.270371Z","shell.execute_reply.started":"2025-02-16T13:31:56.805318Z","shell.execute_reply":"2025-02-16T14:14:55.269015Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### test on AffectNet","metadata":{"id":"7dbe42be"}},{"cell_type":"code","source":"emotion_to_index = {\n            \"angry\": 0,\n            \"disgust\": 1,\n            \"fear\": 2,\n            \"happiness\": 3,\n            \"sadness\": 4,\n            \"surprise\": 5,\n            \"neutral\": 6\n        }\n\nindex_to_emotion = {v: k for k, v in emotion_to_index.items()}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:15:49.515628Z","iopub.execute_input":"2025-02-16T14:15:49.515971Z","iopub.status.idle":"2025-02-16T14:15:49.520340Z","shell.execute_reply.started":"2025-02-16T14:15:49.515939Z","shell.execute_reply":"2025-02-16T14:15:49.519326Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass AffectNetDataset(Dataset):\n    def __init__(self, root_dir, idx_aff, idx_test, split=\"train\", transform=None):\n        \"\"\"\n        Args:\n            root_dir (str): Diretório raiz contendo as pastas train, test e valid.\n            split (str): Qual partição carregar (\"train\", \"test\" ou \"valid\").\n            transform (callable, optional): Transformações a serem aplicadas às imagens.\n        \"\"\"\n        self.root_dir = os.path.join(root_dir, split)\n        self.transform = transform\n        self.idxs_test = idx_test\n        self.idxs_aff = idx_aff\n        \n        self.class_map = {\n            \"angry\": 0,\n            \"disgust\": 1,\n            \"fear\": 2,\n            \"happiness\": 3,\n            \"sadness\": 4,\n            \"surprise\": 5,\n            \"neutral\": 6\n        }\n        \n        self.samples = []\n        for class_idx in range(len(self.class_map)):  # Pastas nomeadas por índice\n            class_path = os.path.join(self.root_dir, str(class_idx))\n            if os.path.exists(class_path):\n                for filename in os.listdir(class_path):\n                    if filename.endswith(('.png', '.jpg', '.jpeg')):  # Filtra apenas imagens\n                        self.samples.append((os.path.join(class_path, filename), class_idx))\n    \n    def __len__(self):\n        return len(self.samples)\n    \n    def __getitem__(self, idx):\n        img_path, label = self.samples[idx]\n        image = Image.open(img_path).convert(\"RGB\")\n\n        label = self.idxs_test[self.idxs_aff[label]]\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        return image, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:15:50.132284Z","iopub.execute_input":"2025-02-16T14:15:50.132575Z","iopub.status.idle":"2025-02-16T14:15:50.139618Z","shell.execute_reply.started":"2025-02-16T14:15:50.132542Z","shell.execute_reply":"2025-02-16T14:15:50.138826Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"root_dir = \"/kaggle/input/affectnetaligned/AffectNetCustom\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:15:52.954467Z","iopub.execute_input":"2025-02-16T14:15:52.954826Z","iopub.status.idle":"2025-02-16T14:15:52.958319Z","shell.execute_reply.started":"2025-02-16T14:15:52.954764Z","shell.execute_reply":"2025-02-16T14:15:52.957613Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = AffectNetDataset(root_dir=root_dir, idx_aff=index_to_emotion, idx_test=emotion_to_index, split=\"train\", transform=train_transforms)\ntrain_loader = torch.utils.data.DataLoader(train_dataset,\n                                           batch_size=args.batch_size,\n                                           #batch_size=1,\n                                           shuffle=True,\n                                           num_workers=args.workers,\n                                           pin_memory=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:15:53.067773Z","iopub.execute_input":"2025-02-16T14:15:53.068026Z","iopub.status.idle":"2025-02-16T14:15:53.427569Z","shell.execute_reply.started":"2025-02-16T14:15:53.068005Z","shell.execute_reply":"2025-02-16T14:15:53.426868Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_dataset = AffectNetDataset(root_dir=root_dir, idx_aff=index_to_emotion, idx_test=emotion_to_index, split=\"test\", transform=eval_transforms)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.batch_size,\n                                          shuffle=False,\n                                          num_workers=args.workers,\n                                          pin_memory=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:15:56.829947Z","iopub.execute_input":"2025-02-16T14:15:56.830234Z","iopub.status.idle":"2025-02-16T14:15:56.846617Z","shell.execute_reply.started":"2025-02-16T14:15:56.830212Z","shell.execute_reply":"2025-02-16T14:15:56.845892Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Obtendo o batch de imagens e rótulos\nfor images, labels in train_loader:\n    # Se você quiser mostrar apenas um batch\n    break\n\n# Definindo o layout para 4 linhas e 8 colunas\nfig, axes = plt.subplots(4, 8, figsize=(20, 10))  # 4x8 layout\naxes = axes.flatten()  # Flatten para facilitar a iteração\n\n# Loop para exibir as imagens no grid\nfor i, (img, label) in enumerate(zip(images, labels)):\n    if i >= len(axes):  # Se houver mais imagens do que subgráficos\n        break\n\n    # Convertendo a imagem para numpy e normalizando\n    img_np = img.permute(1, 2, 0).numpy()\n    img_np = (img_np - img_np.min()) / (img_np.max() - img_np.min())\n\n    # Exibindo a imagem\n    ax = axes[i]\n    ax.imshow(img_np)\n    ax.axis('off')  # Desativar os eixos\n\n    # Usando o mapa de rótulos para mostrar o nome da emoção\n    label_name = index_to_emotion[label.item()]\n    ax.set_title(f\"{label_name}\", fontsize=10)  # Título com o nome do label\n\n# Ajustar o layout para não sobrepor as imagens\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:15:59.256663Z","iopub.execute_input":"2025-02-16T14:15:59.256979Z","iopub.status.idle":"2025-02-16T14:16:03.017697Z","shell.execute_reply.started":"2025-02-16T14:15:59.256952Z","shell.execute_reply":"2025-02-16T14:16:03.016798Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_acc = 0\npatience = 5  # Number of epochs to wait for improvement\nno_improvement = 0\n\nfor i in range(1, args.epochs + 1):\n    train_acc, train_loss = train(model, train_loader, optimizer, scheduler, device)\n    test_acc, test_loss = test(model, test_loader, device)\n    print('epoch: ', i, 'acc_test: ', test_acc, 'acc_train: ', train_acc)\n\n    # Early stopping logic with patience\n    if test_acc > best_acc:\n        best_acc = test_acc\n        no_improvement = 0  # Reset patience counter on improvement\n        torch.save({'model_state_dict': model.state_dict(),}, \"/kaggle/working/ours_best_AffectNet.pth\")\n    else:\n        continue\n        #no_improvement += 1  # Increment patience counter on no improvement\n\n    if no_improvement == patience:\n        print(f\"Early stopping after {i} epochs with no improvement in test accuracy\")\n        break  # Exit the training loop if patience is exhausted\n\n    torch.save({'model_state_dict': model.state_dict(),}, \"/kaggle/working/ours_final_AffectNet.pth\")\n    with open('results.txt', 'a') as f:\n        f.write(str(i)+'_'+str(test_acc)+'\\n')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T16:44:59.457750Z","iopub.execute_input":"2025-02-16T16:44:59.458087Z","iopub.status.idle":"2025-02-16T17:38:38.751415Z","shell.execute_reply.started":"2025-02-16T16:44:59.458057Z","shell.execute_reply":"2025-02-16T17:38:38.749499Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### train on MMADataset","metadata":{}},{"cell_type":"code","source":"emotion_to_index = {\n            \"angry\": 0,\n            \"disgust\": 1,\n            \"fear\": 2,\n            \"happiness\": 3,\n            \"sadness\": 5,\n            \"surprise\": 6,\n            \"neutral\": 4\n        }\n\nindex_to_emotion = {v: k for k, v in emotion_to_index.items()}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T19:12:09.380941Z","iopub.execute_input":"2025-02-15T19:12:09.381244Z","iopub.status.idle":"2025-02-15T19:12:09.385191Z","shell.execute_reply.started":"2025-02-15T19:12:09.381221Z","shell.execute_reply":"2025-02-15T19:12:09.384438Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MMADataset(Dataset):\n    def __init__(self, root_dir, idx_mma, idx_test, split=\"train\", transform=None):\n        \"\"\"\n        Args:\n            root_dir (str): Diretório raiz contendo as pastas train, test e valid.\n            split (str): Qual partição carregar (\"train\", \"test\" ou \"valid\").\n            transform (callable, optional): Transformações a serem aplicadas às imagens.\n        \"\"\"\n        self.root_dir = os.path.join(root_dir, split)\n        self.transform = transform\n        self.classes = sorted(os.listdir(self.root_dir))  # Lista de emoções\n        self.idxs_test = idx_test\n        self.idxs_mma = idx_mma\n        \n        self.samples = []\n        for class_idx, class_name in enumerate(self.classes):\n            class_path = os.path.join(self.root_dir, class_name)\n            for filename in os.listdir(class_path):\n                if filename.endswith(('.png', '.jpg', '.jpeg')):  # Filtra apenas imagens\n                    self.samples.append((os.path.join(class_path, filename), class_idx))\n    \n    def __len__(self):\n        return len(self.samples)\n    \n    def __getitem__(self, idx):\n        img_path, label = self.samples[idx]\n        image = Image.open(img_path).convert(\"RGB\")\n        \n        if self.transform:\n            image = self.transform(image)\n\n        label = self.idxs_test[self.idxs_mma[label]]\n        \n        return image, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T13:14:52.048374Z","iopub.execute_input":"2025-02-16T13:14:52.048670Z","iopub.status.idle":"2025-02-16T13:14:52.055028Z","shell.execute_reply.started":"2025-02-16T13:14:52.048647Z","shell.execute_reply":"2025-02-16T13:14:52.054123Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"root_dir = \"/kaggle/input/mma-facial-expression/MMAFEDB\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T19:12:26.046406Z","iopub.execute_input":"2025-02-15T19:12:26.046796Z","iopub.status.idle":"2025-02-15T19:12:26.051148Z","shell.execute_reply.started":"2025-02-15T19:12:26.046761Z","shell.execute_reply":"2025-02-15T19:12:26.050035Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = MMADataset(root_dir=root_dir, idx_mma=index_to_emotion, idx_test=emotion_to_index, split=\"train\", transform=train_transforms)\ntrain_loader = torch.utils.data.DataLoader(train_dataset,\n                                           batch_size=args.batch_size,\n                                           #batch_size=1,\n                                           shuffle=True,\n                                           num_workers=args.workers,\n                                           pin_memory=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T19:13:08.638186Z","iopub.execute_input":"2025-02-15T19:13:08.638473Z","iopub.status.idle":"2025-02-15T19:13:09.493885Z","shell.execute_reply.started":"2025-02-15T19:13:08.638453Z","shell.execute_reply":"2025-02-15T19:13:09.492920Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_dataset = MMADataset(root_dir=root_dir, idx_mma=index_to_emotion, idx_test=emotion_to_index, split=\"test\", transform=eval_transforms)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.batch_size,\n                                          shuffle=False,\n                                          num_workers=args.workers,\n                                          pin_memory=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T19:13:18.121653Z","iopub.execute_input":"2025-02-15T19:13:18.121991Z","iopub.status.idle":"2025-02-15T19:13:18.304887Z","shell.execute_reply.started":"2025-02-15T19:13:18.121933Z","shell.execute_reply":"2025-02-15T19:13:18.304047Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Obtendo o batch de imagens e rótulos\nfor images, labels in train_loader:\n    # Se você quiser mostrar apenas um batch\n    break\n\n# Definindo o layout para 4 linhas e 8 colunas\nfig, axes = plt.subplots(4, 8, figsize=(20, 10))  # 4x8 layout\naxes = axes.flatten()  # Flatten para facilitar a iteração\n\n# Loop para exibir as imagens no grid\nfor i, (img, label) in enumerate(zip(images, labels)):\n    if i >= len(axes):  # Se houver mais imagens do que subgráficos\n        break\n\n    # Convertendo a imagem para numpy e normalizando\n    img_np = img.permute(1, 2, 0).numpy()\n    img_np = (img_np - img_np.min()) / (img_np.max() - img_np.min())\n\n    # Exibindo a imagem\n    ax = axes[i]\n    ax.imshow(img_np)\n    ax.axis('off')  # Desativar os eixos\n\n    # Usando o mapa de rótulos para mostrar o nome da emoção\n    label_name = index_to_emotion[label.item()]\n    ax.set_title(f\"{label_name}\", fontsize=10)  # Título com o nome do label\n\n# Ajustar o layout para não sobrepor as imagens\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T19:13:18.633738Z","iopub.execute_input":"2025-02-15T19:13:18.634022Z","iopub.status.idle":"2025-02-15T19:13:22.109345Z","shell.execute_reply.started":"2025-02-15T19:13:18.633998Z","shell.execute_reply":"2025-02-15T19:13:22.108385Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_acc = 0\npatience = 5  # Number of epochs to wait for improvement\nno_improvement = 0\n\nfor i in range(1, args.epochs + 1):\n    train_acc, train_loss = train(model, train_loader, optimizer, scheduler, device)\n    test_acc, test_loss = test(model, test_loader, device)\n    print('epoch: ', i, 'acc_test: ', test_acc, 'acc_train: ', train_acc)\n\n    # Early stopping logic with patience\n    if test_acc > best_acc:\n        best_acc = test_acc\n        no_improvement = 0  # Reset patience counter on improvement\n        torch.save({'model_state_dict': model.state_dict(),}, \"/kaggle/working/ours_best_MMA.pth\")\n    else:\n        no_improvement += 1  # Increment patience counter on no improvement\n\n    if no_improvement == patience:\n        print(f\"Early stopping after {i} epochs with no improvement in test accuracy\")\n        break  # Exit the training loop if patience is exhausted\n\n    torch.save({'model_state_dict': model.state_dict(),}, \"/kaggle/working/ours_final_MMA.pth\")\n    with open('results.txt', 'a') as f:\n        f.write(str(i)+'_'+str(test_acc)+'\\n')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T19:13:22.532361Z","iopub.execute_input":"2025-02-15T19:13:22.532649Z","iopub.status.idle":"2025-02-15T20:30:04.379434Z","shell.execute_reply.started":"2025-02-15T19:13:22.532627Z","shell.execute_reply":"2025-02-15T20:30:04.378105Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### SFEW 2.0","metadata":{}},{"cell_type":"code","source":"emotion_to_index = {\n            \"angry\": 0,\n            \"disgust\": 1,\n            \"fear\": 2,\n            \"happiness\": 3,\n            \"sadness\": 5,\n            \"surprise\": 6,\n            \"neutral\": 4\n        }\n\nindex_to_emotion = {v: k for k, v in emotion_to_index.items()}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T12:51:17.127130Z","iopub.execute_input":"2025-02-16T12:51:17.127412Z","iopub.status.idle":"2025-02-16T12:51:17.131380Z","shell.execute_reply.started":"2025-02-16T12:51:17.127388Z","shell.execute_reply":"2025-02-16T12:51:17.130643Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass SFEWDataset(Dataset):\n    def __init__(self, root_dir, idx_test, idx_sfew, split=\"Train\", transform=None):\n        \"\"\"\n        Args:\n            root_dir (str): Diretório raiz contendo as pastas train, test e valid.\n            split (str): Qual partição carregar (\"train\", \"test\" ou \"valid\").\n            transform (callable, optional): Transformações a serem aplicadas às imagens.\n        \"\"\"\n        self.root_dir = os.path.join(root_dir, split, \"Test_Aligned_Faces\") if split == \"Test\" else os.path.join(root_dir, split)\n        print(self.root_dir)\n        self.transform = transform\n        self.classes = sorted(os.listdir(self.root_dir))  # Lista de emoções\n        self.idxs_test = idx_test\n        self.idxs_sfew = idx_sfew\n        \n        self.samples = []\n        for class_idx, class_name in enumerate(self.classes):\n            class_path = os.path.join(self.root_dir, class_name)\n            for filename in os.listdir(class_path):\n                if filename.endswith(('.png', '.jpg', '.jpeg')):  # Filtra apenas imagens\n                    self.samples.append((os.path.join(class_path, filename), class_idx))\n    \n    def __len__(self):\n        return len(self.samples)\n    \n    def __getitem__(self, idx):\n        img_path, label = self.samples[idx]\n        image = Image.open(img_path).convert(\"RGB\")\n\n        label = self.idxs_test[self.idxs_sfew[label]]\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        return image, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T12:51:17.428336Z","iopub.execute_input":"2025-02-16T12:51:17.428551Z","iopub.status.idle":"2025-02-16T12:51:17.434980Z","shell.execute_reply.started":"2025-02-16T12:51:17.428531Z","shell.execute_reply":"2025-02-16T12:51:17.434161Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"root_dir = \"/kaggle/input/datasetsfew\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T12:51:17.748590Z","iopub.execute_input":"2025-02-16T12:51:17.748981Z","iopub.status.idle":"2025-02-16T12:51:17.752904Z","shell.execute_reply.started":"2025-02-16T12:51:17.748948Z","shell.execute_reply":"2025-02-16T12:51:17.751891Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = SFEWDataset(root_dir=root_dir, idx_test=emotion_to_index, idx_sfew=index_to_emotion, split=\"Train\", transform=train_transforms)\ntrain_loader = torch.utils.data.DataLoader(train_dataset,\n                                           batch_size=args.batch_size,\n                                           #batch_size=1,\n                                           shuffle=True,\n                                           num_workers=args.workers,\n                                           pin_memory=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T12:51:58.850562Z","iopub.execute_input":"2025-02-16T12:51:58.850982Z","iopub.status.idle":"2025-02-16T12:51:59.127466Z","shell.execute_reply.started":"2025-02-16T12:51:58.850949Z","shell.execute_reply":"2025-02-16T12:51:59.126597Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_dataset = SFEWDataset(root_dir=root_dir, idx_test=emotion_to_index, idx_sfew=index_to_emotion, split=\"Val\", transform=train_transforms)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.batch_size,\n                                          shuffle=False,\n                                          num_workers=args.workers,\n                                          pin_memory=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T12:51:59.342136Z","iopub.execute_input":"2025-02-16T12:51:59.342402Z","iopub.status.idle":"2025-02-16T12:51:59.446117Z","shell.execute_reply.started":"2025-02-16T12:51:59.342377Z","shell.execute_reply":"2025-02-16T12:51:59.445277Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Obtendo o batch de imagens e rótulos\nfor images, labels in train_loader:\n    # Se você quiser mostrar apenas um batch\n    break\n\n# Definindo o layout para 4 linhas e 8 colunas\nfig, axes = plt.subplots(4, 8, figsize=(20, 10))  # 4x8 layout\naxes = axes.flatten()  # Flatten para facilitar a iteração\n\n# Loop para exibir as imagens no grid\nfor i, (img, label) in enumerate(zip(images, labels)):\n    if i >= len(axes):  # Se houver mais imagens do que subgráficos\n        break\n\n    # Convertendo a imagem para numpy e normalizando\n    img_np = img.permute(1, 2, 0).numpy()\n    img_np = (img_np - img_np.min()) / (img_np.max() - img_np.min())\n\n    # Exibindo a imagem\n    ax = axes[i]\n    ax.imshow(img_np)\n    ax.axis('off')  # Desativar os eixos\n\n    # Usando o mapa de rótulos para mostrar o nome da emoção\n    label_name = index_to_emotion[label.item()]\n    ax.set_title(f\"{label_name}\", fontsize=10)  # Título com o nome do label\n\n# Ajustar o layout para não sobrepor as imagens\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T12:52:00.504660Z","iopub.execute_input":"2025-02-16T12:52:00.504983Z","iopub.status.idle":"2025-02-16T12:52:04.141522Z","shell.execute_reply.started":"2025-02-16T12:52:00.504955Z","shell.execute_reply":"2025-02-16T12:52:04.140495Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_acc = 0\npatience = 100  # Number of epochs to wait for improvement\nno_improvement = 0\n\nfor i in range(1, args.epochs + 1):\n    train_acc, train_loss = train(model, train_loader, optimizer, scheduler, device)\n    test_acc, test_loss = test(model, test_loader, device)\n    print('epoch: ', i, 'acc_test: ', test_acc, 'acc_train: ', train_acc)\n\n    # Early stopping logic with patience\n    if test_acc > best_acc:\n        best_acc = test_acc\n        no_improvement = 0  # Reset patience counter on improvement\n        torch.save({'model_state_dict': model.state_dict(),}, \"/kaggle/working/ours_best_SFEW.pth\")\n    else:\n        no_improvement += 1  # Increment patience counter on no improvement\n\n    if no_improvement == patience:\n        print(f\"Early stopping after {i} epochs with no improvement in test accuracy\")\n        break  # Exit the training loop if patience is exhausted\n\n    torch.save({'model_state_dict': model.state_dict(),}, \"/kaggle/working/ours_final_SFEW.pth\")\n    with open('results.txt', 'a') as f:\n        f.write(str(i)+'_'+str(test_acc)+'\\n')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T13:02:26.792150Z","iopub.execute_input":"2025-02-16T13:02:26.792522Z","iopub.status.idle":"2025-02-16T13:08:14.124485Z","shell.execute_reply.started":"2025-02-16T13:02:26.792487Z","shell.execute_reply":"2025-02-16T13:08:14.123472Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Teste entre dominios","metadata":{}},{"cell_type":"code","source":"emotion_to_index_mma = {\n            \"angry\": 0,\n            \"disgust\": 1,\n            \"fear\": 2,\n            \"happiness\": 3,\n            \"sadness\": 5,\n            \"surprise\": 6,\n            \"neutral\": 4\n        }\n\nindex_to_emotion_mma = {v: k for k, v in emotion_to_index.items()}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T13:14:03.348163Z","iopub.execute_input":"2025-02-16T13:14:03.348456Z","iopub.status.idle":"2025-02-16T13:14:03.352684Z","shell.execute_reply.started":"2025-02-16T13:14:03.348432Z","shell.execute_reply":"2025-02-16T13:14:03.351714Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"emotion_to_index_sfew = {\n            \"angry\": 0,\n            \"disgust\": 1,\n            \"fear\": 2,\n            \"happiness\": 3,\n            \"sadness\": 5,\n            \"surprise\": 6,\n            \"neutral\": 4\n        }\n\nindex_to_emotion_sfew = {v: k for k, v in emotion_to_index.items()}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T13:14:03.710255Z","iopub.execute_input":"2025-02-16T13:14:03.710674Z","iopub.status.idle":"2025-02-16T13:14:03.715003Z","shell.execute_reply.started":"2025-02-16T13:14:03.710633Z","shell.execute_reply":"2025-02-16T13:14:03.714122Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"emotion_to_index_aff = {\n            \"angry\": 0,\n            \"disgust\": 1,\n            \"fear\": 2,\n            \"happiness\": 3,\n            \"sadness\": 4,\n            \"surprise\": 5,\n            \"neutral\": 6\n        }\n\nindex_to_emotion_aff = {v: k for k, v in emotion_to_index.items()}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T13:14:04.090467Z","iopub.execute_input":"2025-02-16T13:14:04.090777Z","iopub.status.idle":"2025-02-16T13:14:04.095155Z","shell.execute_reply.started":"2025-02-16T13:14:04.090740Z","shell.execute_reply":"2025-02-16T13:14:04.094236Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"emotion_to_index_raf = {\n            \"surprise\": 0,\n            \"fear\": 1,\n            \"disgust\": 2,\n            \"happiness\": 3,\n            \"sadness\": 4,\n            \"angry\": 5,\n            \"neutral\": 6\n        }\n\nindex_to_emotion_raf = {v: k for k, v in emotion_to_index.items()}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T13:14:07.048810Z","iopub.execute_input":"2025-02-16T13:14:07.049097Z","iopub.status.idle":"2025-02-16T13:14:07.053393Z","shell.execute_reply.started":"2025-02-16T13:14:07.049075Z","shell.execute_reply":"2025-02-16T13:14:07.052668Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"emotion_to_index_fer = {\n            \"neutral\": 0,\n            \"happiness\": 1,\n            \"surprise\": 2,\n            \"sadness\": 3,\n            \"angry\": 4,\n            \"disgust\": 5,\n            \"fear\": 6\n        }\n\nindex_to_emotion_fer = {v: k for k, v in emotion_to_index_fer.items()}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:50:34.899710Z","iopub.execute_input":"2025-02-16T17:50:34.900105Z","iopub.status.idle":"2025-02-16T17:50:34.905753Z","shell.execute_reply.started":"2025-02-16T17:50:34.900059Z","shell.execute_reply":"2025-02-16T17:50:34.904825Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### SFEW","metadata":{}},{"cell_type":"code","source":"root_dir = \"/kaggle/input/datasetsfew\"\ndataset_sfew = SFEWDataset(root_dir=root_dir, idx_test=emotion_to_index_sfew, idx_sfew=index_to_emotion_sfew, split=\"Val\", transform=train_transforms)\nloader_sfew = torch.utils.data.DataLoader(dataset_sfew, batch_size=args.batch_size,\n                                          shuffle=False,\n                                          num_workers=args.workers,\n                                          pin_memory=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:50:38.230980Z","iopub.execute_input":"2025-02-16T17:50:38.231262Z","iopub.status.idle":"2025-02-16T17:50:38.243605Z","shell.execute_reply.started":"2025-02-16T17:50:38.231242Z","shell.execute_reply":"2025-02-16T17:50:38.242835Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"root_dir = \"/kaggle/input/mma-facial-expression/MMAFEDB\"\ndataset_mma = MMADataset(root_dir=root_dir, idx_test=emotion_to_index_sfew, idx_mma=index_to_emotion_mma, split=\"test\", transform=eval_transforms)\nloader_mma = torch.utils.data.DataLoader(dataset_mma, batch_size=args.batch_size,\n                                          shuffle=False,\n                                          num_workers=args.workers,\n                                          pin_memory=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:50:38.356652Z","iopub.execute_input":"2025-02-16T17:50:38.356883Z","iopub.status.idle":"2025-02-16T17:50:38.402797Z","shell.execute_reply.started":"2025-02-16T17:50:38.356863Z","shell.execute_reply":"2025-02-16T17:50:38.402206Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"root_dir = \"/kaggle/input/affectnetaligned/AffectNetCustom\"\ndataset_affect = AffectNetDataset(root_dir=root_dir, idx_test=emotion_to_index_sfew, idx_aff=index_to_emotion_aff, split=\"test\", transform=eval_transforms)\nloader_affect = torch.utils.data.DataLoader(dataset_affect, batch_size=args.batch_size,\n                                          shuffle=False,\n                                          num_workers=args.workers,\n                                          pin_memory=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:50:38.574713Z","iopub.execute_input":"2025-02-16T17:50:38.574963Z","iopub.status.idle":"2025-02-16T17:50:38.588453Z","shell.execute_reply.started":"2025-02-16T17:50:38.574942Z","shell.execute_reply":"2025-02-16T17:50:38.587840Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset_raf = RafDataSet('/kaggle/input/eacdata/raf-basic', idxs_test=emotion_to_index_sfew, idxs_raf=index_to_emotion_raf, train=False, transform=train_transforms)\nloader_raf = torch.utils.data.DataLoader(dataset_raf,\n                                           batch_size=args.batch_size,\n                                           #batch_size=1,\n                                           shuffle=False,\n                                           num_workers=args.workers,\n                                           pin_memory=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T18:00:57.940452Z","iopub.execute_input":"2025-02-16T18:00:57.940748Z","iopub.status.idle":"2025-02-16T18:00:58.342115Z","shell.execute_reply.started":"2025-02-16T18:00:57.940716Z","shell.execute_reply":"2025-02-16T18:00:58.341333Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"root_dir = \"/kaggle/input/ferplus/FER2013Plus\"\ndataset_fer = FERPlusDataset(root_dir=root_dir, idxs_test=emotion_to_index_sfew, idxs_fer=index_to_emotion_fer, subset=\"FER2013Test\", transform=eval_transforms)\nloader_fer = torch.utils.data.DataLoader(dataset_fer, batch_size=args.batch_size,\n                                          shuffle=False,\n                                          num_workers=args.workers,\n                                          pin_memory=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:50:43.529057Z","iopub.execute_input":"2025-02-16T17:50:43.529381Z","iopub.status.idle":"2025-02-16T17:50:43.542668Z","shell.execute_reply.started":"2025-02-16T17:50:43.529358Z","shell.execute_reply":"2025-02-16T17:50:43.541964Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_pretrained_weights(model, checkpoint):\n    import collections\n    if 'state_dict' in checkpoint:\n        state_dict = checkpoint['state_dict']\n    else:\n        state_dict = checkpoint\n    model_dict = model.state_dict()\n    new_state_dict = collections.OrderedDict()\n    matched_layers, discarded_layers = [], []\n    for k, v in state_dict.items():\n        # If the pretrained state_dict was saved as nn.DataParallel,\n        # keys would contain \"module.\", which should be ignored.\n        if k.startswith('module.'):\n            k = k[7:]\n        if k in model_dict and model_dict[k].size() == v.size():\n            new_state_dict[k] = v\n            matched_layers.append(k)\n        else:\n            discarded_layers.append(k)\n    # new_state_dict.requires_grad = False\n    model_dict.update(new_state_dict)\n\n    model.load_state_dict(model_dict)\n    print('load_weight', len(matched_layers))\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:50:45.790130Z","iopub.execute_input":"2025-02-16T17:50:45.790405Z","iopub.status.idle":"2025-02-16T17:50:45.796278Z","shell.execute_reply.started":"2025-02-16T17:50:45.790383Z","shell.execute_reply":"2025-02-16T17:50:45.795497Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"checkpoint = torch.load('/kaggle/working/ours_best_SFEW.pth')\ncheckpoint = checkpoint[\"model_state_dict\"]\nmodel = load_pretrained_weights(model, checkpoint)\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:50:46.475885Z","iopub.execute_input":"2025-02-16T17:50:46.476114Z","iopub.status.idle":"2025-02-16T17:50:46.567279Z","shell.execute_reply.started":"2025-02-16T17:50:46.476093Z","shell.execute_reply":"2025-02-16T17:50:46.566420Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"###### SFEW\nacc_sfew_sfew, test_loss = test(model, loader_sfew, device)\nprint('test acc dbtrain-sfew dbtest-sfew: ', acc_sfew_sfew)\n\n###### FER\nacc_sfew_fer, test_loss = test(model, loader_fer, device)\nprint('test acc dbtrain-sfew dbtest-fer: ', acc_sfew_fer)\n\n###### AFFECT\nacc_sfew_affect, test_loss = test(model, loader_affect, device)\nprint('test acc dbtrain-sfew dbtest-affect: ', acc_sfew_affect)\n\n###### MMA\nacc_sfew_mma, test_loss = test(model, loader_mma, device)\nprint('test acc dbtrain-sfew dbtest-mma: ', acc_sfew_mma)\n\n### RAFDB\nacc_sfew_raf, test_loss = test(model, loader_raf, device)\nprint('test acc dbtrain-sfew dbtest-raf: ', acc_sfew_raf)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:50:47.434719Z","iopub.execute_input":"2025-02-16T17:50:47.435077Z","iopub.status.idle":"2025-02-16T17:52:32.248728Z","shell.execute_reply.started":"2025-02-16T17:50:47.435049Z","shell.execute_reply":"2025-02-16T17:52:32.247898Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### MMA","metadata":{}},{"cell_type":"code","source":"root_dir = \"/kaggle/input/datasetsfew\"\ndataset_sfew = SFEWDataset(root_dir=root_dir, idx_test=emotion_to_index_mma, idx_sfew=index_to_emotion_sfew, split=\"Val\", transform=train_transforms)\nloader_sfew = torch.utils.data.DataLoader(dataset_sfew, batch_size=args.batch_size,\n                                          shuffle=False,\n                                          num_workers=args.workers,\n                                          pin_memory=True)\n\nroot_dir = \"/kaggle/input/mma-facial-expression/MMAFEDB\"\ndataset_mma = MMADataset(root_dir=root_dir, idx_test=emotion_to_index_mma, idx_mma=index_to_emotion_mma, split=\"test\", transform=eval_transforms)\nloader_mma = torch.utils.data.DataLoader(dataset_mma, batch_size=args.batch_size,\n                                          shuffle=False,\n                                          num_workers=args.workers,\n                                          pin_memory=True)\n\nroot_dir = \"/kaggle/input/affectnetaligned/AffectNetCustom\"\ndataset_affect = AffectNetDataset(root_dir=root_dir, idx_test=emotion_to_index_mma, idx_aff=index_to_emotion_aff, split=\"test\", transform=eval_transforms)\nloader_affect = torch.utils.data.DataLoader(dataset_affect, batch_size=args.batch_size,\n                                          shuffle=False,\n                                          num_workers=args.workers,\n                                          pin_memory=True)\n\ndataset_raf = RafDataSet('/kaggle/input/eacdata/raf-basic', idxs_test=emotion_to_index_mma, idxs_raf=index_to_emotion_raf, train=False, transform=train_transforms)\nloader_raf = torch.utils.data.DataLoader(dataset_raf,\n                                           batch_size=args.batch_size,\n                                           #batch_size=1,\n                                           shuffle=False,\n                                           num_workers=args.workers,\n                                           pin_memory=True)\n\nroot_dir = \"/kaggle/input/ferplus/FER2013Plus\"\ndataset_fer = FERPlusDataset(root_dir=root_dir, idxs_test=emotion_to_index_mma, idxs_fer=index_to_emotion_fer, subset=\"FER2013Test\", transform=eval_transforms)\nloader_fer = torch.utils.data.DataLoader(dataset_fer, batch_size=args.batch_size,\n                                          shuffle=False,\n                                          num_workers=args.workers,\n                                          pin_memory=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T18:00:37.575085Z","iopub.execute_input":"2025-02-16T18:00:37.575394Z","iopub.status.idle":"2025-02-16T18:00:38.045832Z","shell.execute_reply.started":"2025-02-16T18:00:37.575364Z","shell.execute_reply":"2025-02-16T18:00:38.045140Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"checkpoint = torch.load('/kaggle/working/ours_best_MMA.pth')\ncheckpoint = checkpoint[\"model_state_dict\"]\nmodel = load_pretrained_weights(model, checkpoint)\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:55:11.006018Z","iopub.execute_input":"2025-02-16T17:55:11.006331Z","iopub.status.idle":"2025-02-16T17:55:11.065532Z","shell.execute_reply.started":"2025-02-16T17:55:11.006300Z","shell.execute_reply":"2025-02-16T17:55:11.064674Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"###### SFEW\nacc_mma_sfew, test_loss = test(model, loader_sfew, device)\nprint('test acc dbtrain-mma dbtest-sfew: ', acc_mma_sfew)\n\n###### FER\nacc_mma_fer, test_loss = test(model, loader_fer, device)\nprint('test acc dbtrain-mma dbtest-fer: ', acc_mma_fer)\n\n###### AFFECT\nacc_mma_affect, test_loss = test(model, loader_affect, device)\nprint('test acc dbtrain-mma dbtest-affect: ', acc_mma_affect)\n\n###### MMA\nacc_mma_mma, test_loss = test(model, loader_mma, device)\nprint('test acc dbtrain-mma dbtest-mma: ', acc_mma_mma)\n\n### RAFDB\nacc_mma_raf, test_loss = test(model, loader_raf, device)\nprint('test acc dbtrain-mma dbtest-raf: ', acc_mma_raf)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:55:53.530622Z","iopub.execute_input":"2025-02-16T17:55:53.530997Z","iopub.status.idle":"2025-02-16T17:56:46.244629Z","shell.execute_reply.started":"2025-02-16T17:55:53.530958Z","shell.execute_reply":"2025-02-16T17:56:46.243744Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Raf","metadata":{}},{"cell_type":"code","source":"root_dir = \"/kaggle/input/datasetsfew\"\ndataset_sfew = SFEWDataset(root_dir=root_dir, idx_test=emotion_to_index_raf, idx_sfew=index_to_emotion_sfew, split=\"Val\", transform=train_transforms)\nloader_sfew = torch.utils.data.DataLoader(dataset_sfew, batch_size=args.batch_size,\n                                          shuffle=False,\n                                          num_workers=args.workers,\n                                          pin_memory=True)\n\nroot_dir = \"/kaggle/input/mma-facial-expression/MMAFEDB\"\ndataset_mma = MMADataset(root_dir=root_dir, idx_test=emotion_to_index_raf, idx_mma=index_to_emotion_mma, split=\"test\", transform=eval_transforms)\nloader_mma = torch.utils.data.DataLoader(dataset_mma, batch_size=args.batch_size,\n                                          shuffle=False,\n                                          num_workers=args.workers,\n                                          pin_memory=True)\n\nroot_dir = \"/kaggle/input/affectnetaligned/AffectNetCustom\"\ndataset_affect = AffectNetDataset(root_dir=root_dir, idx_test=emotion_to_index_raf, idx_aff=index_to_emotion_aff, split=\"test\", transform=eval_transforms)\nloader_affect = torch.utils.data.DataLoader(dataset_affect, batch_size=args.batch_size,\n                                          shuffle=False,\n                                          num_workers=args.workers,\n                                          pin_memory=True)\n\ndataset_raf = RafDataSet('/kaggle/input/eacdata/raf-basic', idxs_test=emotion_to_index_raf, idxs_raf=index_to_emotion_raf, train=False, transform=train_transforms)\nloader_raf = torch.utils.data.DataLoader(dataset_raf,\n                                           batch_size=args.batch_size,\n                                           #batch_size=1,\n                                           shuffle=False,\n                                           num_workers=args.workers,\n                                           pin_memory=True)\n\nroot_dir = \"/kaggle/input/ferplus/FER2013Plus\"\ndataset_fer = FERPlusDataset(root_dir=root_dir, idxs_test=emotion_to_index_raf, idxs_fer=index_to_emotion_fer, subset=\"FER2013Test\", transform=eval_transforms)\nloader_fer = torch.utils.data.DataLoader(dataset_fer, batch_size=args.batch_size,\n                                          shuffle=False,\n                                          num_workers=args.workers,\n                                          pin_memory=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T18:00:26.572153Z","iopub.execute_input":"2025-02-16T18:00:26.572521Z","iopub.status.idle":"2025-02-16T18:00:27.020095Z","shell.execute_reply.started":"2025-02-16T18:00:26.572474Z","shell.execute_reply":"2025-02-16T18:00:27.019248Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"checkpoint = torch.load('/kaggle/working/ours_best_RAFDB.pth')\ncheckpoint = checkpoint[\"model_state_dict\"]\nmodel = load_pretrained_weights(model, checkpoint)\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:57:38.422638Z","iopub.execute_input":"2025-02-16T17:57:38.422966Z","iopub.status.idle":"2025-02-16T17:57:38.490441Z","shell.execute_reply.started":"2025-02-16T17:57:38.422938Z","shell.execute_reply":"2025-02-16T17:57:38.489718Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"###### SFEW\nacc_raf_sfew, test_loss = test(model, loader_sfew, device)\nprint('test acc dbtrain-raf dbtest-sfew: ', acc_raf_sfew)\n\n###### FER\nacc_raf_fer, test_loss = test(model, loader_fer, device)\nprint('test acc dbtrain-raf dbtest-fer: ', acc_raf_fer)\n\n###### AFFECT\nacc_raf_affect, test_loss = test(model, loader_affect, device)\nprint('test acc dbtrain-raf dbtest-affect: ', acc_raf_affect)\n\n###### MMA\nacc_raf_mma, test_loss = test(model, loader_mma, device)\nprint('test acc dbtrain-raf dbtest-mma: ', acc_raf_mma)\n\n### RAFDB\nacc_raf_raf, test_loss = test(model, loader_raf, device)\nprint('test acc dbtrain-raf dbtest-raf: ', acc_raf_raf)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:58:11.413706Z","iopub.execute_input":"2025-02-16T17:58:11.414060Z","iopub.status.idle":"2025-02-16T17:59:05.514251Z","shell.execute_reply.started":"2025-02-16T17:58:11.414031Z","shell.execute_reply":"2025-02-16T17:59:05.513166Z"},"scrolled":true},"outputs":[],"execution_count":null}]}