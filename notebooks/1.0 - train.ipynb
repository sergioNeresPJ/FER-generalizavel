{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"ZE_ENABLE_TRACING_LAYER\"] = \"1\"\n",
    "\n",
    "import torch\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "\n",
    "device = torch.device('xpu' if torch.xpu.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simple_tokenizer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3682,
     "status": "ok",
     "timestamp": 1736782222336,
     "user": {
      "displayName": "Sergio Neres Pereira Junior",
      "userId": "05770949936273593301"
     },
     "user_tz": 180
    },
    "id": "y_wipLK2yRkq",
    "outputId": "a31cfc60-8704-4636-bd18-4e4b313c2d22",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! pip install ftfy\n",
    "! pip install regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/uc1add2a09dc85742dea98fbdba71022/.local/lib/python3.10/site-packages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import html\n",
    "import os\n",
    "from functools import lru_cache\n",
    "\n",
    "import ftfy\n",
    "import regex as re\n",
    "\n",
    "\n",
    "\n",
    "@lru_cache()\n",
    "def default_bpe():\n",
    "    return os.path.join('/home/uc1add2a09dc85742dea98fbdba71022/FER-generalizavel/src/clip/bpe_simple_vocab_16e6.txt.gz')\n",
    "\n",
    "\n",
    "@lru_cache()\n",
    "def bytes_to_unicode():\n",
    "    \"\"\"\n",
    "    Returns list of utf-8 byte and a corresponding list of unicode strings.\n",
    "    The reversible bpe codes work on unicode strings.\n",
    "    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n",
    "    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n",
    "    This is a signficant percentage of your normal, say, 32K bpe vocab.\n",
    "    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n",
    "    And avoids mapping to whitespace/control characters the bpe code barfs on.\n",
    "    \"\"\"\n",
    "    bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"¡\"), ord(\"¬\")+1))+list(range(ord(\"®\"), ord(\"ÿ\")+1))\n",
    "    cs = bs[:]\n",
    "    n = 0\n",
    "    for b in range(2**8):\n",
    "        if b not in bs:\n",
    "            bs.append(b)\n",
    "            cs.append(2**8+n)\n",
    "            n += 1\n",
    "    cs = [chr(n) for n in cs]\n",
    "    return dict(zip(bs, cs))\n",
    "\n",
    "\n",
    "def get_pairs(word):\n",
    "    \"\"\"Return set of symbol pairs in a word.\n",
    "    Word is represented as tuple of symbols (symbols being variable-length strings).\n",
    "    \"\"\"\n",
    "    pairs = set()\n",
    "    prev_char = word[0]\n",
    "    for char in word[1:]:\n",
    "        pairs.add((prev_char, char))\n",
    "        prev_char = char\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def basic_clean(text):\n",
    "    text = ftfy.fix_text(text)\n",
    "    text = html.unescape(html.unescape(text))\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def whitespace_clean(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "class SimpleTokenizer(object):\n",
    "    def __init__(self, bpe_path: str = default_bpe()):\n",
    "        self.byte_encoder = bytes_to_unicode()\n",
    "        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n",
    "        with gzip.open(bpe_path, 'rb') as f:\n",
    "            merges = f.read().decode(\"utf-8\").split('\\n')\n",
    "        #merges = open(bpe_path, 'rb').read().decode(\"utf-8\").split('\\n')\n",
    "        merges = merges[1:49152-256-2+1]\n",
    "        merges = [tuple(merge.split()) for merge in merges]\n",
    "        vocab = list(bytes_to_unicode().values())\n",
    "        vocab = vocab + [v+'</w>' for v in vocab]\n",
    "        for merge in merges:\n",
    "            vocab.append(''.join(merge))\n",
    "        vocab.extend(['<|startoftext|>', '<|endoftext|>'])\n",
    "        self.encoder = dict(zip(vocab, range(len(vocab))))\n",
    "        self.decoder = {v: k for k, v in self.encoder.items()}\n",
    "        self.bpe_ranks = dict(zip(merges, range(len(merges))))\n",
    "        self.cache = {'<|startoftext|>': '<|startoftext|>', '<|endoftext|>': '<|endoftext|>'}\n",
    "        self.pat = re.compile(r\"\"\"<\\|startoftext\\|>|<\\|endoftext\\|>|'s|'t|'re|'ve|'m|'ll|'d|[\\p{L}]+|[\\p{N}]|[^\\s\\p{L}\\p{N}]+\"\"\", re.IGNORECASE)\n",
    "\n",
    "    def bpe(self, token):\n",
    "        if token in self.cache:\n",
    "            return self.cache[token]\n",
    "        word = tuple(token[:-1]) + ( token[-1] + '</w>',)\n",
    "        pairs = get_pairs(word)\n",
    "\n",
    "        if not pairs:\n",
    "            return token+'</w>'\n",
    "\n",
    "        while True:\n",
    "            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
    "            if bigram not in self.bpe_ranks:\n",
    "                break\n",
    "            first, second = bigram\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                try:\n",
    "                    j = word.index(first, i)\n",
    "                    new_word.extend(word[i:j])\n",
    "                    i = j\n",
    "                except:\n",
    "                    new_word.extend(word[i:])\n",
    "                    break\n",
    "\n",
    "                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
    "                    new_word.append(first+second)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "            new_word = tuple(new_word)\n",
    "            word = new_word\n",
    "            if len(word) == 1:\n",
    "                break\n",
    "            else:\n",
    "                pairs = get_pairs(word)\n",
    "        word = ' '.join(word)\n",
    "        self.cache[token] = word\n",
    "        return word\n",
    "\n",
    "    def encode(self, text):\n",
    "        bpe_tokens = []\n",
    "        text = whitespace_clean(basic_clean(text)).lower()\n",
    "        for token in re.findall(self.pat, text):\n",
    "            token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))\n",
    "            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))\n",
    "        return bpe_tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        text = ''.join([self.decoder[token] for token in tokens])\n",
    "        text = bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors=\"replace\").replace('</w>', ' ')\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        # all conv layers have stride 1. an avgpool is performed after the second convolution when stride > 1\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, 1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(planes, planes, 3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.avgpool = nn.AvgPool2d(stride) if stride > 1 else nn.Identity()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, 1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = None\n",
    "        self.stride = stride\n",
    "\n",
    "        if stride > 1 or inplanes != planes * Bottleneck.expansion:\n",
    "            # downsampling layer is prepended with an avgpool, and the subsequent convolution has stride 1\n",
    "            self.downsample = nn.Sequential(OrderedDict([\n",
    "                (\"-1\", nn.AvgPool2d(stride)),\n",
    "                (\"0\", nn.Conv2d(inplanes, planes * self.expansion, 1, stride=1, bias=False)),\n",
    "                (\"1\", nn.BatchNorm2d(planes * self.expansion))\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        identity = x\n",
    "\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.avgpool(out)\n",
    "        out = self.bn3(self.conv3(out))\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class AttentionPool2d(nn.Module):\n",
    "    def __init__(self, spacial_dim: int, embed_dim: int, num_heads: int, output_dim: int = None):\n",
    "        super().__init__()\n",
    "        self.positional_embedding = nn.Parameter(torch.randn(spacial_dim ** 2 + 1, embed_dim) / embed_dim ** 0.5)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.c_proj = nn.Linear(embed_dim, output_dim or embed_dim)\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(x.shape[0], x.shape[1], x.shape[2] * x.shape[3]).permute(2, 0, 1)  # NCHW -> (HW)NC\n",
    "        x = torch.cat([x.mean(dim=0, keepdim=True), x], dim=0)  # (HW+1)NC\n",
    "        x = x + self.positional_embedding[:, None, :].to(x.dtype)  # (HW+1)NC\n",
    "        x, _ = F.multi_head_attention_forward(\n",
    "            query=x, key=x, value=x,\n",
    "            embed_dim_to_check=x.shape[-1],\n",
    "            num_heads=self.num_heads,\n",
    "            q_proj_weight=self.q_proj.weight,\n",
    "            k_proj_weight=self.k_proj.weight,\n",
    "            v_proj_weight=self.v_proj.weight,\n",
    "            in_proj_weight=None,\n",
    "            in_proj_bias=torch.cat([self.q_proj.bias, self.k_proj.bias, self.v_proj.bias]),\n",
    "            bias_k=None,\n",
    "            bias_v=None,\n",
    "            add_zero_attn=False,\n",
    "            dropout_p=0,\n",
    "            out_proj_weight=self.c_proj.weight,\n",
    "            out_proj_bias=self.c_proj.bias,\n",
    "            use_separate_proj_weight=True,\n",
    "            training=self.training,\n",
    "            need_weights=False\n",
    "        )\n",
    "\n",
    "        return x[0]\n",
    "\n",
    "\n",
    "class ModifiedResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A ResNet class that is similar to torchvision's but contains the following changes:\n",
    "    - There are now 3 \"stem\" convolutions as opposed to 1, with an average pool instead of a max pool.\n",
    "    - Performs anti-aliasing strided convolutions, where an avgpool is prepended to convolutions with stride > 1\n",
    "    - The final pooling layer is a QKV attention instead of an average pool\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layers, output_dim, heads, input_resolution=224, width=64):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.input_resolution = input_resolution\n",
    "\n",
    "        # the 3-layer stem\n",
    "        self.conv1 = nn.Conv2d(3, width // 2, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(width // 2)\n",
    "        self.conv2 = nn.Conv2d(width // 2, width // 2, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(width // 2)\n",
    "        self.conv3 = nn.Conv2d(width // 2, width, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(width)\n",
    "        self.avgpool = nn.AvgPool2d(2)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # residual layers\n",
    "        self._inplanes = width  # this is a *mutable* variable used during construction\n",
    "        self.layer1 = self._make_layer(width, layers[0])\n",
    "        self.layer2 = self._make_layer(width * 2, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(width * 4, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(width * 8, layers[3], stride=2)\n",
    "\n",
    "        embed_dim = width * 32  # the ResNet feature dimension\n",
    "        self.attnpool = AttentionPool2d(input_resolution // 32, embed_dim, heads, output_dim)\n",
    "\n",
    "    def _make_layer(self, planes, blocks, stride=1):\n",
    "        layers = [Bottleneck(self._inplanes, planes, stride)]\n",
    "\n",
    "        self._inplanes = planes * Bottleneck.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(Bottleneck(self._inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        def stem(x):\n",
    "            for conv, bn in [(self.conv1, self.bn1), (self.conv2, self.bn2), (self.conv3, self.bn3)]:\n",
    "                x = self.relu(bn(conv(x)))\n",
    "            x = self.avgpool(x)\n",
    "            return x\n",
    "\n",
    "        x = x.type(self.conv1.weight.dtype)\n",
    "        x = stem(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.attnpool(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class LayerNorm(nn.LayerNorm):\n",
    "    \"\"\"Subclass torch's LayerNorm to handle fp16.\"\"\"\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        orig_type = x.dtype\n",
    "        ret = super().forward(x.type(torch.float32))\n",
    "        return ret.type(orig_type)\n",
    "\n",
    "\n",
    "class QuickGELU(nn.Module):\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return x * torch.sigmoid(1.702 * x)\n",
    "\n",
    "\n",
    "class ResidualAttentionBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, n_head: int, attn_mask: torch.Tensor = None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = nn.MultiheadAttention(d_model, n_head)\n",
    "        self.ln_1 = LayerNorm(d_model)\n",
    "        self.mlp = nn.Sequential(OrderedDict([\n",
    "            (\"c_fc\", nn.Linear(d_model, d_model * 4)),\n",
    "            (\"gelu\", QuickGELU()),\n",
    "            (\"c_proj\", nn.Linear(d_model * 4, d_model))\n",
    "        ]))\n",
    "        self.ln_2 = LayerNorm(d_model)\n",
    "        self.attn_mask = attn_mask\n",
    "\n",
    "    def attention(self, x: torch.Tensor):\n",
    "        self.attn_mask = self.attn_mask.to(dtype=x.dtype, device=x.device) if self.attn_mask is not None else None\n",
    "        return self.attn(x, x, x, need_weights=False, attn_mask=self.attn_mask)[0]\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = x + self.attention(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, width: int, layers: int, heads: int, attn_mask: torch.Tensor = None):\n",
    "        super().__init__()\n",
    "        self.width = width\n",
    "        self.layers = layers\n",
    "        self.resblocks = nn.Sequential(*[ResidualAttentionBlock(width, heads, attn_mask) for _ in range(layers)])\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.resblocks(x)\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, input_resolution: int, patch_size: int, width: int, layers: int, heads: int, output_dim: int):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.output_dim = output_dim\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=width, kernel_size=patch_size, stride=patch_size, bias=False)\n",
    "\n",
    "        scale = width ** -0.5\n",
    "        self.class_embedding = nn.Parameter(scale * torch.randn(width))\n",
    "        self.positional_embedding = nn.Parameter(scale * torch.randn((input_resolution // patch_size) ** 2 + 1, width))\n",
    "        self.ln_pre = LayerNorm(width)\n",
    "\n",
    "        self.transformer = Transformer(width, layers, heads)\n",
    "\n",
    "        self.ln_post = LayerNorm(width)\n",
    "        self.proj = nn.Parameter(scale * torch.randn(width, output_dim))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.conv1(x)  # shape = [*, width, grid, grid]\n",
    "        x = x.reshape(x.shape[0], x.shape[1], -1)  # shape = [*, width, grid ** 2]\n",
    "        x = x.permute(0, 2, 1)  # shape = [*, grid ** 2, width]\n",
    "        x = torch.cat([self.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1)  # shape = [*, grid ** 2 + 1, width]\n",
    "        x = x + self.positional_embedding.to(x.dtype)\n",
    "        x = self.ln_pre(x)\n",
    "\n",
    "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "\n",
    "        x = self.ln_post(x[:, 0, :])\n",
    "\n",
    "        if self.proj is not None:\n",
    "            x = x @ self.proj\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class CLIP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embed_dim: int,\n",
    "                 # vision\n",
    "                 image_resolution: int,\n",
    "                 vision_layers: Union[Tuple[int, int, int, int], int],\n",
    "                 vision_width: int,\n",
    "                 vision_patch_size: int,\n",
    "                 # text\n",
    "                 context_length: int,\n",
    "                 vocab_size: int,\n",
    "                 transformer_width: int,\n",
    "                 transformer_heads: int,\n",
    "                 transformer_layers: int\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.context_length = context_length\n",
    "\n",
    "        if isinstance(vision_layers, (tuple, list)):\n",
    "            vision_heads = vision_width * 32 // 64\n",
    "            self.visual = ModifiedResNet(\n",
    "                layers=vision_layers,\n",
    "                output_dim=embed_dim,\n",
    "                heads=vision_heads,\n",
    "                input_resolution=image_resolution,\n",
    "                width=vision_width\n",
    "            )\n",
    "        else:\n",
    "            vision_heads = vision_width // 64\n",
    "            self.visual = VisionTransformer(\n",
    "                input_resolution=image_resolution,\n",
    "                patch_size=vision_patch_size,\n",
    "                width=vision_width,\n",
    "                layers=vision_layers,\n",
    "                heads=vision_heads,\n",
    "                output_dim=embed_dim\n",
    "            )\n",
    "\n",
    "        self.transformer = Transformer(\n",
    "            width=transformer_width,\n",
    "            layers=transformer_layers,\n",
    "            heads=transformer_heads,\n",
    "            attn_mask=self.build_attention_mask()\n",
    "        )\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.token_embedding = nn.Embedding(vocab_size, transformer_width)\n",
    "        self.positional_embedding = nn.Parameter(torch.empty(self.context_length, transformer_width))\n",
    "        self.ln_final = LayerNorm(transformer_width)\n",
    "\n",
    "        self.text_projection = nn.Parameter(torch.empty(transformer_width, embed_dim))\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        nn.init.normal_(self.token_embedding.weight, std=0.02)\n",
    "        nn.init.normal_(self.positional_embedding, std=0.01)\n",
    "\n",
    "        if isinstance(self.visual, ModifiedResNet):\n",
    "            if self.visual.attnpool is not None:\n",
    "                std = self.visual.attnpool.c_proj.in_features ** -0.5\n",
    "                nn.init.normal_(self.visual.attnpool.q_proj.weight, std=std)\n",
    "                nn.init.normal_(self.visual.attnpool.k_proj.weight, std=std)\n",
    "                nn.init.normal_(self.visual.attnpool.v_proj.weight, std=std)\n",
    "                nn.init.normal_(self.visual.attnpool.c_proj.weight, std=std)\n",
    "\n",
    "            for resnet_block in [self.visual.layer1, self.visual.layer2, self.visual.layer3, self.visual.layer4]:\n",
    "                for name, param in resnet_block.named_parameters():\n",
    "                    if name.endswith(\"bn3.weight\"):\n",
    "                        nn.init.zeros_(param)\n",
    "\n",
    "        proj_std = (self.transformer.width ** -0.5) * ((2 * self.transformer.layers) ** -0.5)\n",
    "        attn_std = self.transformer.width ** -0.5\n",
    "        fc_std = (2 * self.transformer.width) ** -0.5\n",
    "        for block in self.transformer.resblocks:\n",
    "            nn.init.normal_(block.attn.in_proj_weight, std=attn_std)\n",
    "            nn.init.normal_(block.attn.out_proj.weight, std=proj_std)\n",
    "            nn.init.normal_(block.mlp.c_fc.weight, std=fc_std)\n",
    "            nn.init.normal_(block.mlp.c_proj.weight, std=proj_std)\n",
    "\n",
    "        if self.text_projection is not None:\n",
    "            nn.init.normal_(self.text_projection, std=self.transformer.width ** -0.5)\n",
    "\n",
    "    def build_attention_mask(self):\n",
    "        # lazily create causal attention mask, with full attention between the vision tokens\n",
    "        # pytorch uses additive attention mask; fill with -inf\n",
    "        mask = torch.empty(self.context_length, self.context_length)\n",
    "        mask.fill_(float(\"-inf\"))\n",
    "        mask.triu_(1)  # zero out the lower diagonal\n",
    "        return mask\n",
    "\n",
    "    @property\n",
    "    def dtype(self):\n",
    "        return self.visual.conv1.weight.dtype\n",
    "\n",
    "    def encode_image(self, image):\n",
    "        return self.visual(image.type(self.dtype))\n",
    "\n",
    "    def encode_text(self, text):\n",
    "        x = self.token_embedding(text).type(self.dtype)  # [batch_size, n_ctx, d_model]\n",
    "\n",
    "        x = x + self.positional_embedding.type(self.dtype)\n",
    "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "        x = self.ln_final(x).type(self.dtype)\n",
    "\n",
    "        # x.shape = [batch_size, n_ctx, transformer.width]\n",
    "        # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "        x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ self.text_projection\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, image, text):\n",
    "        image_features = self.encode_image(image)\n",
    "        text_features = self.encode_text(text)\n",
    "\n",
    "        # normalized features\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # cosine similarity as logits\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logits_per_image = logit_scale * image_features @ text_features.t()\n",
    "        logits_per_text = logits_per_image.t()\n",
    "\n",
    "        # shape = [global_batch_size, global_batch_size]\n",
    "        return logits_per_image, logits_per_text\n",
    "\n",
    "\n",
    "def convert_weights(model: nn.Module):\n",
    "    \"\"\"Convert applicable model parameters to fp16\"\"\"\n",
    "\n",
    "    def _convert_weights_to_fp16(l):\n",
    "        if isinstance(l, (nn.Conv1d, nn.Conv2d, nn.Linear)):\n",
    "            l.weight.data = l.weight.data.half()\n",
    "            if l.bias is not None:\n",
    "                l.bias.data = l.bias.data.half()\n",
    "\n",
    "        if isinstance(l, nn.MultiheadAttention):\n",
    "            for attr in [*[f\"{s}_proj_weight\" for s in [\"in\", \"q\", \"k\", \"v\"]], \"in_proj_bias\", \"bias_k\", \"bias_v\"]:\n",
    "                tensor = getattr(l, attr)\n",
    "                if tensor is not None:\n",
    "                    tensor.data = tensor.data.half()\n",
    "\n",
    "        for name in [\"text_projection\", \"proj\"]:\n",
    "            if hasattr(l, name):\n",
    "                attr = getattr(l, name)\n",
    "                if attr is not None:\n",
    "                    attr.data = attr.data.half()\n",
    "\n",
    "    model.apply(_convert_weights_to_fp16)\n",
    "\n",
    "\n",
    "def build_model(state_dict: dict):\n",
    "    vit = \"visual.proj\" in state_dict\n",
    "\n",
    "    if vit:\n",
    "        vision_width = state_dict[\"visual.conv1.weight\"].shape[0]\n",
    "        vision_layers = len([k for k in state_dict.keys() if k.startswith(\"visual.\") and k.endswith(\".attn.in_proj_weight\")])\n",
    "        vision_patch_size = state_dict[\"visual.conv1.weight\"].shape[-1]\n",
    "        grid_size = round((state_dict[\"visual.positional_embedding\"].shape[0] - 1) ** 0.5)\n",
    "        image_resolution = vision_patch_size * grid_size\n",
    "    else:\n",
    "        counts: list = [len(set(k.split(\".\")[2] for k in state_dict if k.startswith(f\"visual.layer{b}\"))) for b in [1, 2, 3, 4]]\n",
    "        vision_layers = tuple(counts)\n",
    "        vision_width = state_dict[\"visual.layer1.0.conv1.weight\"].shape[0]\n",
    "        output_width = round((state_dict[\"visual.attnpool.positional_embedding\"].shape[0] - 1) ** 0.5)\n",
    "        vision_patch_size = None\n",
    "        assert output_width ** 2 + 1 == state_dict[\"visual.attnpool.positional_embedding\"].shape[0]\n",
    "        image_resolution = output_width * 32\n",
    "\n",
    "    embed_dim = state_dict[\"text_projection\"].shape[1]\n",
    "    context_length = state_dict[\"positional_embedding\"].shape[0]\n",
    "    vocab_size = state_dict[\"token_embedding.weight\"].shape[0]\n",
    "    transformer_width = state_dict[\"ln_final.weight\"].shape[0]\n",
    "    transformer_heads = transformer_width // 64\n",
    "    transformer_layers = len(set(k.split(\".\")[2] for k in state_dict if k.startswith(f\"transformer.resblocks\")))\n",
    "\n",
    "    model = CLIP(\n",
    "        embed_dim,\n",
    "        image_resolution, vision_layers, vision_width, vision_patch_size,\n",
    "        context_length, vocab_size, transformer_width, transformer_heads, transformer_layers\n",
    "    )\n",
    "\n",
    "    for key in [\"input_resolution\", \"context_length\", \"vocab_size\"]:\n",
    "        if key in state_dict:\n",
    "            del state_dict[key]\n",
    "\n",
    "    convert_weights(model)\n",
    "    model.load_state_dict(state_dict)\n",
    "    return model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clip.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import os\n",
    "import urllib\n",
    "import warnings\n",
    "from typing import Any, Union, List\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    from torchvision.transforms import InterpolationMode\n",
    "    BICUBIC = InterpolationMode.BICUBIC\n",
    "except ImportError:\n",
    "    BICUBIC = Image.BICUBIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.__version__.split(\".\") < [\"1\", \"7\", \"1\"]:\n",
    "    warnings.warn(\"PyTorch version 1.7.1 or higher is recommended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _download(url: str, root: str):\n",
    "    os.makedirs(root, exist_ok=True)\n",
    "    filename = os.path.basename(url)\n",
    "\n",
    "    expected_sha256 = url.split(\"/\")[-2]\n",
    "    download_target = os.path.join(root, filename)\n",
    "\n",
    "    if os.path.exists(download_target) and not os.path.isfile(download_target):\n",
    "        raise RuntimeError(f\"{download_target} exists and is not a regular file\")\n",
    "\n",
    "    if os.path.isfile(download_target):\n",
    "        if hashlib.sha256(open(download_target, \"rb\").read()).hexdigest() == expected_sha256:\n",
    "            return download_target\n",
    "        else:\n",
    "            warnings.warn(f\"{download_target} exists, but the SHA256 checksum does not match; re-downloading the file\")\n",
    "\n",
    "    with urllib.request.urlopen(url) as source, open(download_target, \"wb\") as output:\n",
    "        with tqdm(total=int(source.info().get(\"Content-Length\")), ncols=80, unit='iB', unit_scale=True, unit_divisor=1024) as loop:\n",
    "            while True:\n",
    "                buffer = source.read(8192)\n",
    "                if not buffer:\n",
    "                    break\n",
    "\n",
    "                output.write(buffer)\n",
    "                loop.update(len(buffer))\n",
    "\n",
    "    if hashlib.sha256(open(download_target, \"rb\").read()).hexdigest() != expected_sha256:\n",
    "        raise RuntimeError(f\"Model has been downloaded but the SHA256 checksum does not not match\")\n",
    "\n",
    "    return download_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_image_to_rgb(image):\n",
    "    return image.convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _transform(n_px):\n",
    "    return Compose([\n",
    "        Resize(n_px, interpolation=BICUBIC),\n",
    "        CenterCrop(n_px),\n",
    "        _convert_image_to_rgb,\n",
    "        ToTensor(),\n",
    "        Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def available_models() -> List[str]:\n",
    "    \"\"\"Returns the names of available CLIP models\"\"\"\n",
    "    return list(_MODELS.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_device(module):\n",
    "        try:\n",
    "            graphs = [module.graph] if hasattr(module, \"graph\") else []\n",
    "        except RuntimeError:\n",
    "            graphs = []\n",
    "\n",
    "        if hasattr(module, \"forward1\"):\n",
    "            graphs.append(module.forward1.graph)\n",
    "\n",
    "        for graph in graphs:\n",
    "            for node in graph.findAllNodes(\"prim::Constant\"):\n",
    "                if \"value\" in node.attributeNames() and str(node[\"value\"]).startswith(\"cuda\"):\n",
    "                    node.copyAttributes(device_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 989,
     "status": "ok",
     "timestamp": 1736782190915,
     "user": {
      "displayName": "Sergio Neres Pereira Junior",
      "userId": "05770949936273593301"
     },
     "user_tz": 180
    },
    "id": "9uM4i71QtdsU"
   },
   "outputs": [],
   "source": [
    "def load(name: str, device: Union[str, torch.device] = \"cuda\" if torch.cuda.is_available() else \"cpu\", jit: bool = False, download_root: str = None):\n",
    "    \"\"\"Load a CLIP model\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    name : str\n",
    "        A model name listed by `clip.available_models()`, or the path to a model checkpoint containing the state_dict\n",
    "\n",
    "    device : Union[str, torch.device]\n",
    "        The device to put the loaded model\n",
    "\n",
    "    jit : bool\n",
    "        Whether to load the optimized JIT model or more hackable non-JIT model (default).\n",
    "\n",
    "    download_root: str\n",
    "        path to download the model files; by default, it uses \"~/.cache/clip\"\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model : torch.nn.Module\n",
    "        The CLIP model\n",
    "\n",
    "    preprocess : Callable[[PIL.Image], torch.Tensor]\n",
    "        A torchvision transform that converts a PIL image into a tensor that the returned model can take as its input\n",
    "    \"\"\"\n",
    "    if name in _MODELS:\n",
    "        model_path = _download(_MODELS[name], download_root or os.path.expanduser(\"~/.cache/clip\"))\n",
    "    elif os.path.isfile(name):\n",
    "        model_path = name\n",
    "    else:\n",
    "        raise RuntimeError(f\"Model {name} not found; available models = {available_models()}\")\n",
    "\n",
    "    try:\n",
    "        # loading JIT archive\n",
    "        model = torch.jit.load(model_path, map_location=device if jit else \"cpu\").eval()\n",
    "        state_dict = None\n",
    "    except RuntimeError:\n",
    "        # loading saved state dict\n",
    "        if jit:\n",
    "            warnings.warn(f\"File {model_path} is not a JIT archive. Loading as a state dict instead\")\n",
    "            jit = False\n",
    "        state_dict = torch.load(model_path, map_location=\"cpu\")\n",
    "\n",
    "    if not jit:\n",
    "        model = build_model(state_dict or model.state_dict()).to(device)\n",
    "        if str(device) == \"cpu\":\n",
    "            model.float()\n",
    "        return model, _transform(model.visual.input_resolution)\n",
    "\n",
    "    # patch the device names\n",
    "    device_holder = torch.jit.trace(lambda: torch.ones([]).to(torch.device(device)), example_inputs=[])\n",
    "    device_node = [n for n in device_holder.graph.findAllNodes(\"prim::Constant\") if \"Device\" in repr(n)][-1]\n",
    "\n",
    "    model.apply(patch_device)\n",
    "    patch_device(model.encode_image)\n",
    "    patch_device(model.encode_text)\n",
    "\n",
    "    # patch dtype to float32 on CPU\n",
    "    if str(device) == \"cpu\":\n",
    "        float_holder = torch.jit.trace(lambda: torch.ones([]).float(), example_inputs=[])\n",
    "        float_input = list(float_holder.graph.findNode(\"aten::to\").inputs())[1]\n",
    "        float_node = float_input.node()\n",
    "\n",
    "        def patch_float(module):\n",
    "            try:\n",
    "                graphs = [module.graph] if hasattr(module, \"graph\") else []\n",
    "            except RuntimeError:\n",
    "                graphs = []\n",
    "\n",
    "            if hasattr(module, \"forward1\"):\n",
    "                graphs.append(module.forward1.graph)\n",
    "\n",
    "            for graph in graphs:\n",
    "                for node in graph.findAllNodes(\"aten::to\"):\n",
    "                    inputs = list(node.inputs())\n",
    "                    for i in [1, 2]:  # dtype can be the second or third argument to aten::to()\n",
    "                        if inputs[i].node()[\"value\"] == 5:\n",
    "                            inputs[i].node().copyAttributes(float_node)\n",
    "\n",
    "        model.apply(patch_float)\n",
    "        patch_float(model.encode_image)\n",
    "        patch_float(model.encode_text)\n",
    "\n",
    "        model.float()\n",
    "\n",
    "    return model, _transform(model.input_resolution.item())\n",
    "\n",
    "\n",
    "def tokenize(texts: Union[str, List[str]], context_length: int = 77, truncate: bool = False) -> torch.LongTensor:\n",
    "    \"\"\"\n",
    "    Returns the tokenized representation of given input string(s)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    texts : Union[str, List[str]]\n",
    "        An input string or a list of input strings to tokenize\n",
    "\n",
    "    context_length : int\n",
    "        The context length to use; all CLIP models use 77 as the context length\n",
    "\n",
    "    truncate: bool\n",
    "        Whether to truncate the text in case its encoding is longer than the context length\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A two-dimensional tensor containing the resulting tokens, shape = [number of input strings, context_length]\n",
    "    \"\"\"\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "\n",
    "    sot_token = _tokenizer.encoder[\"<|startoftext|>\"]\n",
    "    eot_token = _tokenizer.encoder[\"<|endoftext|>\"]\n",
    "    all_tokens = [[sot_token] + _tokenizer.encode(text) + [eot_token] for text in texts]\n",
    "    result = torch.zeros(len(all_tokens), context_length, dtype=torch.long)\n",
    "\n",
    "    for i, tokens in enumerate(all_tokens):\n",
    "        if len(tokens) > context_length:\n",
    "            if truncate:\n",
    "                tokens = tokens[:context_length]\n",
    "                tokens[-1] = eot_token\n",
    "            else:\n",
    "                raise RuntimeError(f\"Input {texts[i]} is too long for context length {context_length}\")\n",
    "        result[i, :len(tokens)] = torch.tensor(tokens)\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tokenizer = SimpleTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__all__ = [\"available_models\", \"load\", \"tokenize\"]\n",
    "\n",
    "_MODELS = {\n",
    "    \"RN50\": \"https://openaipublic.azureedge.net/clip/models/afeb0e10f9e5a86da6080e35cf09123aca3b358a0c3e3b6c78a7b63bc04b6762/RN50.pt\",\n",
    "    \"RN101\": \"https://openaipublic.azureedge.net/clip/models/8fa8567bab74a42d41c5915025a8e4538c3bdbe8804a470a72f30b0d94fab599/RN101.pt\",\n",
    "    \"RN50x4\": \"https://openaipublic.azureedge.net/clip/models/7e526bd135e493cef0776de27d5f42653e6b4c8bf9e0f653bb11773263205fdd/RN50x4.pt\",\n",
    "    \"RN50x16\": \"https://openaipublic.azureedge.net/clip/models/52378b407f34354e150460fe41077663dd5b39c54cd0bfd2b27167a4a06ec9aa/RN50x16.pt\",\n",
    "    \"ViT-B/32\": \"https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt\",\n",
    "    \"ViT-B/16\": \"https://openaipublic.azureedge.net/clip/models/5806e77cd80f8b59890b7e101eabd078d9fb84e6937f9e85e4ecb61988df416f/ViT-B-16.pt\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1736782222337,
     "user": {
      "displayName": "Sergio Neres Pereira Junior",
      "userId": "05770949936273593301"
     },
     "user_tz": 180
    },
    "id": "KbEsq3Rw0zdc"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/uc1add2a09dc85742dea98fbdba71022/FER-generalizavel/src/clip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 21888,
     "status": "ok",
     "timestamp": 1736782244220,
     "user": {
      "displayName": "Sergio Neres Pereira Junior",
      "userId": "05770949936273593301"
     },
     "user_tz": 180
    },
    "id": "xsWRxVafuQ3C"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import csv\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import pickle\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data\n",
    "import pandas as pd\n",
    "import random\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14797,
     "status": "ok",
     "timestamp": 1736782259010,
     "user": {
      "displayName": "Sergio Neres Pereira Junior",
      "userId": "05770949936273593301"
     },
     "user_tz": 180
    },
    "id": "0ybRCZQUukWl",
    "outputId": "3cc11ee8-a377-462a-9214-6b1f16824f2e"
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "device = torch.device(\"xpu\" if torch.xpu.is_available() else \"cpu\")\n",
    "clip_model, preprocess = load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 425,
     "status": "ok",
     "timestamp": 1736782259432,
     "user": {
      "displayName": "Sergio Neres Pereira Junior",
      "userId": "05770949936273593301"
     },
     "user_tz": 180
    },
    "id": "8zH1mnyP_qbW"
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--raf_path', type=str, default='../../data/raf-basic', help='raf_dataset_path')\n",
    "parser.add_argument('--resnet50_path', type=str, default='../../data/resnet50_ft_weight.pkl', help='pretrained_backbone_path')\n",
    "parser.add_argument('--label_path', type=str, default='list_patition_label.txt', help='label_path')\n",
    "parser.add_argument('--workers', type=int, default=2, help='number of workers')\n",
    "parser.add_argument('--batch_size', type=int, default=32, help='batch_size')\n",
    "parser.add_argument('--w', type=int, default=7, help='width of the attention map')\n",
    "parser.add_argument('--h', type=int, default=7, help='height of the attention map')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='the number of the device')\n",
    "parser.add_argument('--lam', type=float, default=5, help='kl_lambda')\n",
    "parser.add_argument('--epochs', type=int, default=60, help='number of epochs')\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7gTrxyDwX05"
   },
   "source": [
    "### Arquitetura do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1736782259432,
     "user": {
      "displayName": "Sergio Neres Pereira Junior",
      "userId": "05770949936273593301"
     },
     "user_tz": 180
    },
    "id": "Uc94_GwGwSnP"
   },
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1736782259433,
     "user": {
      "displayName": "Sergio Neres Pereira Junior",
      "userId": "05770949936273593301"
     },
     "user_tz": 180
    },
    "id": "TYyPd4MowTW4"
   },
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride = 1, downsample = False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size = 3,\n",
    "                               stride = stride, padding = 1, bias = False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size = 3,\n",
    "                               stride = 1, padding = 1, bias = False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace = True)\n",
    "\n",
    "        if downsample:\n",
    "            conv = nn.Conv2d(in_channels, out_channels, kernel_size = 1,\n",
    "                             stride = stride, bias = False)\n",
    "            bn = nn.BatchNorm2d(out_channels)\n",
    "            downsample = nn.Sequential(conv, bn)\n",
    "        else:\n",
    "            downsample = None\n",
    "\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        i = x\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            i = self.downsample(i)\n",
    "\n",
    "        x += i\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 490,
     "status": "ok",
     "timestamp": 1736782259918,
     "user": {
      "displayName": "Sergio Neres Pereira Junior",
      "userId": "05770949936273593301"
     },
     "user_tz": 180
    },
    "id": "-kvynhInwkJI"
   },
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, n_blocks, channels, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        self.in_channels = channels[0]\n",
    "\n",
    "        assert len(n_blocks) == len(channels) == 4\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, self.in_channels, kernel_size = 7, stride = 2, padding = 3, bias = False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.in_channels)\n",
    "        self.relu = nn.ReLU(inplace = True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)\n",
    "\n",
    "        self.layer1 = self.get_resnet_layer(block, n_blocks[0], channels[0])\n",
    "        self.layer2 = self.get_resnet_layer(block, n_blocks[1], channels[1], stride = 2)\n",
    "        self.layer3 = self.get_resnet_layer(block, n_blocks[2], channels[2], stride = 2)\n",
    "        self.layer4 = self.get_resnet_layer(block, n_blocks[3], channels[3], stride = 2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc = nn.Linear(self.in_channels, output_dim)\n",
    "\n",
    "    def get_resnet_layer(self, block=BasicBlock, n_blocks=[2,2,2,2], channels=[64, 128, 256, 512], stride = 1):\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        if self.in_channels != block.expansion * channels:\n",
    "            downsample = True\n",
    "        else:\n",
    "            downsample = False\n",
    "\n",
    "        layers.append(block(self.in_channels, channels, stride, downsample))\n",
    "\n",
    "        for i in range(1, n_blocks):\n",
    "            layers.append(block(block.expansion * channels, channels))\n",
    "\n",
    "        self.in_channels = block.expansion * channels\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        h = x.view(x.shape[0], -1)\n",
    "        x = self.fc(h)\n",
    "\n",
    "        return x, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1736782259919,
     "user": {
      "displayName": "Sergio Neres Pereira Junior",
      "userId": "05770949936273593301"
     },
     "user_tz": 180
    },
    "id": "vcdd3Kpuwr4s"
   },
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 271,
     "status": "ok",
     "timestamp": 1736782517023,
     "user": {
      "displayName": "Sergio Neres Pereira Junior",
      "userId": "05770949936273593301"
     },
     "user_tz": 180
    },
    "id": "w7Ktp8OsTcjG"
   },
   "outputs": [],
   "source": [
    "def Mask(nb_batch):\n",
    "    bar = []\n",
    "    for i in range(7):\n",
    "        foo = [1] * 63 + [0] * 10\n",
    "        if i == 6:\n",
    "            foo = [1] * 64 + [0] * 10\n",
    "        random.shuffle(foo)  # generate mask\n",
    "        bar += foo\n",
    "    bar = [bar for _ in range(nb_batch)]\n",
    "    bar = np.array(bar).astype(\"float32\")\n",
    "    bar = bar.reshape(nb_batch, 512, 1, 1)\n",
    "    bar = torch.from_numpy(bar)\n",
    "\n",
    "    # Verifica se XPU está disponível\n",
    "    device = torch.device(\"xpu\" if torch.xpu.is_available() else \"cpu\")\n",
    "    bar = bar.to(device)\n",
    "    bar = Variable(bar)\n",
    "    return bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1736782259919,
     "user": {
      "displayName": "Sergio Neres Pereira Junior",
      "userId": "05770949936273593301"
     },
     "user_tz": 180
    },
    "id": "WiNmF39ESMlG"
   },
   "outputs": [],
   "source": [
    "###### channel separation and channel diverse loss\n",
    "def supervisor(x, targets, cnum):\n",
    "    branch = x\n",
    "    branch = branch.reshape(branch.size(0),branch.size(1), 1, 1)\n",
    "    branch = my_MaxPool2d(kernel_size=(1,cnum), stride=(1,cnum))(branch)\n",
    "    branch = branch.reshape(branch.size(0),branch.size(1), branch.size(2) * branch.size(3))\n",
    "    loss_2 = 1.0 - 1.0*torch.mean(torch.sum(branch,2))/cnum # set margin = 3.0\n",
    "\n",
    "    mask = Mask(x.size(0))\n",
    "    branch_1 = x.reshape(x.size(0),x.size(1), 1, 1) * mask\n",
    "    branch_1 = my_MaxPool2d(kernel_size=(1,cnum), stride=(1,cnum))(branch_1)\n",
    "    branch_1 = branch_1.view(branch_1.size(0), -1)\n",
    "    loss_1 = nn.CrossEntropyLoss()(branch_1, targets)\n",
    "    return [loss_1, loss_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 293,
     "status": "ok",
     "timestamp": 1736782522392,
     "user": {
      "displayName": "Sergio Neres Pereira Junior",
      "userId": "05770949936273593301"
     },
     "user_tz": 180
    },
    "id": "fj5xkMsyTK68"
   },
   "outputs": [],
   "source": [
    "from torch.nn.modules.utils import _single, _pair, _triple\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "class my_MaxPool2d(nn.Module):\n",
    "    def __init__(self, kernel_size, stride=None, padding=0, dilation=1,\n",
    "                 return_indices=False, ceil_mode=False):\n",
    "        super(my_MaxPool2d, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride or kernel_size\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.return_indices = return_indices\n",
    "        self.ceil_mode = ceil_mode\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = input.transpose(3,1)\n",
    "\n",
    "\n",
    "        input = F.max_pool2d(input, self.kernel_size, self.stride,\n",
    "                            self.padding, self.dilation, self.ceil_mode,\n",
    "                            self.return_indices)\n",
    "        input = input.transpose(3,1).contiguous()\n",
    "\n",
    "        return input\n",
    "\n",
    "    def __repr__(self):\n",
    "        kh, kw = _pair(self.kernel_size)\n",
    "        dh, dw = _pair(self.stride)\n",
    "        padh, padw = _pair(self.padding)\n",
    "        dilh, dilw = _pair(self.dilation)\n",
    "        padding_str = ', padding=(' + str(padh) + ', ' + str(padw) + ')' \\\n",
    "            if padh != 0 or padw != 0 else ''\n",
    "        dilation_str = (', dilation=(' + str(dilh) + ', ' + str(dilw) + ')'\n",
    "                        if dilh != 0 and dilw != 0 else '')\n",
    "        ceil_str = ', ceil_mode=' + str(self.ceil_mode)\n",
    "        return self.__class__.__name__ + '(' \\\n",
    "            + 'kernel_size=(' + str(kh) + ', ' + str(kw) + ')' \\\n",
    "            + ', stride=(' + str(dh) + ', ' + str(dw) + ')' \\\n",
    "            + padding_str + dilation_str + ceil_str + ')'\n",
    "\n",
    "\n",
    "class my_AvgPool2d(nn.Module):\n",
    "    def __init__(self, kernel_size, stride=None, padding=0, ceil_mode=False,\n",
    "                 count_include_pad=True):\n",
    "        super(my_AvgPool2d, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride or kernel_size\n",
    "        self.padding = padding\n",
    "        self.ceil_mode = ceil_mode\n",
    "        self.count_include_pad = count_include_pad\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = input.transpose(3,1)\n",
    "        input = F.avg_pool2d(input, self.kernel_size, self.stride,\n",
    "                            self.padding, self.ceil_mode, self.count_include_pad)\n",
    "        input = input.transpose(3,1).contiguous()\n",
    "        return input\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(' \\\n",
    "            + 'kernel_size=' + str(self.kernel_size) \\\n",
    "            + ', stride=' + str(self.stride) \\\n",
    "            + ', padding=' + str(self.padding) \\\n",
    "            + ', ceil_mode=' + str(self.ceil_mode) \\\n",
    "            + ', count_include_pad=' + str(self.count_include_pad) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1736782371449,
     "user": {
      "displayName": "Sergio Neres Pereira Junior",
      "userId": "05770949936273593301"
     },
     "user_tz": 180
    },
    "id": "ahqTRFFSwsXl"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, pretrained=True, num_classes=7, drop_rate=0, model_path=None):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        res18 = ResNet(block = BasicBlock, n_blocks = [2,2,2,2], channels = [64, 128, 256, 512], output_dim=1000)\n",
    "        msceleb_model = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "        state_dict = msceleb_model['state_dict']\n",
    "        res18.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "        self.drop_rate = drop_rate\n",
    "        self.features = nn.Sequential(*list(res18.children())[:-2])\n",
    "        self.features2 = nn.Sequential(*list(res18.children())[-2:-1])\n",
    "\n",
    "        fc_in_dim = list(res18.children())[-1].in_features  # original fc layer's in dimention 512\n",
    "        self.fc = nn.Linear(fc_in_dim, num_classes)  # new fc layer 512x7\n",
    "\n",
    "        self.parm={}\n",
    "        for name,parameters in self.fc.named_parameters():\n",
    "            print(name,':',parameters.size())\n",
    "            self.parm[name]=parameters\n",
    "\n",
    "    def forward(self, x, clip_model, targets, phase='train'):\n",
    "        with torch.no_grad():\n",
    "            image_features = clip_model.encode_image(x)\n",
    "\n",
    "        x = self.features(x)\n",
    "        feat = x\n",
    "\n",
    "        x = self.features2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        ################### sigmoid mask (important)\n",
    "        if phase=='train':\n",
    "            MC_loss = supervisor(image_features * torch.sigmoid(x), targets, cnum=73)\n",
    "\n",
    "        x = image_features * torch.sigmoid(x)\n",
    "        out = self.fc(x)\n",
    "\n",
    "        if phase=='train':\n",
    "            return out, MC_loss\n",
    "        else:\n",
    "            return out, out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lkEjrsCVwmUg"
   },
   "source": [
    "### Treinamento E Teste Codigos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, train_loader, optimizer, scheduler, device):\n",
    "  running_loss = 0.0\n",
    "  iter_cnt = 0\n",
    "  correct_sum = 0\n",
    "\n",
    "  model.to(device)\n",
    "  model.train()\n",
    "\n",
    "  total_loss = []\n",
    "  with tqdm(total=len(train_loader)) as pbar:\n",
    "      for batch_i, (imgs1, labels) in enumerate(train_loader):\n",
    "        imgs1 = imgs1.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "        output, MC_loss = model(imgs1, clip_model, labels, phase='train')\n",
    "\n",
    "        loss1 = nn.CrossEntropyLoss()(output, labels)\n",
    "\n",
    "        loss = loss1 + 5 * MC_loss[1] + 1.5 * MC_loss[0]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        iter_cnt += 1\n",
    "        _, predicts = torch.max(output, 1)\n",
    "        correct_num = torch.eq(predicts, labels).sum()\n",
    "        correct_sum += correct_num\n",
    "        running_loss += loss\n",
    "\n",
    "        pbar.update(1)  # Update progress bar for each batch\n",
    "\n",
    "  scheduler.step()\n",
    "  running_loss = running_loss / iter_cnt\n",
    "  acc = correct_sum.float() / float(train_loader.dataset.__len__())\n",
    "  return acc, running_loss\n",
    "\n",
    "setup_seed(3407)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, device):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        iter_cnt = 0\n",
    "        correct_sum = 0\n",
    "        data_num = 0\n",
    "\n",
    "\n",
    "        for batch_i, (imgs1, labels) in enumerate(test_loader):\n",
    "            imgs1 = imgs1.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "\n",
    "            outputs, _ = model(imgs1, clip_model, labels, phase='test')\n",
    "\n",
    "\n",
    "            loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "\n",
    "            iter_cnt += 1\n",
    "            _, predicts = torch.max(outputs, 1)\n",
    "\n",
    "            correct_num = torch.eq(predicts, labels).sum()\n",
    "            correct_sum += correct_num\n",
    "\n",
    "            running_loss += loss\n",
    "            data_num += outputs.size(0)\n",
    "\n",
    "        running_loss = running_loss / iter_cnt\n",
    "        test_acc = correct_sum.float() / float(data_num)\n",
    "        \n",
    "    return test_acc, running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_path = '/home/uc1add2a09dc85742dea98fbdba71022/FER-generalizavel/models/resnet18_msceleb.pth'\n",
    "model = Model(model_path = model_path)\n",
    "device = torch.device(\"xpu\" if torch.xpu.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0002, weight_decay=0.0001)\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_transforms = transforms.Compose([\n",
    "    #transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    #transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomErasing(scale=(0.02, 0.25))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caregamento de Dataset RafDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "kaggle_path = os.path.expanduser(\"~/.local/bin/kaggle\")\n",
    "\n",
    "subprocess.run([kaggle_path, \"datasets\", \"download\", \"-d\", \"kuryakin/eacdata\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import os\n",
    "# import image_utils\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "class RafDataSet(data.Dataset):\n",
    "    def __init__(self, raf_path, idxs_raf, idxs_test, dataidxs=None, train=True, transform=None, basic_aug=False, download=False):\n",
    "        self.train = train\n",
    "        self.dataidxs = dataidxs\n",
    "        self.transform = transform\n",
    "        self.raf_path = raf_path\n",
    "        self.idxs_raf = idxs_raf\n",
    "        self.idxs_test = idxs_test\n",
    "\n",
    "        NAME_COLUMN = 0\n",
    "        LABEL_COLUMN = 1\n",
    "        df = pd.read_csv(os.path.join(self.raf_path, 'EmoLabel/list_patition_label.txt'), sep=' ', header=None)\n",
    "        if self.train:\n",
    "            dataset = df[df[NAME_COLUMN].str.startswith('train')]\n",
    "        else:\n",
    "            dataset = df[df[NAME_COLUMN].str.startswith('test')]\n",
    "        file_names = dataset.iloc[:, NAME_COLUMN].values\n",
    "        self.target = dataset.iloc[:, LABEL_COLUMN].astype(int).values - 1  # 0:Surprise, 1:Fear, 2:Disgust, 3:Happiness, 4:Sadness, 5:Anger, 6:Neutral\n",
    "        self.target = np.array(self.target)\n",
    "\n",
    "        self.file_paths = []\n",
    "        for f in file_names:    # use raf-db aligned images for training/testing\n",
    "            f = f.split(\".\")[0]\n",
    "            f = f + \"_aligned.jpg\"\n",
    "            print(f)\n",
    "            path = os.path.join(self.raf_path, 'Image/aligned', f)\n",
    "            self.file_paths.append(path)\n",
    "\n",
    "        self.basic_aug = basic_aug\n",
    "        ################\n",
    "        self.file_paths = np.array(self.file_paths)\n",
    "        if self.dataidxs is not None:\n",
    "            self.file_paths = self.file_paths[self.dataidxs]\n",
    "            self.target = self.target[self.dataidxs]\n",
    "        else:\n",
    "            self.file_paths = self.file_paths\n",
    "        self.file_paths = self.file_paths.tolist()\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def get_labels(self):\n",
    "        return self.target\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.file_paths[idx]\n",
    "        sample = cv2.imread(path)\n",
    "        sample = sample[:, :, ::-1]  # BGR to RGB (Optional)\n",
    "        target = self.target[idx]\n",
    "\n",
    "        target = self.idxs_test[self.idxs_raf[target]]\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            \n",
    "            sample = Image.fromarray(sample.copy())  # Convert NumPy array to PIL image\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        return sample, target  # , idx (Optional to return index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_to_index = {\n",
    "            \"surprise\": 0,\n",
    "            \"fear\": 1,\n",
    "            \"disgust\": 2,\n",
    "            \"happiness\": 3,\n",
    "            \"sadness\": 4,\n",
    "            \"angry\": 5,\n",
    "            \"neutral\": 6\n",
    "        }\n",
    "\n",
    "index_to_emotion = {v: k for k, v in emotion_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_dataset = RafDataSet('../data/raf-basic', idxs_raf=index_to_emotion, idxs_test=emotion_to_index, train=False, transform=eval_transforms)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=args.workers,\n",
    "                                          pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset = RafDataSet('../data/raf-basic', idxs_raf=index_to_emotion, idxs_test=emotion_to_index, train=True, transform=train_transforms)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                           batch_size=args.batch_size,\n",
    "                                           #batch_size=1,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=args.workers,\n",
    "                                           pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Obtendo o batch de imagens e rótulos\n",
    "for images, labels in test_loader:\n",
    "    # Se você quiser mostrar apenas um batch\n",
    "    break\n",
    "\n",
    "# Definindo o layout para 4 linhas e 8 colunas\n",
    "fig, axes = plt.subplots(4, 8, figsize=(20, 10))  # 4x8 layout\n",
    "axes = axes.flatten()  # Flatten para facilitar a iteração\n",
    "\n",
    "# Loop para exibir as imagens no grid\n",
    "for i, (img, label) in enumerate(zip(images, labels)):\n",
    "    if i >= len(axes):  # Se houver mais imagens do que subgráficos\n",
    "        break\n",
    "\n",
    "    # Convertendo a imagem para numpy e normalizando\n",
    "    img_np = img.permute(1, 2, 0).numpy()\n",
    "    img_np = (img_np - img_np.min()) / (img_np.max() - img_np.min())\n",
    "\n",
    "    # Exibindo a imagem\n",
    "    ax = axes[i]\n",
    "    ax.imshow(img_np)\n",
    "    ax.axis('off')  # Desativar os eixos\n",
    "\n",
    "    # Usando o mapa de rótulos para mostrar o nome da emoção\n",
    "    label_name = index_to_emotion[label.item()]\n",
    "    ax.set_title(f\"{label_name}\", fontsize=10)  # Título com o nome do label\n",
    "\n",
    "# Ajustar o layout para não sobrepor as imagens\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_acc = 0\n",
    "patience = 100  # Number of epochs to wait for improvement\n",
    "no_improvement = 0\n",
    "\n",
    "for i in range(1, args.epochs + 1):\n",
    "    train_acc, train_loss = train(model, train_loader, optimizer, scheduler, device)\n",
    "    test_acc, test_loss = test(model, test_loader, device)\n",
    "    print('epoch: ', i, 'acc_test: ', test_acc, 'acc_train: ', train_acc)\n",
    "\n",
    "    # Early stopping logic with patience\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "        no_improvement = 0  # Reset patience counter on improvement\n",
    "        torch.save({'model_state_dict': model.state_dict(),}, \"/home/uc1add2a09dc85742dea98fbdba71022/FER-generalizavel/models/ours_best_RAFDB.pth\")\n",
    "    else:\n",
    "        no_improvement += 1  # Increment patience counter on no improvement\n",
    "\n",
    "    if no_improvement == patience:\n",
    "        print(f\"Early stopping after {i} epochs with no improvement in test accuracy\")\n",
    "        break  # Exit the training loop if patience is exhausted\n",
    "\n",
    "    #torch.save({'model_state_dict': model.state_dict(),}, \"/kaggle/working/ours_final_RAFDB.pth\")\n",
    "    with open('results.txt', 'a') as f:\n",
    "        f.write(str(i)+'_'+str(test_acc)+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YblEzXiawbnA"
   },
   "source": [
    "### Caregamento de Dataset FER+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Caminho para o seu arquivo kaggle.json\n",
    "kaggle_json_path = os.path.expanduser(\"~/FER-generalizavel/addons/kaggle.json\")\n",
    "\n",
    "# Carrega o JSON com as credenciais\n",
    "with open(kaggle_json_path) as f:\n",
    "    kaggle_creds = json.load(f)\n",
    "\n",
    "# Define as variáveis de ambiente\n",
    "os.environ['KAGGLE_USERNAME'] = kaggle_creds['username']\n",
    "os.environ['KAGGLE_KEY'] = kaggle_creds['key']\n",
    "\n",
    "# Agora pode importar e usar a API\n",
    "import kaggle\n",
    "\n",
    "kaggle_path = os.path.expanduser(\"~/FER-generalizavel/data\")\n",
    "kaggle.api.dataset_download_files('ss1033741293/ferplus', path=kaggle_path, unzip=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1736782274014,
     "user": {
      "displayName": "Sergio Neres Pereira Junior",
      "userId": "05770949936273593301"
     },
     "user_tz": 180
    },
    "id": "n2feF-1szmkc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class FERPlusDataset(Dataset):\n",
    "    def __init__(self, root_dir, idxs_fer, idxs_test, subset=\"FER2013Train\", transform=None):\n",
    "        \"\"\"\n",
    "        Classe para lidar com o dataset FERPlus.\n",
    "\n",
    "        Args:\n",
    "            root_dir (str): Diretório raiz do dataset (ex: 'FER2013Plus').\n",
    "            subset (str): Subconjunto a ser usado ('FER2013Train', 'FER2013Test', 'FER2013Valid').\n",
    "            transform (callable, optional): Transformações para aplicar nas imagens.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "        self.idxs_fer = idxs_fer\n",
    "        self.idxs_test = idxs_test\n",
    "\n",
    "        # Caminhos para imagens e labels\n",
    "        self.images_dir = os.path.join(root_dir, \"Images\", subset)\n",
    "        self.labels_path = os.path.join(root_dir, \"Labels\", subset, \"label.csv\")\n",
    "\n",
    "        # Carregar o arquivo de labels\n",
    "        if not os.path.exists(self.labels_path):\n",
    "            raise FileNotFoundError(f\"Arquivo de labels não encontrado: {self.labels_path}\")\n",
    "\n",
    "        self.columns = [\n",
    "            \"image_name\", \"format\", \"neutral\", \"happiness\", \"surprise\", \"sadness\",\n",
    "            \"anger\", \"disgust\", \"fear\", \"contempt\", \"unknown\", \"NF\"\n",
    "        ]\n",
    "\n",
    "        self.labels = pd.read_csv(self.labels_path, header=None, names=self.columns)\n",
    "\n",
    "        # Validar se os arquivos de imagem existem\n",
    "        self.image_files = self.labels['image_name']\n",
    "\n",
    "        # Dicionário para mapear emoções para índices\n",
    "        self.emotion_to_index = {\n",
    "            \"neutral\": 0,\n",
    "            \"happiness\": 1,\n",
    "            \"surprise\": 2,\n",
    "            \"sadness\": 3,\n",
    "            \"anger\": 4,\n",
    "            \"disgust\": 5,\n",
    "            \"fear\": 6\n",
    "        }\n",
    "\n",
    "    def get_single_label_filtered(self, row):\n",
    "        \"\"\"\n",
    "        Obtém o índice do rótulo mais votado entre as emoções, excluindo \"unknown\" e \"NF\".\n",
    "\n",
    "        Args:\n",
    "            row (pd.Series): Linha do DataFrame de rótulos.\n",
    "\n",
    "        Returns:\n",
    "            int: Índice do rótulo mais votado.\n",
    "        \"\"\"\n",
    "        # Filtrar rótulos \"unknown\" e \"NF\"\n",
    "        emotion_columns = [\"neutral\", \"happiness\", \"surprise\", \"sadness\",\n",
    "                           \"anger\", \"disgust\", \"fear\"]\n",
    "        # Obter o nome do rótulo mais votado\n",
    "        emotion_name = row[emotion_columns].idxmax()\n",
    "        # Retornar o índice correspondente\n",
    "        return self.emotion_to_index[emotion_name]\n",
    "\n",
    "    def __len__(self):\n",
    "        #return 1\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, slice):\n",
    "            raise NotImplementedError(\"Slices não são suportados nesta implementação.\")\n",
    "\n",
    "        # Obter caminho da imagem\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.images_dir, img_name)\n",
    "\n",
    "        # Carregar imagem\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # Aplicar transformações se existirem\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Obter o rótulo correspondente\n",
    "        label_row = self.labels.iloc[idx]\n",
    "        single_label = self.get_single_label_filtered(label_row)\n",
    "\n",
    "        single_label = self.idxs_test[self.idxs_fer[single_label]]\n",
    "\n",
    "        return image, single_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 331,
     "status": "ok",
     "timestamp": 1736782274341,
     "user": {
      "displayName": "Sergio Neres Pereira Junior",
      "userId": "05770949936273593301"
     },
     "user_tz": 180
    },
    "id": "CY0B9YMxzq57",
    "outputId": "32036971-5a85-42cb-fcae-e14a65b2512c"
   },
   "outputs": [],
   "source": [
    "root_dir =  os.path.expanduser(\"~/FER-generalizavel/data/FER2013Plus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1736782274341,
     "user": {
      "displayName": "Sergio Neres Pereira Junior",
      "userId": "05770949936273593301"
     },
     "user_tz": 180
    },
    "id": "licDm86q4Xho"
   },
   "outputs": [],
   "source": [
    "emotion_to_index = {\n",
    "            \"neutral\": 0,\n",
    "            \"happiness\": 1,\n",
    "            \"surprise\": 2,\n",
    "            \"sadness\": 3,\n",
    "            \"anger\": 4,\n",
    "            \"disgust\": 5,\n",
    "            \"fear\": 6\n",
    "        }\n",
    "\n",
    "index_to_emotion = {v: k for k, v in emotion_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1736782274341,
     "user": {
      "displayName": "Sergio Neres Pereira Junior",
      "userId": "05770949936273593301"
     },
     "user_tz": 180
    },
    "id": "yl2C2-Xv39u5"
   },
   "outputs": [],
   "source": [
    "train_dataset = FERPlusDataset(root_dir=root_dir, idxs_fer=index_to_emotion, idxs_test=emotion_to_index, subset=\"FER2013Train\", transform=train_transforms)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                           batch_size=args.batch_size,\n",
    "                                           #batch_size=1,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=args.workers,\n",
    "                                           pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 254,
     "status": "ok",
     "timestamp": 1736705738313,
     "user": {
      "displayName": "Sergio Neres Pereira Junior",
      "userId": "05770949936273593301"
     },
     "user_tz": 180
    },
    "id": "ut6ROsPfxglo"
   },
   "outputs": [],
   "source": [
    "test_dataset = FERPlusDataset(root_dir=root_dir, idxs_fer=index_to_emotion, idxs_test=emotion_to_index, subset=\"FER2013Test\", transform=eval_transforms)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=args.workers,\n",
    "                                          pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5689,
     "status": "ok",
     "timestamp": 1736782280028,
     "user": {
      "displayName": "Sergio Neres Pereira Junior",
      "userId": "05770949936273593301"
     },
     "user_tz": 180
    },
    "id": "LOT8SIFf3yF3",
    "outputId": "cee5b32e-3b5f-4048-ad40-477b63a851ad"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Obtendo o batch de imagens e rótulos\n",
    "for images, labels in train_loader:\n",
    "    # Se você quiser mostrar apenas um batch\n",
    "    break\n",
    "\n",
    "# Definindo o layout para 4 linhas e 8 colunas\n",
    "fig, axes = plt.subplots(4, 8, figsize=(20, 10))  # 4x8 layout\n",
    "axes = axes.flatten()  # Flatten para facilitar a iteração\n",
    "\n",
    "# Loop para exibir as imagens no grid\n",
    "for i, (img, label) in enumerate(zip(images, labels)):\n",
    "    if i >= len(axes):  # Se houver mais imagens do que subgráficos\n",
    "        break\n",
    "\n",
    "    # Convertendo a imagem para numpy e normalizando\n",
    "    img_np = img.permute(1, 2, 0).numpy()\n",
    "    img_np = (img_np - img_np.min()) / (img_np.max() - img_np.min())\n",
    "\n",
    "    # Exibindo a imagem\n",
    "    ax = axes[i]\n",
    "    ax.imshow(img_np)\n",
    "    ax.axis('off')  # Desativar os eixos\n",
    "\n",
    "    # Usando o mapa de rótulos para mostrar o nome da emoção\n",
    "    label_name = index_to_emotion[label.item()]\n",
    "    ax.set_title(f\"{label_name}\", fontsize=10)  # Título com o nome do label\n",
    "\n",
    "# Ajustar o layout para não sobrepor as imagens\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_path = '/home/uc1add2a09dc85742dea98fbdba71022/FER-generalizavel/models/resnet18_msceleb.pth'\n",
    "model = Model(model_path = model_path)\n",
    "device = torch.device(\"xpu\" if torch.xpu.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_acc = 0\n",
    "patience = 1\n",
    "no_improvement = 0\n",
    "\n",
    "for i in range(1, args.epochs + 1):\n",
    "    train_acc, train_loss = train(model, train_loader, optimizer, scheduler, device)\n",
    "    test_acc, test_loss = test(model, test_loader, device)\n",
    "    print('epoch: ', i, 'acc_test: ', test_acc, 'acc_train: ', train_acc)\n",
    "\n",
    "    # Early stopping logic with patience\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "        no_improvement = 0  # Reset patience counter on improvement\n",
    "        torch.save({'model_state_dict': model.state_dict(),}, os.path.expanduser(\"~/FER-generalizavel/models/ours_best_FERPlus.pth\"))\n",
    "    else:\n",
    "        continue\n",
    "        #no_improvement += 1  # Increment patience counter on no improvement\n",
    "\n",
    "    #if no_improvement == patience:\n",
    "        #print(f\"Early stopping after {i} epochs with no improvement in test accuracy\")\n",
    "        #break  # Exit the training loop if patience is exhausted\n",
    "        \n",
    "    #torch.save({'model_state_dict': model.state_dict(),}, \"/kaggle/working/ours_final_FERPlus.pth\")\n",
    "    #with open('results.txt', 'a') as f:\n",
    "    #    f.write(str(i)+'_'+str(test_acc)+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7dbe42be"
   },
   "source": [
    "### test on AffectNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Caminho para o seu arquivo kaggle.json\n",
    "kaggle_json_path = os.path.expanduser(\"~/FER-generalizavel/addons/kaggle.json\")\n",
    "\n",
    "# Carrega o JSON com as credenciais\n",
    "with open(kaggle_json_path) as f:\n",
    "    kaggle_creds = json.load(f)\n",
    "\n",
    "# Define as variáveis de ambiente\n",
    "os.environ['KAGGLE_USERNAME'] = kaggle_creds['username']\n",
    "os.environ['KAGGLE_KEY'] = kaggle_creds['key']\n",
    "\n",
    "# Agora pode importar e usar a API\n",
    "import kaggle\n",
    "\n",
    "kaggle_path = os.path.expanduser(\"~/FER-generalizavel/data\")\n",
    "kaggle.api.dataset_download_files(\"yakhyokhuja/affectnetaligned\", path=kaggle_path, unzip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_to_index = {\n",
    "            \"angry\": 0,\n",
    "            \"disgust\": 1,\n",
    "            \"fear\": 2,\n",
    "            \"happiness\": 3,\n",
    "            \"sadness\": 4,\n",
    "            \"surprise\": 5,\n",
    "            \"neutral\": 6\n",
    "        }\n",
    "\n",
    "index_to_emotion = {v: k for k, v in emotion_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class AffectNetDataset(Dataset):\n",
    "    def __init__(self, root_dir, idx_aff, idx_test, split=\"train\", transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Diretório raiz contendo as pastas train, test e valid.\n",
    "            split (str): Qual partição carregar (\"train\", \"test\" ou \"valid\").\n",
    "            transform (callable, optional): Transformações a serem aplicadas às imagens.\n",
    "        \"\"\"\n",
    "        self.root_dir = os.path.join(root_dir, split)\n",
    "        self.transform = transform\n",
    "        self.idxs_test = idx_test\n",
    "        self.idxs_aff = idx_aff\n",
    "        \n",
    "        self.class_map = {\n",
    "            \"angry\": 0,\n",
    "            \"disgust\": 1,\n",
    "            \"fear\": 2,\n",
    "            \"happiness\": 3,\n",
    "            \"sadness\": 4,\n",
    "            \"surprise\": 5,\n",
    "            \"neutral\": 6\n",
    "        }\n",
    "        \n",
    "        self.samples = []\n",
    "        for class_idx in range(len(self.class_map)):  # Pastas nomeadas por índice\n",
    "            class_path = os.path.join(self.root_dir, str(class_idx))\n",
    "            if os.path.exists(class_path):\n",
    "                for filename in os.listdir(class_path):\n",
    "                    if filename.endswith(('.png', '.jpg', '.jpeg')):  # Filtra apenas imagens\n",
    "                        self.samples.append((os.path.join(class_path, filename), class_idx))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        label = self.idxs_test[self.idxs_aff[label]]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = os.path.expanduser(\"~/FER-generalizavel/data/AffectNetCustom\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AffectNetDataset(root_dir=root_dir, idx_aff=index_to_emotion, idx_test=emotion_to_index, split=\"train\", transform=train_transforms)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                           batch_size=args.batch_size,\n",
    "                                           #batch_size=1,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=args.workers,\n",
    "                                           pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = AffectNetDataset(root_dir=root_dir, idx_aff=index_to_emotion, idx_test=emotion_to_index, split=\"test\", transform=eval_transforms)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=args.workers,\n",
    "                                          pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Obtendo o batch de imagens e rótulos\n",
    "for images, labels in train_loader:\n",
    "    # Se você quiser mostrar apenas um batch\n",
    "    break\n",
    "\n",
    "# Definindo o layout para 4 linhas e 8 colunas\n",
    "fig, axes = plt.subplots(4, 8, figsize=(20, 10))  # 4x8 layout\n",
    "axes = axes.flatten()  # Flatten para facilitar a iteração\n",
    "\n",
    "# Loop para exibir as imagens no grid\n",
    "for i, (img, label) in enumerate(zip(images, labels)):\n",
    "    if i >= len(axes):  # Se houver mais imagens do que subgráficos\n",
    "        break\n",
    "\n",
    "    # Convertendo a imagem para numpy e normalizando\n",
    "    img_np = img.permute(1, 2, 0).numpy()\n",
    "    img_np = (img_np - img_np.min()) / (img_np.max() - img_np.min())\n",
    "\n",
    "    # Exibindo a imagem\n",
    "    ax = axes[i]\n",
    "    ax.imshow(img_np)\n",
    "    ax.axis('off')  # Desativar os eixos\n",
    "\n",
    "    # Usando o mapa de rótulos para mostrar o nome da emoção\n",
    "    label_name = index_to_emotion[label.item()]\n",
    "    ax.set_title(f\"{label_name}\", fontsize=10)  # Título com o nome do label\n",
    "\n",
    "# Ajustar o layout para não sobrepor as imagens\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_path = '/home/uc1add2a09dc85742dea98fbdba71022/FER-generalizavel/models/resnet18_msceleb.pth'\n",
    "model = Model(model_path = model_path)\n",
    "device = torch.device(\"xpu\" if torch.xpu.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_acc = 0\n",
    "patience = 5  # Number of epochs to wait for improvement\n",
    "no_improvement = 0\n",
    "\n",
    "for i in range(1, args.epochs + 1):\n",
    "    train_acc, train_loss = train(model, train_loader, optimizer, scheduler, device)\n",
    "    test_acc, test_loss = test(model, test_loader, device)\n",
    "    print('epoch: ', i, 'acc_test: ', test_acc, 'acc_train: ', train_acc)\n",
    "\n",
    "    # Early stopping logic with patience\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "        no_improvement = 0  # Reset patience counter on improvement\n",
    "        torch.save({'model_state_dict': model.state_dict(),},  os.path.expanduser(\"~/FER-generalizavel/models/ours_best_Affectnet.pth\"))\n",
    "    else:\n",
    "        continue\n",
    "        #no_improvement += 1  # Increment patience counter on no improvement\n",
    "\n",
    "    \"\"\"\n",
    "    if no_improvement == patience:\n",
    "        print(f\"Early stopping after {i} epochs with no improvement in test accuracy\")\n",
    "        break  # Exit the training loop if patience is exhausted\n",
    "\n",
    "    torch.save({'model_state_dict': model.state_dict(),}, \"/kaggle/working/ours_final_AffectNet.pth\")\n",
    "    with open('results.txt', 'a') as f:\n",
    "        f.write(str(i)+'_'+str(test_acc)+'\\n')\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train on MMADataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Caminho para o seu arquivo kaggle.json\n",
    "kaggle_json_path = os.path.expanduser(\"~/FER-generalizavel/addons/kaggle.json\")\n",
    "\n",
    "# Carrega o JSON com as credenciais\n",
    "with open(kaggle_json_path) as f:\n",
    "    kaggle_creds = json.load(f)\n",
    "\n",
    "# Define as variáveis de ambiente\n",
    "os.environ['KAGGLE_USERNAME'] = kaggle_creds['username']\n",
    "os.environ['KAGGLE_KEY'] = kaggle_creds['key']\n",
    "\n",
    "# Agora pode importar e usar a API\n",
    "import kaggle\n",
    "\n",
    "kaggle_path = os.path.expanduser(\"~/FER-generalizavel/data\")\n",
    "kaggle.api.dataset_download_files(\"mahmoudima/mma-facial-expression\", path=kaggle_path, unzip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_to_index = {\n",
    "            \"angry\": 0,\n",
    "            \"disgust\": 1,\n",
    "            \"fear\": 2,\n",
    "            \"happiness\": 3,\n",
    "            \"sadness\": 5,\n",
    "            \"surprise\": 6,\n",
    "            \"neutral\": 4\n",
    "        }\n",
    "\n",
    "index_to_emotion = {v: k for k, v in emotion_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MMADataset(Dataset):\n",
    "    def __init__(self, root_dir, idx_mma, idx_test, split=\"train\", transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Diretório raiz contendo as pastas train, test e valid.\n",
    "            split (str): Qual partição carregar (\"train\", \"test\" ou \"valid\").\n",
    "            transform (callable, optional): Transformações a serem aplicadas às imagens.\n",
    "        \"\"\"\n",
    "        self.root_dir = os.path.join(root_dir, split)\n",
    "        self.transform = transform\n",
    "        self.classes = sorted(os.listdir(self.root_dir))  # Lista de emoções\n",
    "        self.idxs_test = idx_test\n",
    "        self.idxs_mma = idx_mma\n",
    "        \n",
    "        self.samples = []\n",
    "        for class_idx, class_name in enumerate(self.classes):\n",
    "            class_path = os.path.join(self.root_dir, class_name)\n",
    "            for filename in os.listdir(class_path):\n",
    "                if filename.endswith(('.png', '.jpg', '.jpeg')):  # Filtra apenas imagens\n",
    "                    self.samples.append((os.path.join(class_path, filename), class_idx))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        label = self.idxs_test[self.idxs_mma[label]]\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = os.path.expanduser(\"~/FER-generalizavel/data\") \"/kaggle/input/mma-facial-expression/MMAFEDB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MMADataset(root_dir=root_dir, idx_mma=index_to_emotion, idx_test=emotion_to_index, split=\"train\", transform=train_transforms)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                           batch_size=args.batch_size,\n",
    "                                           #batch_size=1,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=args.workers,\n",
    "                                           pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = MMADataset(root_dir=root_dir, idx_mma=index_to_emotion, idx_test=emotion_to_index, split=\"test\", transform=eval_transforms)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=args.workers,\n",
    "                                          pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Obtendo o batch de imagens e rótulos\n",
    "for images, labels in train_loader:\n",
    "    # Se você quiser mostrar apenas um batch\n",
    "    break\n",
    "\n",
    "# Definindo o layout para 4 linhas e 8 colunas\n",
    "fig, axes = plt.subplots(4, 8, figsize=(20, 10))  # 4x8 layout\n",
    "axes = axes.flatten()  # Flatten para facilitar a iteração\n",
    "\n",
    "# Loop para exibir as imagens no grid\n",
    "for i, (img, label) in enumerate(zip(images, labels)):\n",
    "    if i >= len(axes):  # Se houver mais imagens do que subgráficos\n",
    "        break\n",
    "\n",
    "    # Convertendo a imagem para numpy e normalizando\n",
    "    img_np = img.permute(1, 2, 0).numpy()\n",
    "    img_np = (img_np - img_np.min()) / (img_np.max() - img_np.min())\n",
    "\n",
    "    # Exibindo a imagem\n",
    "    ax = axes[i]\n",
    "    ax.imshow(img_np)\n",
    "    ax.axis('off')  # Desativar os eixos\n",
    "\n",
    "    # Usando o mapa de rótulos para mostrar o nome da emoção\n",
    "    label_name = index_to_emotion[label.item()]\n",
    "    ax.set_title(f\"{label_name}\", fontsize=10)  # Título com o nome do label\n",
    "\n",
    "# Ajustar o layout para não sobrepor as imagens\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/home/uc1add2a09dc85742dea98fbdba71022/FER-generalizavel/models/resnet18_msceleb.pth'\n",
    "model = Model(model_path = model_path)\n",
    "device = torch.device(\"xpu\" if torch.xpu.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0\n",
    "patience = 5  # Number of epochs to wait for improvement\n",
    "no_improvement = 0\n",
    "\n",
    "for i in range(1, args.epochs + 1):\n",
    "    train_acc, train_loss = train(model, train_loader, optimizer, scheduler, device)\n",
    "    test_acc, test_loss = test(model, test_loader, device)\n",
    "    print('epoch: ', i, 'acc_test: ', test_acc, 'acc_train: ', train_acc)\n",
    "\n",
    "    # Early stopping logic with patience\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "        no_improvement = 0  # Reset patience counter on improvement\n",
    "        torch.save({'model_state_dict': model.state_dict(),}, os.path.expanduser(\"~/FER-generalizavel/models/ours_best_MMA.pth\"))\n",
    "    else:\n",
    "        no_improvement += 1  # Increment patience counter on no improvement\n",
    "\n",
    "    \"\"\"\n",
    "    if no_improvement == patience:\n",
    "        print(f\"Early stopping after {i} epochs with no improvement in test accuracy\")\n",
    "        break  # Exit the training loop if patience is exhausted\n",
    "\n",
    "    torch.save({'model_state_dict': model.state_dict(),}, \"/kaggle/working/ours_final_MMA.pth\")\n",
    "    with open('results.txt', 'a') as f:\n",
    "        f.write(str(i)+'_'+str(test_acc)+'\\n')\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SFEW 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Caminho para o seu arquivo kaggle.json\n",
    "kaggle_json_path = os.path.expanduser(\"~/FER-generalizavel/addons/kaggle.json\")\n",
    "\n",
    "# Carrega o JSON com as credenciais\n",
    "with open(kaggle_json_path) as f:\n",
    "    kaggle_creds = json.load(f)\n",
    "\n",
    "# Define as variáveis de ambiente\n",
    "os.environ['KAGGLE_USERNAME'] = kaggle_creds['username']\n",
    "os.environ['KAGGLE_KEY'] = kaggle_creds['key']\n",
    "\n",
    "# Agora pode importar e usar a API\n",
    "import kaggle\n",
    "\n",
    "kaggle_path = os.path.expanduser(\"~/FER-generalizavel/data\")\n",
    "kaggle.api.dataset_download_files(\"vlntnstarodub/datasetsfew\", path=kaggle_path, unzip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_to_index = {\n",
    "            \"angry\": 0,\n",
    "            \"disgust\": 1,\n",
    "            \"fear\": 2,\n",
    "            \"happiness\": 3,\n",
    "            \"sadness\": 5,\n",
    "            \"surprise\": 6,\n",
    "            \"neutral\": 4\n",
    "        }\n",
    "\n",
    "index_to_emotion = {v: k for k, v in emotion_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SFEWDataset(Dataset):\n",
    "    def __init__(self, root_dir, idx_test, idx_sfew, split=\"Train\", transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Diretório raiz contendo as pastas train, test e valid.\n",
    "            split (str): Qual partição carregar (\"train\", \"test\" ou \"valid\").\n",
    "            transform (callable, optional): Transformações a serem aplicadas às imagens.\n",
    "        \"\"\"\n",
    "        self.root_dir = os.path.join(root_dir, split, \"Test_Aligned_Faces\") if split == \"Test\" else os.path.join(root_dir, split)\n",
    "        print(self.root_dir)\n",
    "        self.transform = transform\n",
    "        self.classes = sorted(os.listdir(self.root_dir))  # Lista de emoções\n",
    "        self.idxs_test = idx_test\n",
    "        self.idxs_sfew = idx_sfew\n",
    "        \n",
    "        self.samples = []\n",
    "        for class_idx, class_name in enumerate(self.classes):\n",
    "            class_path = os.path.join(self.root_dir, class_name)\n",
    "            for filename in os.listdir(class_path):\n",
    "                if filename.endswith(('.png', '.jpg', '.jpeg')):  # Filtra apenas imagens\n",
    "                    self.samples.append((os.path.join(class_path, filename), class_idx))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        label = self.idxs_test[self.idxs_sfew[label]]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = os.path.expanduser(\"~/FER-generalizavel/data\") \"/kaggle/input/datasetsfew\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SFEWDataset(root_dir=root_dir, idx_test=emotion_to_index, idx_sfew=index_to_emotion, split=\"Train\", transform=train_transforms)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                           batch_size=args.batch_size,\n",
    "                                           #batch_size=1,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=args.workers,\n",
    "                                           pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = SFEWDataset(root_dir=root_dir, idx_test=emotion_to_index, idx_sfew=index_to_emotion, split=\"Val\", transform=train_transforms)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=args.workers,\n",
    "                                          pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Obtendo o batch de imagens e rótulos\n",
    "for images, labels in train_loader:\n",
    "    # Se você quiser mostrar apenas um batch\n",
    "    break\n",
    "\n",
    "# Definindo o layout para 4 linhas e 8 colunas\n",
    "fig, axes = plt.subplots(4, 8, figsize=(20, 10))  # 4x8 layout\n",
    "axes = axes.flatten()  # Flatten para facilitar a iteração\n",
    "\n",
    "# Loop para exibir as imagens no grid\n",
    "for i, (img, label) in enumerate(zip(images, labels)):\n",
    "    if i >= len(axes):  # Se houver mais imagens do que subgráficos\n",
    "        break\n",
    "\n",
    "    # Convertendo a imagem para numpy e normalizando\n",
    "    img_np = img.permute(1, 2, 0).numpy()\n",
    "    img_np = (img_np - img_np.min()) / (img_np.max() - img_np.min())\n",
    "\n",
    "    # Exibindo a imagem\n",
    "    ax = axes[i]\n",
    "    ax.imshow(img_np)\n",
    "    ax.axis('off')  # Desativar os eixos\n",
    "\n",
    "    # Usando o mapa de rótulos para mostrar o nome da emoção\n",
    "    label_name = index_to_emotion[label.item()]\n",
    "    ax.set_title(f\"{label_name}\", fontsize=10)  # Título com o nome do label\n",
    "\n",
    "# Ajustar o layout para não sobrepor as imagens\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/home/uc1add2a09dc85742dea98fbdba71022/FER-generalizavel/models/resnet18_msceleb.pth'\n",
    "model = Model(model_path = model_path)\n",
    "device = torch.device(\"xpu\" if torch.xpu.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_acc = 0\n",
    "patience = 100  # Number of epochs to wait for improvement\n",
    "no_improvement = 0\n",
    "\n",
    "for i in range(1, args.epochs + 1):\n",
    "    train_acc, train_loss = train(model, train_loader, optimizer, scheduler, device)\n",
    "    test_acc, test_loss = test(model, test_loader, device)\n",
    "    print('epoch: ', i, 'acc_test: ', test_acc, 'acc_train: ', train_acc)\n",
    "\n",
    "    # Early stopping logic with patience\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "        no_improvement = 0  # Reset patience counter on improvement\n",
    "        torch.save({'model_state_dict': model.state_dict(),}, os.path.expanduser(\"~/FER-generalizavel/models/ours_best_sfew.pth\"))\n",
    "    else:\n",
    "        no_improvement += 1  # Increment patience counter on no improvement\n",
    "\n",
    "    \"\"\"\n",
    "    if no_improvement == patience:\n",
    "        print(f\"Early stopping after {i} epochs with no improvement in test accuracy\")\n",
    "        break  # Exit the training loop if patience is exhausted\n",
    "\n",
    "    torch.save({'model_state_dict': model.state_dict(),}, \"/kaggle/working/ours_final_SFEW.pth\")\n",
    "    with open('results.txt', 'a') as f:\n",
    "        f.write(str(i)+'_'+str(test_acc)+'\\n')\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste entre dominios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_to_index_mma = {\n",
    "            \"angry\": 0,\n",
    "            \"disgust\": 1,\n",
    "            \"fear\": 2,\n",
    "            \"happiness\": 3,\n",
    "            \"sadness\": 5,\n",
    "            \"surprise\": 6,\n",
    "            \"neutral\": 4\n",
    "        }\n",
    "\n",
    "index_to_emotion_mma = {v: k for k, v in emotion_to_index_mma.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_to_index_sfew = {\n",
    "            \"angry\": 0,\n",
    "            \"disgust\": 1,\n",
    "            \"fear\": 2,\n",
    "            \"happiness\": 3,\n",
    "            \"sadness\": 5,\n",
    "            \"surprise\": 6,\n",
    "            \"neutral\": 4\n",
    "        }\n",
    "\n",
    "index_to_emotion_sfew = {v: k for k, v in emotion_to_index_sfew.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_to_index_aff = {\n",
    "            \"angry\": 0,\n",
    "            \"disgust\": 1,\n",
    "            \"fear\": 2,\n",
    "            \"happiness\": 3,\n",
    "            \"sadness\": 4,\n",
    "            \"surprise\": 5,\n",
    "            \"neutral\": 6\n",
    "        }\n",
    "\n",
    "index_to_emotion_aff = {v: k for k, v in emotion_to_index_aff.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_to_index_raf = {\n",
    "            \"surprise\": 0,\n",
    "            \"fear\": 1,\n",
    "            \"disgust\": 2,\n",
    "            \"happiness\": 3,\n",
    "            \"sadness\": 4,\n",
    "            \"angry\": 5,\n",
    "            \"neutral\": 6\n",
    "        }\n",
    "\n",
    "index_to_emotion_raf = {v: k for k, v in emotion_to_index_raf.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_to_index_fer = {\n",
    "            \"neutral\": 0,\n",
    "            \"happiness\": 1,\n",
    "            \"surprise\": 2,\n",
    "            \"sadness\": 3,\n",
    "            \"angry\": 4,\n",
    "            \"disgust\": 5,\n",
    "            \"fear\": 6\n",
    "        }\n",
    "\n",
    "index_to_emotion_fer = {v: k for k, v in emotion_to_index_fer.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### SFEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/kaggle/input/datasetsfew\"\n",
    "dataset_sfew = SFEWDataset(root_dir=root_dir, idx_test=emotion_to_index_sfew, idx_sfew=index_to_emotion_sfew, split=\"Val\", transform=train_transforms)\n",
    "loader_sfew = torch.utils.data.DataLoader(dataset_sfew, batch_size=args.batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=args.workers,\n",
    "                                          pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/kaggle/input/mma-facial-expression/MMAFEDB\"\n",
    "dataset_mma = MMADataset(root_dir=root_dir, idx_test=emotion_to_index_sfew, idx_mma=index_to_emotion_mma, split=\"test\", transform=eval_transforms)\n",
    "loader_mma = torch.utils.data.DataLoader(dataset_mma, batch_size=args.batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=args.workers,\n",
    "                                          pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/kaggle/input/affectnetaligned/AffectNetCustom\"\n",
    "dataset_affect = AffectNetDataset(root_dir=root_dir, idx_test=emotion_to_index_sfew, idx_aff=index_to_emotion_aff, split=\"test\", transform=eval_transforms)\n",
    "loader_affect = torch.utils.data.DataLoader(dataset_affect, batch_size=args.batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=args.workers,\n",
    "                                          pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_raf = RafDataSet('/kaggle/input/eacdata/raf-basic', idxs_test=emotion_to_index_sfew, idxs_raf=index_to_emotion_raf, train=False, transform=train_transforms)\n",
    "loader_raf = torch.utils.data.DataLoader(dataset_raf,\n",
    "                                           batch_size=args.batch_size,\n",
    "                                           #batch_size=1,\n",
    "                                           shuffle=False,\n",
    "                                           num_workers=args.workers,\n",
    "                                           pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/kaggle/input/ferplus/FER2013Plus\"\n",
    "dataset_fer = FERPlusDataset(root_dir=root_dir, idxs_test=emotion_to_index_sfew, idxs_fer=index_to_emotion_fer, subset=\"FER2013Test\", transform=eval_transforms)\n",
    "loader_fer = torch.utils.data.DataLoader(dataset_fer, batch_size=args.batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=args.workers,\n",
    "                                          pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_weights(model, checkpoint):\n",
    "    import collections\n",
    "    if 'state_dict' in checkpoint:\n",
    "        state_dict = checkpoint['state_dict']\n",
    "    else:\n",
    "        state_dict = checkpoint\n",
    "    model_dict = model.state_dict()\n",
    "    new_state_dict = collections.OrderedDict()\n",
    "    matched_layers, discarded_layers = [], []\n",
    "    for k, v in state_dict.items():\n",
    "        # If the pretrained state_dict was saved as nn.DataParallel,\n",
    "        # keys would contain \"module.\", which should be ignored.\n",
    "        if k.startswith('module.'):\n",
    "            k = k[7:]\n",
    "        if k in model_dict and model_dict[k].size() == v.size():\n",
    "            new_state_dict[k] = v\n",
    "            matched_layers.append(k)\n",
    "        else:\n",
    "            discarded_layers.append(k)\n",
    "    # new_state_dict.requires_grad = False\n",
    "    model_dict.update(new_state_dict)\n",
    "\n",
    "    model.load_state_dict(model_dict)\n",
    "    print('load_weight', len(matched_layers))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load('/kaggle/working/ours_best_SFEW.pth')\n",
    "checkpoint = checkpoint[\"model_state_dict\"]\n",
    "model = load_pretrained_weights(model, checkpoint)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###### SFEW\n",
    "acc_sfew_sfew, test_loss = test(model, loader_sfew, device)\n",
    "print('test acc dbtrain-sfew dbtest-sfew: ', acc_sfew_sfew)\n",
    "\n",
    "###### FER\n",
    "acc_sfew_fer, test_loss = test(model, loader_fer, device)\n",
    "print('test acc dbtrain-sfew dbtest-fer: ', acc_sfew_fer)\n",
    "\n",
    "###### AFFECT\n",
    "acc_sfew_affect, test_loss = test(model, loader_affect, device)\n",
    "print('test acc dbtrain-sfew dbtest-affect: ', acc_sfew_affect)\n",
    "\n",
    "###### MMA\n",
    "acc_sfew_mma, test_loss = test(model, loader_mma, device)\n",
    "print('test acc dbtrain-sfew dbtest-mma: ', acc_sfew_mma)\n",
    "\n",
    "### RAFDB\n",
    "acc_sfew_raf, test_loss = test(model, loader_raf, device)\n",
    "print('test acc dbtrain-sfew dbtest-raf: ', acc_sfew_raf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### MMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "root_dir = \"/kaggle/input/datasetsfew\"\n",
    "dataset_sfew = SFEWDataset(root_dir=root_dir, idx_test=emotion_to_index_mma, idx_sfew=index_to_emotion_sfew, split=\"Val\", transform=train_transforms)\n",
    "loader_sfew = torch.utils.data.DataLoader(dataset_sfew, batch_size=args.batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=args.workers,\n",
    "                                          pin_memory=True)\n",
    "\n",
    "root_dir = \"/kaggle/input/mma-facial-expression/MMAFEDB\"\n",
    "dataset_mma = MMADataset(root_dir=root_dir, idx_test=emotion_to_index_mma, idx_mma=index_to_emotion_mma, split=\"test\", transform=eval_transforms)\n",
    "loader_mma = torch.utils.data.DataLoader(dataset_mma, batch_size=args.batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=args.workers,\n",
    "                                          pin_memory=True)\n",
    "\n",
    "root_dir = \"/kaggle/input/affectnetaligned/AffectNetCustom\"\n",
    "dataset_affect = AffectNetDataset(root_dir=root_dir, idx_test=emotion_to_index_mma, idx_aff=index_to_emotion_aff, split=\"test\", transform=eval_transforms)\n",
    "loader_affect = torch.utils.data.DataLoader(dataset_affect, batch_size=args.batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=args.workers,\n",
    "                                          pin_memory=True)\n",
    "\n",
    "dataset_raf = RafDataSet('/kaggle/input/eacdata/raf-basic', idxs_test=emotion_to_index_mma, idxs_raf=index_to_emotion_raf, train=False, transform=train_transforms)\n",
    "loader_raf = torch.utils.data.DataLoader(dataset_raf,\n",
    "                                           batch_size=args.batch_size,\n",
    "                                           #batch_size=1,\n",
    "                                           shuffle=False,\n",
    "                                           num_workers=args.workers,\n",
    "                                           pin_memory=True)\n",
    "\n",
    "root_dir = \"/kaggle/input/ferplus/FER2013Plus\"\n",
    "dataset_fer = FERPlusDataset(root_dir=root_dir, idxs_test=emotion_to_index_mma, idxs_fer=index_to_emotion_fer, subset=\"FER2013Test\", transform=eval_transforms)\n",
    "loader_fer = torch.utils.data.DataLoader(dataset_fer, batch_size=args.batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=args.workers,\n",
    "                                          pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load('/kaggle/working/ours_best_MMA.pth')\n",
    "checkpoint = checkpoint[\"model_state_dict\"]\n",
    "model = load_pretrained_weights(model, checkpoint)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###### SFEW\n",
    "acc_mma_sfew, test_loss = test(model, loader_sfew, device)\n",
    "print('test acc dbtrain-mma dbtest-sfew: ', acc_mma_sfew)\n",
    "\n",
    "###### FER\n",
    "acc_mma_fer, test_loss = test(model, loader_fer, device)\n",
    "print('test acc dbtrain-mma dbtest-fer: ', acc_mma_fer)\n",
    "\n",
    "###### AFFECT\n",
    "acc_mma_affect, test_loss = test(model, loader_affect, device)\n",
    "print('test acc dbtrain-mma dbtest-affect: ', acc_mma_affect)\n",
    "\n",
    "###### MMA\n",
    "acc_mma_mma, test_loss = test(model, loader_mma, device)\n",
    "print('test acc dbtrain-mma dbtest-mma: ', acc_mma_mma)\n",
    "\n",
    "### RAFDB\n",
    "acc_mma_raf, test_loss = test(model, loader_raf, device)\n",
    "print('test acc dbtrain-mma dbtest-raf: ', acc_mma_raf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "root_dir = \"../data/datasetsfew\"\n",
    "dataset_sfew = SFEWDataset(root_dir=root_dir, idx_test=emotion_to_index_raf, idx_sfew=index_to_emotion_sfew, split=\"Val\", transform=eval_transforms)\n",
    "loader_sfew = torch.utils.data.DataLoader(dataset_sfew, batch_size=args.batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=args.workers,\n",
    "                                          pin_memory=True)\n",
    "\n",
    "root_dir = \"../data/MMAFEDB\"\n",
    "dataset_mma = MMADataset(root_dir=root_dir, idx_test=emotion_to_index_raf, idx_mma=index_to_emotion_mma, split=\"test\", transform=eval_transforms)\n",
    "loader_mma = torch.utils.data.DataLoader(dataset_mma, batch_size=args.batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=args.workers,\n",
    "                                          pin_memory=True)\n",
    "\n",
    "root_dir = \"../data/AffectNetCustom\"\n",
    "dataset_affect = AffectNetDataset(root_dir=root_dir, idx_test=emotion_to_index_raf, idx_aff=index_to_emotion_aff, split=\"test\", transform=eval_transforms)\n",
    "loader_affect = torch.utils.data.DataLoader(dataset_affect, batch_size=args.batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=args.workers,\n",
    "                                          pin_memory=True)\n",
    "\n",
    "dataset_raf = RafDataSet('../data/raf-basic', idxs_test=emotion_to_index_raf, idxs_raf=index_to_emotion_raf, train=False, transform=eval_transforms)\n",
    "loader_raf = torch.utils.data.DataLoader(dataset_raf,\n",
    "                                           batch_size=args.batch_size,\n",
    "                                           #batch_size=1,\n",
    "                                           shuffle=False,\n",
    "                                           num_workers=args.workers,\n",
    "                                           pin_memory=True)\n",
    "\n",
    "root_dir = \"../data/FER2013Plus\"\n",
    "dataset_fer = FERPlusDataset(root_dir=root_dir, idxs_test=emotion_to_index_raf, idxs_fer=index_to_emotion_fer, subset=\"FER2013Test\", transform=eval_transforms)\n",
    "loader_fer = torch.utils.data.DataLoader(dataset_fer, batch_size=args.batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=args.workers,\n",
    "                                          pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load('../models/ours_best_RAFDB.pth')\n",
    "checkpoint = checkpoint[\"model_state_dict\"]\n",
    "model = load_pretrained_weights(model, checkpoint)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### RAFDB\n",
    "acc_raf_raf, test_loss = test(model, loader_raf, device)\n",
    "print('test acc dbtrain-raf dbtest-raf: ', acc_raf_raf)\n",
    "\n",
    "###### FER\n",
    "acc_raf_fer, test_loss = test(model, loader_fer, device)\n",
    "print('test acc dbtrain-raf dbtest-fer: ', acc_raf_fer)\n",
    "\n",
    "###### AFFECT\n",
    "acc_raf_affect, test_loss = test(model, loader_affect, device)\n",
    "print('test acc dbtrain-raf dbtest-affect: ', acc_raf_affect)\n",
    "\n",
    "###### SFEW\n",
    "acc_raf_sfew, test_loss = test(model, loader_sfew, device)\n",
    "print('test acc dbtrain-raf dbtest-sfew: ', acc_raf_sfew)\n",
    "\n",
    "###### MMA\n",
    "acc_raf_mma, test_loss = test(model, loader_mma, device)\n",
    "print('test acc dbtrain-raf dbtest-mma: ', acc_raf_mma)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 696331,
     "sourceId": 1218456,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1780823,
     "sourceId": 2905596,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 2784784,
     "sourceId": 4876418,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5369417,
     "sourceId": 8926506,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5954662,
     "sourceId": 9730490,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6477402,
     "sourceId": 10462385,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6478017,
     "sourceId": 10463310,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "PyTorch GPU",
   "language": "python",
   "name": "pytorch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
